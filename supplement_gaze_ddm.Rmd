---
title: 'Supplement for Lasagna et al. 2024'
subtitle: '"Cognitive Mechanisms of Self-Referential Social Perception in Psychosis and Bipolar Disorder: Insights from Computational Modeling"'
author: "Carly A. Lasagna, Ivy F. Tso, Scott D. Blain, Timothy J. Pleskac"
output:
  html_document:
    toc: yes
    toc_float: true
    toc_collapsed: true
    number_sections: yes
    fig_width: 4
    fig_height: 4
    df_print: paged
    toc_depth: 4
geometry: margin=2cm
classoption: 6pt
header-includes:
  - \usepackage{amsmath}
---

\newpage

# Code Availability

**Code used to perform all modeling, simulation, and analysis is available on OSF and GitHub:**

-   OSF [(https://osf.io/x5n93/?view_only=c5e6d4bf6fbe48bebe9e28d6938b9246)](https://osf.io/x5n93/?view_only=c5e6d4bf6fbe48bebe9e28d6938b9246)
-   GitHub [(https://github.com/CarlyLasagna/gazeddm_sz_bd)](https://github.com/CarlyLasagna/gazeddm_sz_bd).

```{r id1, echo=FALSE, message=FALSE, warning=FALSE, out.width='1500px'}

# Coding key for groups, task conditions, and task responses:__
# 
# *Diagnostic groups:* 1=HC, 2=BD, 3=SZ
# 
# *DDM parameters:* alpha=threshold separation, beta=start point, ndt=non-decision time, delta=drift rate, delta bias=drift bias
# 
# *Task stimuli:* 
# 
# * gaze direction: 1=direct, 2=indirect 
# * head orientation: 1=forward, 2=deviated 
# * emotion: 1=neutral, 2=fearful 
# * emo*head: 1=neutral-forward, 2=fearful-forward, 3=neutral-deviated, 4=fearful-deviated
# 
# *Task responses:* 
# 
# * resp: 1=yes (looking at me), 2=no (not looking at me)
# * acc: 0=incorrect, 1=correct

# Setup: load packages and custom functions

rm(list=ls()) #clear environment
cat("\f") #clear console
set.seed(42) #clear previous seed

options(width = 10000)
options(max.print=10000)
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rstan)
library(ggplot2)
library(tidyr)
library(plyr)
library(dplyr)
library(formatR)
library(loo)
library(bayesplot)
library(shredder)
library(RWiener)
library(hrbrthemes)
library(viridis)
library(ggridges)
library(HDInterval)
library(boot)
library(pkgcond)
library(cmdstanr)
library(posterior)
library(data.table)
library(parallel)
library(doParallel)
library(foreach)
library(psych)
library(kableExtra)
library(reshape2)
library(BayesFactor)

#from HBayesDM toolbox  (Ahn et al 2014)
HDIofMCMC <- function(sampleVec,credMass = 0.9) {
  sortedPts = sort(sampleVec)
  ciIdxInc = floor(credMass * length(sortedPts))
  nCIs = length(sortedPts) - ciIdxInc
  ciWidth = rep(0 , nCIs)
  for (i in 1:nCIs) {
    ciWidth[i] = sortedPts[i + ciIdxInc] - sortedPts[i]
  }
  HDImin = sortedPts[which.min(ciWidth)]
  HDImax = sortedPts[which.min(ciWidth) + ciIdxInc]
  HDIlim = c(HDImin , HDImax)
  return(HDIlim)
}

```
 
# [Methods Supplement]{.underline} {.unnumbered}

# Study Sample

Participants were recruited from the University of Michigan Prechter Bipolar Longitudinal Study, community advertisements, and local clinics. During recruitment,  distributions of HC subsamples were age- and sex-matched to those of the SZ and BD sub-samples. Participants had no history of medical conditions with neurological sequelae, visual acuity of 20/30 or better on Snellen chart, and no recent substance use disorder (patients had no substance abuse or dependence in past year, HC in last five years). BD met criteria for bipolar I and SZ met criteria for schizophrenia or schizoaffective disorder. HC had no history of axis I disorders and no first-degree relatives with bipolar or psychotic disorders. 

# Data Collection

Participants gave written informed consent and were compensated for participation. The study received approval from the Institutional Review Board at the University of Michigan Medical School. Study procedures included: diagnostic assessments and clinical ratings by trained assessors, neuropsychological tests of general/social cognition, self-reports, and a behavioral gaze discrimination task. The task was completed during the acquisition of electroencephalography (EEG) data, but EEG analysis is outside the scope of this paper.

# Assessments


Diagnoses were confirmed by trained assessors using the Structured Clinical Interview for DSM-IV-TR or Diagnostic Interview for Genetic Studies. In the SZ sub-study, SZ symptoms (Scale for Assessment of Positive Symptoms [SAPS]; Scale for Assessment of Negative Symptoms [SANS]), depressive symptoms (Beck Depression Inventory-II [BDI-II 53]), general cognition (Brief Assessment of Cognition in Schizophrenia [BACS]), emotion-related social cognition (Mayer-Salovey-Caruso Emotional Intelligence Test [MSCEIT]), and social functioning (Social Adjustment Scale Self-Report, Social/Leisure scale [SASSR-Social]; inverse-coded) were assessed. In the BD sub-study, mania (Altman Self-Rated Mania scale [ASRM 52]) and depressive symptoms (BDI-II) were assessed. Inter-rater reliability was > 80% for diagnoses and clinician-assessed ratings.

For analyses in the main text, the measures of our clinical correlates were scored as follows:

* __BACS scoring:__ BACS subtests were z-scored relative to published age- and gender- norms of a normative HC sample and averaged to obtain a composite of general cognition (Keefe, 2008). 
* __MSCEIT scoring:__ MSCEIT subscales were converted to age- and gender-corrected standard scores (Mayer, 2002), z-scored relative to the full sample, and averaged to obtain a composite of emotion-based social cognition. 
* __SANS-Amotivation scoring (Negative symptom-related amotivation factor):__ The amotivation factor was calculated by applying published factor loadings (Sayers, 1996) to SANS avolition/apathy and asociality/anhedonia items and summing those to obtain a composite.
* __SAPS-Delusion scoring:__ Delusional symptoms were scored at the total summed score of delusional items on the SAPS.
* __SAPS-Hallucination scoring:__ Hallucination symptoms were scored at the total summed score of hallucination items on the SAPS.
* __BDI-Depression scoring:__ Depressive symptoms were scored as the total summed score on the BDI-II.
* __ASRM-Mania scoring:__ Mania symptoms were scored as the total summed score on the ASRM scale.

# [Modeling Supplement]{.underline} {.unnumbered}

# Defining/Refining Model Space

The steps below detail the full process of defining and refining the full model space explored in the current study. After completing these steps, we were left with 8 models that underwent more comprehensive testing: 1,2,5,6,7,8,9, and 10. Full specification of these models is provided in the next section.

```{r id2, echo=FALSE, message=FALSE, warning=FALSE}

model_space<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/define_model_space.csv",header=T)

model_space$Step<-""
model_space$Model.Class<-""

get_col<-which(colnames(model_space)=="Model.Class")
model_space<-model_space[,-get_col]

colnames(model_space)<-c("Step / Model Type","Model","Tested","Rationale","Pars Vary by GAZE","Pars Vary by HEAD","Pars Vary by EMO")

model_space %>%
  kbl(caption = "Defining Model Space",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  pack_rows("Step 1: Define baseline model", 1, 1) %>%
  pack_rows("Step 2: Define model space for models in which 1 single parameter varies by one or several task conditions", 2,22) %>%
  pack_rows("Step 3: For plausible models defined above, build upon those by letting one additional parameter vary by task conditions. Begin with simplest model (model 2)", 23, 28) %>%
  pack_rows("Step 4: Initial model comparisons models made 2 things clear: 1) the models ran into difficulties when start point parameters varied by emotion condition (model 7 and 8); AND 2) fit was better fit when drift rate varied by gaze AND head (model 9 and 10 vs. others). Thus, the only additional models that made sense to consider building upon were model 9 and 10. Thus 2 more complex models were considered but not tested for reasons specified below", 29, 30)  %>%
  pack_rows("Baseline", 1, 1,hline_before = T,italic=T) %>%
  pack_rows("Draft Rate Vary Only", 2, 8,hline_before = T,italic=T) %>%
  pack_rows("Threshold Separation Vary Only", 9, 15,hline_before = T,italic=T) %>%
  pack_rows("Start Point Vary Only", 16, 22,hline_before = T,italic=T) %>%
  pack_rows("Building Upon Model 2", 23, 28,hline_before = T,italic=T) %>%
  pack_rows("Building Upon Model 9", 29, 29,hline_before = T,italic=T) %>%
  pack_rows("Building Upon Model 10", 30, 30,hline_before = T,italic=T) %>%
  kable_styling(full_width=T)

```

# Model Specification

After refining the model space, we were left with 8 models that underwent more comprehensive testing: 1,2,5,6,7,8,9, and 10. We provide graphical and descriptive specification of these models is in the sections that follow.

For all groups h (from 1 to 3, where 1=hc, 2=bd, 3=sz), subjects i (from 1 to 100), and trials j (from 1 to max of 512), C~ij~ indicates the choice (1=yes/upper bound, 2=no/lower bound) and RT~ij~ indicates the reaction time in seconds. RT~ij~ is Wiener distributed W(α~i~,β~i~,δ~i~,τ~i~,), where α~i~ is the subject-level threshold separation (0.1\<α~i~\<3.9), β~i~ is the subject-level start point (0\<β~i~\<1), δ~i~ is the subject-level drift rate (-4\<δ~i~\<4), and τ~i~ is the subject-level NDT (0\<τ~i~\<scaled minimum reaction time [minRT~i~] for subject i). Because the Wiener distribution in Stan allows upper-boundary responses only, lower boundary choices were modeled using -δ~i~ for drift rate and 1-β~i~ for start point.

To facilitate sampling, a non-centered parameterization is used. This means that parameters are sampled in a 'standardized' space and then transformed into target distributions. For a given parameter (say, threshold separation), the diagnostic group- (α~μh~) and subject-level means (α~i~') were sampled from a normal N(0,1), while group-level variances (α~σh~) were sampled from a positive-bound normal distribution N(0,.2)^+^. Then, the subject's threshold separation parameter (α~i~) was transformed via φ(α~μh~+α~σh~ x α~i~) x 3.9+0.1, such that the α~i~ is informed by subject- (α~i~') and group-level information (α~μh~ and α~σh~), transformed to a 0\<x\<1 scale using the φ standard cumulative normal distribution function, and scaled\* to the desired range.

*\*NDT is estimated as a proportion (0\<x\<1) and scaled by each subject's minRT~i~ to bring the values into units of seconds. To help with issues sampling the NDT (i.e., the models had difficulty sampling on the trial in which each subject's minimum RT occurred), we scaled the NDT by 0.9 of the subject's actual minRT~i~.*

For models 2, 5,6,7,8,9, and 10, gaze direction conditions, k (from 1 to 2, where 1=direct,2=indirect) were also accounted for by different parameters as indicated below. For models 5,6,7,8,9, and 10, head orientation conditions, m (from 1 to 2, where 1=forward,2=deviated), and/or emotion conditions, l (from 1 to 2, where=neutral,2=fearful), are also accounted for by different parameters as indicated below.

## Model 1

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). (tau; τ).**

```{r id3, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m1.jpg')
```

## Model 2

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), NDT (tau; τ). Gaze condition effect (k) for drift rate (delta; δ).**

```{r id4, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m2.jpg')
```

## Model 5

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). Gaze (k) and emotion (l) condition effect for drift rate (delta; δ).**

```{r id5, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m5.jpg')
```

## Model 6

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). Gaze (k) and emotion condition (l) effect for drift rate (delta; δ). Emotion condition (l) effect for start point (beta; β).**

```{r id6, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m6.jpg')
```

## Model 7

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). Gaze (k) and emotion (l) condition effect for drift rate (delta; δ). Emotion condition (l) effect for threshold separation (alpha; α).**

```{r id7, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m7.jpg')
```

## Model 8

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). Gaze (k) and emotion (l) condition effect for drift rate (delta; δ). Emotion condition (l) effect for start point (beta; β) and threshold separation alpha; α).**

```{r id8, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%',cache=TRUE}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m8.jpg')
```

## Model 9

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). Gaze (k) and head (m) condition effect for drift rate (delta; δ).**

```{r id12334432323, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m9.jpg')
```

## Model 10 (Winning Model)

**Description: Group (h) and subject (i) effects for drift rate (delta; δ), threshold separation (alpha; α), start point (beta; β), ndt (tau; τ). Gaze (k), emotion (l), and head (m) condition effect for drift rate (delta; δ).**

```{r id9, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/figures_tables/plots/m10.jpg')
```

# Convergence Checks

Basic convergence diagnostics were performed for all models. This included checking for 0 divergences, ensuring that all Rhat values were \<1.1, checking that trace plots were well-mixed, verifying that autocorrelation was low by lag of \~30, and checking that the effective sample size (ESS) was sufficient. Models generally performed well by these standards, suggesting no problems with convergence. Though model 7 and model 8 (which allowed start point to vary by emotion condition) has some trouble with sampling efficiency. We do not include full diagnostics here for brevity. Instead, we only present these complete diagnostics below for the final winning model (Model 10).

# Parameter Recovery

Parameter recovery analyses were performed at the start of model testing (for our baseline model) and prior to analyses (for our winning model) to ensure we could recover parameters for this task and DDM parameterization. Details of parameter recovery from both stages of modeling testing are detailed below.

## Parameter Recovery (Model 1)

We fixed NDT at 0.2 here 1) to save on storage/computing time, and 2) because we were not interested in individual differences in NDT based on subject-level parameter estimates.

To begin, we selected plausible values for group-level "generating parameter values" (threshold separation=2, start point=.5, drift rate=-0.5). Group-level posteriors for each parameter were then defined as normal distributions with means equal to the aforementioned values and each with a SD of 0.1. Using these group-level parameter distributions, we randomly sampled subject-level parameter values for 25 subjects. These "subject-level generating values" were then fed into the "rwiener" function of the RWiener package (Wabersich & Vandekerckhove, 2014) to simulate choice and RTs for 512 trials---the number of trials on the current task. We fit simulated data in Stan using 1000 warmup draws and 4000 postwarmup draws sampled over multiple chains. Convergence checks (described previously) indicated that all models had converged. After fitting models, we calculated the means and 95% highest density intervals (HDIs) for the estimated posteriors (from the model fits) for group- and subject-level parameters. These steps were repeated for each of 50 separate simulations. We then evaluated parameter recovery in 3 ways, described below in the following 3 sections.

```{r id10, echo=FALSE, message=FALSE, warning=FALSE,cache=TRUE}

# Code below calls summary data for fitted parameter recovery simulations that were run using separate 'parameter_recovery.R' script.

# PREP DATA

#set MODEL parameters for simulations
h=.01             #time interval (in seconds)
seconds=5         #max time (in seconds) for a response
sigma=1           #diffusion coefficient

#parameters in transformed space
alpha=c(2)
beta=c(.5) 
delta=c(-.5)
ndt=c(.2) 
rtBound=0 #lowest rt allowed

#starting values
alphainits=alpha
betainits=beta
deltainits=delta
ndtinits=ndt #ndt fixed for this

# untransformed sigma
sigdelta=.1 
sigalpha=.1 
sigbeta=.1 
#signdt<-.02 #ndt fixed for this

# back transform to standard space
rawalpha=qnorm((alpha-.1)/3.9)
rawdelta=qnorm((delta+4)/8)
rawbeta=qnorm(beta)

# set SIMULATION parameters
numsims <- 50 #number of simulations 
numsubjs <- 25 # subjects 
trials <- 512 # trials
seed <- 42 #random seed 
plots = FALSE

#set TASK parameters
n_cond <- 1 #number of task conditions 
n_choice <- 2 #number of choice alternatives
n_groups<-3 #number of diagnostic groups

#set MCMC SAMPLER parameters
warmup=1000 #number of burn in samples
cores=4 #number of cores to use (default=1)
iter=2000 #total number of samples (total # postwarmup draws = (iter-warmup)*chains)
chains=4 #number of mcmc chains

simpath<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m1_par_recovery/output/"

#load data
file<-paste(simpath,'simmcmc_all.csv',sep="")
allmcmc<-read.csv(file,header=T)

file<-paste(simpath,'sub_summary.csv',sep="")
sub_summary<-read.csv(file,header = T)

file<-paste(simpath,'sim_summary.csv',sep="")
group_summary<-read.csv(file,header = T)

sims<-max(sub_summary$sim)
numsubjs<-max(sub_summary$subj)

# define which subjects recovered generating vals
sub_summary$recover_alpha<-as.numeric(sub_summary$genalpha>sub_summary$fitalpha_hdilo & sub_summary$genalpha<sub_summary$fitalpha_hdihi)
sub_summary$recover_alpha<-as.factor(sub_summary$recover_alpha)
sub_summary$recover_beta<-as.numeric(sub_summary$genbeta>sub_summary$fitbeta_hdilo & sub_summary$genbeta<sub_summary$fitbeta_hdihi)
sub_summary$recover_beta<-as.factor(sub_summary$recover_beta)
sub_summary$recover_delta<-as.numeric(sub_summary$gendelta>sub_summary$fitdelta_hdilo & sub_summary$gendelta<sub_summary$fitdelta_hdihi)
sub_summary$recover_delta<-as.factor(sub_summary$recover_delta)

# define which simulations recovered group level generating vals
group_summary$recover_alpha<-as.numeric(group_summary$gen_alpha>group_summary$alpha_hdi_lo & group_summary$gen_alpha<group_summary$alpha_hdi_hi)
group_summary$recover_alpha<-as.numeric(group_summary$recover_alpha)
group_summary$recover_beta<-as.numeric(group_summary$gen_beta>group_summary$beta_hdi_lo & group_summary$gen_beta<group_summary$beta_hdi_hi)
group_summary$recover_beta<-as.numeric(group_summary$recover_beta)
group_summary$recover_delta<-as.integer(group_summary$gen_delta>group_summary$delta_hdi_lo & group_summary$gen_delta<group_summary$delta_hdi_hi)
group_summary$recover_delta<-as.numeric(group_summary$recover_delta)
group_summary$recover_ndt<-as.numeric(group_summary$gen_ndt>group_summary$ndt_hdi_lo & group_summary$gen_ndt<group_summary$ndt_hdi_hi)
group_summary$recover_ndt<-as.numeric(group_summary$recover_ndt)

# #define at the subject level which recovered subject level gen values
# numsims<-max(as.numeric(allmcmc$sim))
# 
# for (i in 1:numsims){ #fill in which simulations recovered group parameters in long form 'allmcmc' data
#   summary_row<-which(sim_summary$sim==i,arr.ind = TRUE)
#   mcmc_row<-which(allmcmc$sim == i, arr.ind=TRUE) #get all rows in all mcmc for current simulation #
#   allmcmc$recover_alpha[mcmc_row]<-sim_summary$recover_alpha[summary_row]
#   allmcmc$recover_beta[mcmc_row]<-sim_summary$recover_beta[summary_row]
#   allmcmc$recover_delta[mcmc_row]<-sim_summary$recover_delta[summary_row]
#   allmcmc$recover_ndt[mcmc_row]<-sim_summary$recover_ndt[summary_row]
# }
# 
# allmcmc$recover_alpha_f<-as.factor(allmcmc$recover_alpha)
# allmcmc$recover_beta_f<-as.factor(allmcmc$recover_beta)
# allmcmc$recover_delta_f<-as.factor(allmcmc$recover_delta)
# allmcmc$recover_ndt_f<-as.factor(allmcmc$recover_ndt)


```

### Group-Level Parameters (Averaged over simulations)

First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits.

Results below suggest that group level parameters were well-recovered on average because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id11, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=2, fig.width=3.25,cache=TRUE}

# remove ndt columns
ndt_cols <- grep("ndt", names(group_summary), value = FALSE)
plot_data<-group_summary[,-ndt_cols]
plot_data2 <- data.frame(t(colMeans(plot_data)))
plot_data2$group<-"Mean_50Sims" #not done for any group, just any plausible values

#ALPHA
p <- ggplot(data = plot_data2, aes(x = group, y = mu_alpha, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = alpha_hdi_lo, ymax = alpha_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  ggtitle("Threshold Separation\n(Averaged over 50 sims)") + 
  xlab("") +
  ylab("Parameter Estimate") +
  scale_color_manual(values = c('black')) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1),
        legend.position = "none") +  # This hides the legend
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_alpha), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip() +
  guides(color = FALSE)  # This hides the color legend

print(p)

#BETA
p <- ggplot(data = plot_data2, aes(x = group, y = mu_beta, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = beta_hdi_lo, ymax = beta_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  ggtitle("Start Point\n(Averaged over 50 sims)") + 
  xlab("") +
  ylab("Parameter Estimate") +
  scale_color_manual(values = c('black')) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1),
        legend.position = "none") +  # This hides the legend
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_beta), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip() +
  guides(color = FALSE)  # This hides the color legend

print(p)

#DELTA
p <- ggplot(data = plot_data2, aes(x = group, y = mu_delta, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = delta_hdi_lo, ymax = delta_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  ggtitle("Drift Rate\n(Averaged over 50 sims)") + 
  xlab("") +
  ylab("Parameter Estimate") +
  scale_color_manual(values = c('black')) +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1),
        legend.position = "none") +  # This hides the legend
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_delta), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip() +
  guides(color = FALSE)  # This hides the color legend

print(p)

```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not.

Results below suggest that group level generating parameters were well-recovered because the generating values (vertical black line in plots below) were contained within the average 95% HDI (error bars in plots below) in 94-98% of simulations for all parameters.

```{r id12, echo=FALSE, fig.height=5.5, fig.width=2.5, message=FALSE, warning=FALSE,fig.show="hold"}

group_summary$sim<-seq(from=1,to=nrow(group_summary),by=1)

## ALPHA
text<-paste(round(mean(group_summary$recover_alpha),3)*100,"% Recovery",sep="")
group_summary$recover_alpha <- factor(group_summary$recover_alpha, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = group_summary, aes(x = mu_alpha,y=factor(sim), color=recover_alpha)) +
  geom_point(data = group_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(group_summary$gen_alpha))+
  theme_bw() +
  geom_errorbar(data = group_summary, aes(xmin = alpha_hdi_lo, xmax = alpha_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Threshold Separation\nParameter Recovery") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.9, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## BETA
text<-paste(round(mean(group_summary$recover_beta),3)*100,"% Recovery",sep="")
group_summary$recover_beta <- factor(group_summary$recover_beta, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = group_summary, aes(x = mu_beta,y=factor(sim), color=recover_beta)) +
  geom_point(data = group_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(group_summary$gen_beta))+
  theme_bw() +
  geom_errorbar(data = group_summary, aes(xmin = beta_hdi_lo, xmax = beta_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Start Point\nParameter Recovery") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = .47, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA
text<-paste(round(mean(group_summary$recover_delta),3)*100,"% Recovery",sep="")
group_summary$recover_delta <- factor(group_summary$recover_delta, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = group_summary, aes(x = mu_delta,y=factor(sim), color=recover_delta)) +
  geom_point(data = group_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(group_summary$gen_delta))+
  theme_bw() +
  geom_errorbar(data = group_summary, aes(xmin = delta_hdi_lo, xmax = delta_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\nParameter Recovery") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -.7, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 94-96% of simulated participants.

```{r id13, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",fig.height=3.5,fig.width=3.35}

#ALPHA
text<-paste(round(mean((as.numeric(sub_summary$recover_alpha)-1)),2)*100,"% Recovery",sep="")
sub_summary$recover_alpha <- factor(sub_summary$recover_alpha, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=sub_summary,aes(x=genalpha,y=fitalpha,color=factor(recover_alpha)))+
  geom_point(data=sub_summary,alpha=.2)+
  geom_errorbar(data = sub_summary, aes(ymin=fitalpha_hdilo,ymax=fitalpha_hdihi),alpha=.2)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1.4,2.7),ylim = c(1.4, 2.7))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Threshold Separation\nParameter Recovery") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.4, y =2.4, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

#BETA
text<-paste(round(mean((as.numeric(sub_summary$recover_beta)-1)),2)*100,"% Recovery",sep="")
sub_summary$recover_beta <- factor(sub_summary$recover_beta, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=sub_summary,aes(x=genbeta,y=fitbeta,color=factor(recover_beta)))+
  geom_point(data=sub_summary,alpha=.2)+
  geom_errorbar(data = sub_summary, aes(ymin=fitbeta_hdilo,ymax=fitbeta_hdihi),alpha=.2)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(.3,.7),ylim = c(.3, .7))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Start Point\nParameter Recovery") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = .3, y =.65, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

#DELTA
text<-paste(round(mean((as.numeric(sub_summary$recover_delta)-1)),2)*100,"% Recovery",sep="")
sub_summary$recover_delta <- factor(sub_summary$recover_delta, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=sub_summary,aes(x=gendelta,y=fitdelta,color=factor(recover_delta)))+
  geom_point(data=sub_summary,alpha=.2)+
  geom_errorbar(data = sub_summary, aes(ymin=fitdelta_hdilo,ymax=fitdelta_hdihi),alpha=.2)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-1.8,.8),ylim = c(-1.8, .8))+theme_bw()+  
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate\nParameter Recovery") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.4, y =.4, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

rm(list=c('allmcmc','sub_summary','group_summary','sim_summary'))
invisible(gc())

```

## Parameter Recovery (Model 10)

Parameter recovery was also performed for the winning model: Model 10. We used the same simulation and fitting procedure for Model 10 as we described above for Model 1, with a few exceptions: 1) simulated data and model fits accounted for the effects of gaze direction, head orientation, and facial emotion on drift rates; 2) parameter recovery was performed separately for HC, SZ, and BD; 3) group-level generating values were defined as normal distributions with means and SDs matching those of each diagnostic group's group level mean posteriors from the "final model fit" of Model 10 described in section 10 below; and 4) for each simulation, the simulated number of subjects matched the actual number of subjects for each group. We evaluated parameter recovery in 3 ways, as before, which is described in the following 3 sections (broken down by by diagnostic group and task conditions).

```{r id101, echo=FALSE, message=FALSE, warning=FALSE,cache=TRUE}

#Parameter recovery was performed by calling R scripts of the format: run_m10_parRecovery_hc_um_hpc_run1.R. The code below calls the output of those scripts and combines them together for each group.

recover_path<-'/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10_par_recovery/par_recovery'
n_groups<-3
n_runs<-3 #each group was run for 3 separate runs of 20 simulations run concurrently on hpc

#loop through groups and combine summary outputs
for (i in 1:n_groups){
  for (j in 1:n_runs){
    irecover_path<-paste(recover_path,"/group",i,sep="")
    group_filename<-paste(irecover_path,"/parRecovery_groupLvl_summary_run",j,".csv",sep="")
    subj_filename<-paste(irecover_path,"/parRecovery_subjLvl_summary_run",j,".csv",sep="")
    
    tmp_group_summary<-read.csv(group_filename)
    tmp_subj_summary<-read.csv(subj_filename)
    
    if(i==1&j==1){
      group_summary_all<-tmp_group_summary
      subj_summary_all<-tmp_subj_summary
    }else{
      group_summary_all<-rbind(group_summary_all,tmp_group_summary)
      subj_summary_all<-rbind(subj_summary_all,tmp_subj_summary)
    }
  }
}

#loop through again and grab only first 50 simulations (extras were run in case of issues with any fits, but no issues observed)
for (i in 1:n_groups){
  tmp_group_summary<-subset(group_summary_all, group_summary_all$group==i)
  tmp_group_summary$sim_id<-match(tmp_group_summary$sim, unique(tmp_group_summary$sim))
  remove<-which(tmp_group_summary$sim_id>50)
  
  if (length(remove) > 0) {
    tmp_group_summary <- tmp_group_summary[-remove,]
  }
  
  tmp_subj_summary<-subset(subj_summary_all, subj_summary_all$group==i)
  tmp_subj_summary$sim_id<-match(tmp_subj_summary$sim, unique(tmp_subj_summary$sim))
  remove<-which(tmp_subj_summary$sim_id>50)
  
  if (length(remove) > 0) {
    tmp_subj_summary <- tmp_subj_summary[-remove,]
  }
  
  if(i==1){
      group_summary<-tmp_group_summary
      subj_summary<-tmp_subj_summary
    }else{
      group_summary<-rbind(group_summary,tmp_group_summary)
      subj_summary<-rbind(subj_summary,tmp_subj_summary)
    }
}


group_summary$group <- ifelse(group_summary$group == 1, "HC",
                              ifelse(group_summary$group == 2, "BD",
                                     ifelse(group_summary$group == 3, "SZ", group_summary$group)))

subj_summary$group <- ifelse(subj_summary$group == 1, "HC",
                              ifelse(subj_summary$group == 2, "BD",
                                     ifelse(subj_summary$group == 3, "SZ", subj_summary$group)))

setwd(recover_path)
write.csv(group_summary,"parRecovery_groupLvl_summary.csv",row.names=F)
write.csv(subj_summary,"parRecovery_subjLvl_summary.csv",row.names=F)

```

### Group-Level Parameters (Averaged over simulations)

First, we examined how well the group-level generating values were recovered by simulated models fits *on average*. We averaged over the posterior means and 95% HDIs for group-level parameters for all 50 simulations; plotted generating group-level values (red X's in plots below) superimposed over the averaged mean and 95% HDI (points and error bars in figure below); and assessed how well, on average, the group-level generating values were captured by simulated model fits. This was done separately for each diagnostic group and task condition.

Results below suggest that group level parameters were well-recovered on average for SZ, BD, and HC because 1) the generating values (red X's in plots below) were contained within the average 95% HDI for all parameters (error bars in plots below), and 2) the generating values are closely aligned with the group-level posterior mean (data points in plots below) of simulated fits on average for all parameters.

```{r id111, echo=FALSE, message=FALSE, warning=FALSE,fig.show="hold",fig.height=3, fig.width=3.25,cache=TRUE}

# average over recovered posterior means and HDI's of all simulations (within group)
plot_data<-group_summary %>%
  group_by(group) %>%
  summarise_all(mean)

#ALPHA
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$alpha_grp_sim_mean,
                       gen_mean=plot_data$alpha_grp_gen,
                       sim_hdi_lo=plot_data$alpha_grp_sim_hdi_lo,
                       sim_hdi_hi=plot_data$alpha_grp_sim_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Threshold Separation\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#BETA
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$beta_grp_sim_mean,
                       gen_mean=plot_data$beta_grp_gen,
                       sim_hdi_lo=plot_data$beta_grp_sim_hdi_lo,
                       sim_hdi_hi=plot_data$beta_grp_sim_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Start Point\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 1.1
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim1.1_mean,
                       gen_mean=plot_data$delta_grp_gen1.1,
                       sim_hdi_lo=plot_data$delta_grp_sim1.1_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim1.1_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Direct: Neutral/Forward)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 1.2 
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim1.2_mean,
                       gen_mean=plot_data$delta_grp_gen1.2,
                       sim_hdi_lo=plot_data$delta_grp_sim1.2_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim1.2_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Direct: Fearful/Forward)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 1.3
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim1.3_mean,
                       gen_mean=plot_data$delta_grp_gen1.3,
                       sim_hdi_lo=plot_data$delta_grp_sim1.3_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim1.3_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Direct: Neutral/Deviated)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 1.4
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim1.4_mean,
                       gen_mean=plot_data$delta_grp_gen1.4,
                       sim_hdi_lo=plot_data$delta_grp_sim1.4_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim1.4_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Direct: Fearful/Deviated)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 2.1
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim2.1_mean,
                       gen_mean=plot_data$delta_grp_gen2.1,
                       sim_hdi_lo=plot_data$delta_grp_sim2.1_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim2.1_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Indirect: Neutral/Forward)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 2.2 
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim2.2_mean,
                       gen_mean=plot_data$delta_grp_gen2.2,
                       sim_hdi_lo=plot_data$delta_grp_sim2.2_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim2.2_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Indirect: Fearful/Forward)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 2.3
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim2.3_mean,
                       gen_mean=plot_data$delta_grp_gen2.3,
                       sim_hdi_lo=plot_data$delta_grp_sim2.3_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim2.3_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Indirect: Neutral/Deviated)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

#DELTA 2.4
plot_data2<-data.frame(group=plot_data$group,
                       sim_mean=plot_data$delta_grp_sim2.4_mean,
                       gen_mean=plot_data$delta_grp_gen2.4,
                       sim_hdi_lo=plot_data$delta_grp_sim2.4_hdi_lo,
                       sim_hdi_hi=plot_data$delta_grp_sim2.4_hdi_hi)

p <- ggplot(data = plot_data2, aes(x = group, y = sim_mean, color = group)) +
  geom_point(data = plot_data2, alpha = 1, position = position_dodge(width = .5), size = 3) + 
  theme_bw() +
  geom_errorbar(data = plot_data2, aes(ymin = sim_hdi_lo, ymax = sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.55, 1.8)) +
  ggtitle("Drift Rate\n(Indirect: Fearful/Deviated)\n(Averaged over 50 sims)") + 
  scale_x_discrete(limits = c("HC", "BD", "SZ")) +
  scale_color_manual(values = c('#287c8e', '#482475', '#addc30')) +
  ylab("Parameter Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +
  geom_point(data = data.frame(category = plot_data2$group, value = plot_data2$gen_mean), 
             aes(x = category, y = value), shape = 4, size = 4, stroke = 2, color = "red",alpha=.4) +
  coord_flip()

print(p)

```

### Group-Level Parameters (Separate for each simulation)

Second, we examined group-level parameters for the individual simulations to determine what % of the 50 simulations successfully recovered the group-level generating parameter values (vertical black line in plots below). Recovery was "successful" if the 95% HDI of the fitted group-level posterior (error bars in plots below) for a given simulation contained the original subject-level generating value. Data in green in the plots below indicate the group-level parameters were recovered and data in red indicates they were not. This was done separately for each diagnostic group and task condition.

Results below suggest that group level generating parameters were well---recovered because--across all diagnostic groups and task conditions---the generating values (vertical black line in plots below) were contained within the average 95% HDI (error bars in plots below) in 88-100% of simulations for all parameters.

```{r id121, echo=FALSE, fig.height=5.5, fig.width=2.5, message=FALSE, warning=FALSE,fig.show="hold",cache=TRUE}

#############################################################################
# HC 
#############################################################################

## ALPHA
temp_summary<-subset(group_summary,group_summary$group=="HC")
temp_summary$sim<-seq(from=1,to=nrow(temp_summary),by=1)
text<-paste(round(mean(temp_summary$alpha_recover),2)*100,"% Recovery",sep="")
temp_summary$alpha_recover <- factor(temp_summary$alpha_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = alpha_grp_sim_mean,y=factor(sim), color=alpha_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$alpha_grp_gen))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = alpha_grp_sim_hdi_lo, xmax = alpha_grp_sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Threshold Separation\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.57, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## BETA
text<-paste(round(mean(temp_summary$beta_recover),2)*100,"% Recovery",sep="")
temp_summary$beta_recover <- factor(temp_summary$beta_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = beta_grp_sim_mean,y=factor(sim), color=beta_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$beta_grp_gen))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = beta_grp_sim_hdi_lo, xmax = beta_grp_sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Start Point\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.52, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.1
text<-paste(round(mean(temp_summary$delta1.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.1_recover <- factor(temp_summary$delta1.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim1.1_mean,y=factor(sim), color=delta1.1_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.1))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.1_hdi_lo, xmax = delta_grp_sim1.1_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct/Neutral/Forward)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.67, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.2
text<-paste(round(mean(temp_summary$delta1.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.2_recover <- factor(temp_summary$delta1.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim1.2_mean,y=factor(sim), color=delta1.2_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.2))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.2_hdi_lo, xmax = delta_grp_sim1.2_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct/Fearful/Forward)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.75, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.3
text<-paste(round(mean(temp_summary$delta1.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.3_recover <- factor(temp_summary$delta1.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim1.3_mean,y=factor(sim), color=delta1.3_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.3))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.3_hdi_lo, xmax = delta_grp_sim1.3_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct/Neutral/Deviated)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.35, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.4
text<-paste(round(mean(temp_summary$delta1.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.4_recover <- factor(temp_summary$delta1.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim1.4_mean,y=factor(sim), color=delta1.4_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.4))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.4_hdi_lo, xmax = delta_grp_sim1.4_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct/Fearful/Deviated)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.25, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.1
text<-paste(round(mean(temp_summary$delta2.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.1_recover <- factor(temp_summary$delta2.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim2.1_mean,y=factor(sim), color=delta2.1_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.1))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.1_hdi_lo, xmax = delta_grp_sim2.1_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect/Neutral/Forward)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.3, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.2
text<-paste(round(mean(temp_summary$delta2.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.2_recover <- factor(temp_summary$delta2.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim2.2_mean,y=factor(sim), color=delta2.2_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.2))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.2_hdi_lo, xmax = delta_grp_sim2.2_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect/Fearful/Forward)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.25, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.3
text<-paste(round(mean(temp_summary$delta2.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.3_recover <- factor(temp_summary$delta2.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim2.3_mean,y=factor(sim), color=delta2.3_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.3))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.3_hdi_lo, xmax = delta_grp_sim2.3_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect/Neutral/Deviated)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -3.15, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.4
text<-paste(round(mean(temp_summary$delta2.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.4_recover <- factor(temp_summary$delta2.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim2.4_mean,y=factor(sim), color=delta2.4_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.4))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.4_hdi_lo, xmax = delta_grp_sim2.4_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect/Fearful/Deviated)\nParameter Recovery (HC)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -3.2, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

#############################################################################
# BD 
#############################################################################

## ALPHA
temp_summary<-subset(group_summary,group_summary$group=="BD")
temp_summary$sim<-seq(from=1,to=nrow(temp_summary),by=1)
text<-paste(round(mean(temp_summary$alpha_recover),2)*100,"% Recovery",sep="")
temp_summary$alpha_recover <- factor(temp_summary$alpha_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = alpha_grp_sim_mean,y=factor(sim), color=alpha_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$alpha_grp_gen))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = alpha_grp_sim_hdi_lo, xmax = alpha_grp_sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Threshold Separation\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.58, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## BETA
text<-paste(round(mean(temp_summary$beta_recover),2)*100,"% Recovery",sep="")
temp_summary$beta_recover <- factor(temp_summary$beta_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = beta_grp_sim_mean,y=factor(sim), color=beta_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$beta_grp_gen))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = beta_grp_sim_hdi_lo, xmax = beta_grp_sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Start Point\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.515, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.1
text<-paste(round(mean(temp_summary$delta1.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.1_recover <- factor(temp_summary$delta1.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim1.1_mean,y=factor(sim), color=delta1.1_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.1))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.1_hdi_lo, xmax = delta_grp_sim1.1_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Neutral/Forward)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.35, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.2
text<-paste(round(mean(temp_summary$delta1.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.2_recover <- factor(temp_summary$delta1.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim1.2_mean,y=factor(sim), color=delta1.2_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.2))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.2_hdi_lo, xmax = delta_grp_sim1.2_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Fearful/Forward)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.4, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.3
text<-paste(round(mean(temp_summary$delta1.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.3_recover <- factor(temp_summary$delta1.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim1.3_mean,y=factor(sim), color=delta1.3_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.3))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.3_hdi_lo, xmax = delta_grp_sim1.3_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Neutral/Deviated)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.45, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.4
text<-paste(round(mean(temp_summary$delta1.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.4_recover <- factor(temp_summary$delta1.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim1.4_mean,y=factor(sim), color=delta1.4_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.4))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.4_hdi_lo, xmax = delta_grp_sim1.4_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Fearful/Deviated)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.25, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.1
text<-paste(round(mean(temp_summary$delta2.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.1_recover <- factor(temp_summary$delta2.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim2.1_mean,y=factor(sim), color=delta2.1_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.1))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.1_hdi_lo, xmax = delta_grp_sim2.1_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Neutral/Forward)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.25, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.2
text<-paste(round(mean(temp_summary$delta2.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.2_recover <- factor(temp_summary$delta2.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim2.2_mean,y=factor(sim), color=delta2.2_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.2))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.2_hdi_lo, xmax = delta_grp_sim2.2_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Fearful/Forward)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.2, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.3
text<-paste(round(mean(temp_summary$delta2.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.3_recover <- factor(temp_summary$delta2.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim2.3_mean,y=factor(sim), color=delta2.3_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.3))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.3_hdi_lo, xmax = delta_grp_sim2.3_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Neutral/Deviated)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.1, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.4
text<-paste(round(mean(temp_summary$delta2.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.4_recover <- factor(temp_summary$delta2.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim2.4_mean,y=factor(sim), color=delta2.4_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.4))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.4_hdi_lo, xmax = delta_grp_sim2.4_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Fearful/Deviated)\nParameter Recovery (BD)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.25, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

#############################################################################
# SZ
#############################################################################

## ALPHA
temp_summary<-subset(group_summary,group_summary$group=="SZ")
temp_summary$sim<-seq(from=1,to=nrow(temp_summary),by=1)
text<-paste(round(mean(temp_summary$alpha_recover),2)*100,"% Recovery",sep="")
temp_summary$alpha_recover <- factor(temp_summary$alpha_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = alpha_grp_sim_mean,y=factor(sim), color=alpha_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$alpha_grp_gen))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = alpha_grp_sim_hdi_lo, xmax = alpha_grp_sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Threshold Separation\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.66, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## BETA
text<-paste(round(mean(temp_summary$beta_recover),2)*100,"% Recovery",sep="")
temp_summary$beta_recover <- factor(temp_summary$beta_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = beta_grp_sim_mean,y=factor(sim), color=beta_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$beta_grp_gen))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = beta_grp_sim_hdi_lo, xmax = beta_grp_sim_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Start Point\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.51, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.1
text<-paste(round(mean(temp_summary$delta1.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.1_recover <- factor(temp_summary$delta1.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim1.1_mean,y=factor(sim), color=delta1.1_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.1))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.1_hdi_lo, xmax = delta_grp_sim1.1_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Neutral/Forward)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.4, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.2
text<-paste(round(mean(temp_summary$delta1.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.2_recover <- factor(temp_summary$delta1.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim1.2_mean,y=factor(sim), color=delta1.2_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.2))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.2_hdi_lo, xmax = delta_grp_sim1.2_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Fearful/Forward)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.35, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.3
text<-paste(round(mean(temp_summary$delta1.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.3_recover <- factor(temp_summary$delta1.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim1.3_mean,y=factor(sim), color=delta1.3_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.3))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.3_hdi_lo, xmax = delta_grp_sim1.3_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Neutral/Deviated)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.55, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 1.4
text<-paste(round(mean(temp_summary$delta1.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.4_recover <- factor(temp_summary$delta1.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim1.4_mean,y=factor(sim), color=delta1.4_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen1.4))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim1.4_hdi_lo, xmax = delta_grp_sim1.4_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Direct: Fearful/Deviated)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.3, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.1
text<-paste(round(mean(temp_summary$delta2.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.1_recover <- factor(temp_summary$delta2.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim2.1_mean,y=factor(sim), color=delta2.1_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.1))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.1_hdi_lo, xmax = delta_grp_sim2.1_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Neutral/Forward)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.4, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.2
text<-paste(round(mean(temp_summary$delta2.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.2_recover <- factor(temp_summary$delta2.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p <- ggplot(data = temp_summary, aes(x = delta_grp_sim2.2_mean,y=factor(sim), color=delta2.2_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.2))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.2_hdi_lo, xmax = delta_grp_sim2.2_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Fearful/Forward)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.5, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.3
text<-paste(round(mean(temp_summary$delta2.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.3_recover <- factor(temp_summary$delta2.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim2.3_mean,y=factor(sim), color=delta2.3_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.3))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.3_hdi_lo, xmax = delta_grp_sim2.3_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Neutral/Deviated)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.2, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

## DELTA 2.4
text<-paste(round(mean(temp_summary$delta2.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.4_recover <- factor(temp_summary$delta2.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data = temp_summary, aes(x = delta_grp_sim2.4_mean,y=factor(sim), color=delta2.4_recover)) +
  geom_point(data = temp_summary, alpha = 1, position = position_dodge(width = .5), size = 2) + 
  geom_vline(xintercept=mean(temp_summary$delta_grp_gen2.4))+
  theme_bw() +
  geom_errorbar(data = temp_summary, aes(xmin = delta_grp_sim2.4_hdi_lo, xmax = delta_grp_sim2.4_hdi_hi), 
                linewidth = .5, width = 0, alpha = 1, position = position_dodge(width = .5)) +
  coord_cartesian(ylim=c(0,60)) +
  ggtitle("Drift Rate\n(Indirect: Fearful/Deviated)\nParameter Recovery (SZ)") +
  xlab("Parameter Estimate") +
  ylab("Simulation Number") +
  theme(legend.position="none",plot.title = element_text(size=12),axis.text.y=element_text(size=6))+ 
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.4, y =59, label = text, hjust = "left",size=3,lineheight=.9)

print(p)

```

### Subject-Level Parameters

Third, we examined how well subject-level generating values were captured by simulated model fits. We determined what % of subject-level generating parameter values were successfully recovered across the 50 simulations. Recovery was "successful" if the 95% HDI of the fitted subject-level posterior contained the original subject-level generating value. Data in green in the plots below indicate the subject-level generating values were recovered and data in red indicates they were not. This was done separately for each diagnostic group and task condition.

Results below suggest that subject level generating parameters were well-recovered because the generating values (x-axis) were contained within the average 95% HDI of fitted values (error bars in plots below) in 92-97% of simulated participants.

```{r id131, echo=FALSE, message=FALSE, warning=FALSE, tidy=TRUE,fig.show="hold",cache=TRUE,fig.height=3.5,fig.width=3.35}

#############################################################################
# HC 
#############################################################################

## ALPHA
temp_summary<-subset(subj_summary,subj_summary$group=="HC")
temp_summary$sim<-seq(from=1,to=nrow(temp_summary),by=1)
text<-paste(round(mean(temp_summary$alpha_recover),2)*100,"% Recovery",sep="")
temp_summary$alpha_recover <- factor(temp_summary$alpha_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=alpha_sub_gen,y=alpha_sub_sim_mean,color=factor(alpha_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=alpha_sub_sim_hdi_lo,ymax=alpha_sub_sim_hdi_hi),alpha=.2)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1.4,1.8),ylim = c(1.4, 1.8))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Threshold Separation\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.4, y =1.78, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## BETA
text<-paste(round(mean(temp_summary$beta_recover),2)*100,"% Recovery",sep="")
temp_summary$beta_recover <- factor(temp_summary$beta_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=beta_sub_gen,y=beta_sub_sim_mean,color=factor(beta_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=beta_sub_sim_hdi_lo,ymax=beta_sub_sim_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(.49,.58),ylim = c(.49, .58))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Start Point\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = .5, y =.57, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.1
text<-paste(round(mean(temp_summary$delta1.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.1_recover <- factor(temp_summary$delta1.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.1,y=delta_sub_sim1.1_mean,color=factor(delta1.1_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.1_hdi_lo,ymax=delta_sub_sim1.1_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1,2.5),ylim = c(1, 2.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate\n(Direct/Neutral/Forward)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1, y =2.4, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.2
text<-paste(round(mean(temp_summary$delta1.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.2_recover <- factor(temp_summary$delta1.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.2,y=delta_sub_sim1.2_mean,color=factor(delta1.2_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.2_hdi_lo,ymax=delta_sub_sim1.2_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1,2.5),ylim = c(1, 2.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Forward)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1, y =2.4, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.3
text<-paste(round(mean(temp_summary$delta1.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.3_recover <- factor(temp_summary$delta1.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.3,y=delta_sub_sim1.3_mean,color=factor(delta1.3_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.3_hdi_lo,ymax=delta_sub_sim1.3_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-0.2,1.2),ylim = c(-0.2, 1.2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Deviated)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -0.1, y =1.1, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.4
text<-paste(round(mean(temp_summary$delta1.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.4_recover <- factor(temp_summary$delta1.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.4,y=delta_sub_sim1.4_mean,color=factor(delta1.4_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.4_hdi_lo,ymax=delta_sub_sim1.4_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-0.2,1.2),ylim = c(-0.2, 1.2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Deviated)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -0.1, y =1.1, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.1
text<-paste(round(mean(temp_summary$delta2.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.1_recover <- factor(temp_summary$delta2.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.1,y=delta_sub_sim2.1_mean,color=factor(delta2.1_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.1_hdi_lo,ymax=delta_sub_sim2.1_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-2.7,-1.5),ylim = c(-2.7, -1.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Forward)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.5, y =-1.6, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.2
text<-paste(round(mean(temp_summary$delta2.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.2_recover <- factor(temp_summary$delta2.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.2,y=delta_sub_sim2.2_mean,color=factor(delta2.2_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.2_hdi_lo,ymax=delta_sub_sim2.2_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-2.7,-1.4),ylim = c(-2.7, -1.4))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Forward)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.6, y =-1.5, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.3
text<-paste(round(mean(temp_summary$delta2.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.3_recover <- factor(temp_summary$delta2.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.3,y=delta_sub_sim2.3_mean,color=factor(delta2.3_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.3_hdi_lo,ymax=delta_sub_sim2.3_hdi_hi),alpha=0.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-3.4,-2.6),ylim = c(-3.4, -2.6))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Deviated)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -3.4, y =-2.6, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.4
text<-paste(round(mean(temp_summary$delta2.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.4_recover <- factor(temp_summary$delta2.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.4,y=delta_sub_sim2.4_mean,color=factor(delta2.4_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.4_hdi_lo,ymax=delta_sub_sim2.4_hdi_hi),alpha=0.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-3.5,-2.7),ylim = c(-3.5,-2.7))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Indirect/Fearful/Deviated)\nParameter Recovery (HC)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -3.4, y =-2.8, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

#############################################################################
# BD 
#############################################################################

## ALPHA
temp_summary<-subset(subj_summary,subj_summary$group=="BD")
temp_summary$sim<-seq(from=1,to=nrow(temp_summary),by=1)
text<-paste(round(mean(temp_summary$alpha_recover),2)*100,"% Recovery",sep="")
temp_summary$alpha_recover <- factor(temp_summary$alpha_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=alpha_sub_gen,y=alpha_sub_sim_mean,color=factor(alpha_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=alpha_sub_sim_hdi_lo,ymax=alpha_sub_sim_hdi_hi),alpha=.2)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1.4,1.8),ylim = c(1.4, 1.8))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Threshold Separation\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.4, y =1.78, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## BETA
text<-paste(round(mean(temp_summary$beta_recover),2)*100,"% Recovery",sep="")
temp_summary$beta_recover <- factor(temp_summary$beta_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=beta_sub_gen,y=beta_sub_sim_mean,color=factor(beta_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=beta_sub_sim_hdi_lo,ymax=beta_sub_sim_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(.5,.56),ylim = c(.5, .56))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Start Point\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = .5, y =.55, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.1
text<-paste(round(mean(temp_summary$delta1.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.1_recover <- factor(temp_summary$delta1.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.1,y=delta_sub_sim1.1_mean,color=factor(delta1.1_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.1_hdi_lo,ymax=delta_sub_sim1.1_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1,2),ylim = c(1, 2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Forward)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1, y =2, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.2
text<-paste(round(mean(temp_summary$delta1.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.2_recover <- factor(temp_summary$delta1.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.2,y=delta_sub_sim1.2_mean,color=factor(delta1.2_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.2_hdi_lo,ymax=delta_sub_sim1.2_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1,2),ylim = c(1, 2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Forward)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1, y =2, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.3
text<-paste(round(mean(temp_summary$delta1.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.3_recover <- factor(temp_summary$delta1.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.3,y=delta_sub_sim1.3_mean,color=factor(delta1.3_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.3_hdi_lo,ymax=delta_sub_sim1.3_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-0.2,1.2),ylim = c(-0.2, 1.2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Deviated)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -0.1, y =1.1, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.4
text<-paste(round(mean(temp_summary$delta1.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.4_recover <- factor(temp_summary$delta1.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.4,y=delta_sub_sim1.4_mean,color=factor(delta1.4_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.4_hdi_lo,ymax=delta_sub_sim1.4_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-0.2,1.1),ylim = c(-0.2, 1.1))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Deviated)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -0.15, y =1, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.1
text<-paste(round(mean(temp_summary$delta2.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.1_recover <- factor(temp_summary$delta2.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.1,y=delta_sub_sim2.1_mean,color=factor(delta2.1_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.1_hdi_lo,ymax=delta_sub_sim2.1_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-1.8,-.5),ylim = c(-1.8, -.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Forward)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.7, y =-0.6, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.2
text<-paste(round(mean(temp_summary$delta2.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.2_recover <- factor(temp_summary$delta2.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.2,y=delta_sub_sim2.2_mean,color=factor(delta2.2_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.2_hdi_lo,ymax=delta_sub_sim2.2_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-1.7,-.4),ylim = c(-1.7, -.4))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Forward)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -1.5, y =-.5, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.3
text<-paste(round(mean(temp_summary$delta2.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.3_recover <- factor(temp_summary$delta2.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.3,y=delta_sub_sim2.3_mean,color=factor(delta2.3_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.3_hdi_lo,ymax=delta_sub_sim2.3_hdi_hi),alpha=0.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-2.6,-1.4),ylim = c(-2.6, -1.4))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Deviated)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.5, y =-1.5, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.4
text<-paste(round(mean(temp_summary$delta2.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.4_recover <- factor(temp_summary$delta2.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.4,y=delta_sub_sim2.4_mean,color=factor(delta2.4_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.4_hdi_lo,ymax=delta_sub_sim2.4_hdi_hi),alpha=0.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-2.5,-1.7),ylim = c(-2.5,-1.7))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Indirect/Fearful/Deviated)\nParameter Recovery (BD)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.4, y =-1.75, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

#############################################################################
# SZ
#############################################################################

## ALPHA
temp_summary<-subset(subj_summary,subj_summary$group=="SZ")
temp_summary$sim<-seq(from=1,to=nrow(temp_summary),by=1)
text<-paste(round(mean(temp_summary$alpha_recover),2)*100,"% Recovery",sep="")
temp_summary$alpha_recover <- factor(temp_summary$alpha_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=alpha_sub_gen,y=alpha_sub_sim_mean,color=factor(alpha_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=alpha_sub_sim_hdi_lo,ymax=alpha_sub_sim_hdi_hi),alpha=.2)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(1.4,2),ylim = c(1.4, 2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Threshold Separation\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1.4, y =1.95, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## BETA
text<-paste(round(mean(temp_summary$beta_recover),2)*100,"% Recovery",sep="")
temp_summary$beta_recover <- factor(temp_summary$beta_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=beta_sub_gen,y=beta_sub_sim_mean,color=factor(beta_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=beta_sub_sim_hdi_lo,ymax=beta_sub_sim_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(.48,.58),ylim = c(.48, .58))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Start Point\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = .48, y =.57, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.1
text<-paste(round(mean(temp_summary$delta1.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.1_recover <- factor(temp_summary$delta1.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.1,y=delta_sub_sim1.1_mean,color=factor(delta1.1_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.1_hdi_lo,ymax=delta_sub_sim1.1_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(0.8,2.5),ylim = c(0.8, 2.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Forward)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 1, y =2.4, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.2
text<-paste(round(mean(temp_summary$delta1.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.2_recover <- factor(temp_summary$delta1.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.2,y=delta_sub_sim1.2_mean,color=factor(delta1.2_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.2_hdi_lo,ymax=delta_sub_sim1.2_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(0.7,2.5),ylim = c(0.7, 2.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Forward)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = 0.8, y =2.4, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.3
text<-paste(round(mean(temp_summary$delta1.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.3_recover <- factor(temp_summary$delta1.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.3,y=delta_sub_sim1.3_mean,color=factor(delta1.3_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.3_hdi_lo,ymax=delta_sub_sim1.3_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-0.2,1.6),ylim = c(-0.2, 1.6))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+  
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Deviated)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -0.1, y =1.5, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 1.4
text<-paste(round(mean(temp_summary$delta1.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta1.4_recover <- factor(temp_summary$delta1.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen1.4,y=delta_sub_sim1.4_mean,color=factor(delta1.4_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim1.4_hdi_lo,ymax=delta_sub_sim1.4_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-0.3,1.3),ylim = c(-0.3, 1.3))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Deviated)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -0.1, y =1.1, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.1
text<-paste(round(mean(temp_summary$delta2.1_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.1_recover <- factor(temp_summary$delta2.1_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.1,y=delta_sub_sim2.1_mean,color=factor(delta2.1_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.1_hdi_lo,ymax=delta_sub_sim2.1_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-2.2,-0.2),ylim = c(-2.2, -0.2))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Forward)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2, y =-0.5, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.2
text<-paste(round(mean(temp_summary$delta2.2_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.2_recover <- factor(temp_summary$delta2.2_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.2,y=delta_sub_sim2.2_mean,color=factor(delta2.2_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.2_hdi_lo,ymax=delta_sub_sim2.2_hdi_hi),alpha=.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-2.1,-.4),ylim = c(-2.1, -.4))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Fearful/Forward)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2, y =-.5, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.3
text<-paste(round(mean(temp_summary$delta2.3_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.3_recover <- factor(temp_summary$delta2.3_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.3,y=delta_sub_sim2.3_mean,color=factor(delta2.3_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.3_hdi_lo,ymax=delta_sub_sim2.3_hdi_hi),alpha=0.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-3,-1.1),ylim = c(-3, -1.1))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Direct/Neutral/Deviated)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.8, y =-1.2, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

## DELTA 2.4
text<-paste(round(mean(temp_summary$delta2.4_recover),2)*100,"% Recovery",sep="")
temp_summary$delta2.4_recover <- factor(temp_summary$delta2.4_recover, levels = c("1", "0"), labels = c("recovered", "not_recovered"))

p<-ggplot(data=temp_summary,aes(x=delta_sub_gen2.4,y=delta_sub_sim2.4_mean,color=factor(delta2.4_recover)))+
  geom_point(data=temp_summary,alpha=.2)+
  geom_errorbar(data = temp_summary, aes(ymin=delta_sub_sim2.4_hdi_lo,ymax=delta_sub_sim2.4_hdi_hi),alpha=0.1)+
  geom_abline(intercept=0,slope=1)+
  coord_cartesian(xlim = c(-3,-1.5),ylim = c(-3,-1.5))+theme_bw()+
  theme(legend.position="none",plot.title = element_text(size=12))+
  xlab("Generating Parameter Value")+ylab("Estimated Parameter Value")+ 
  ggtitle("Drift Rate (Indirect/Fearful/Deviated)\nParameter Recovery (SZ)") +
  scale_color_manual(values = c("#66a182","#d1495b"))+
  annotate(geom = "text", x = -2.9, y =-1.6, label = text, hjust = "left",size=3,lineheight=.9)
print(p)

```

# Model Comparison

We performed model comparisons for models 1, 2, 5, 6, 7, 8, 9, and 10 using leave-one-out (LOO) cross-validation (Vehtari, 2017). LOO model comparisons were performed in the full sample and within diagnostic groups to ensure that the full sample winning model was also the winning model within diagnostic groups.

In the tables below, models are presented in order from best- to worst-fitting based on the expected log pointwise predictive density, where higher ELPD values indicate better model fit. Differences in out-of-sample predictive accuracy were assessed for all models relative to the best fitting model based on changes in the ELPD (∆ELPD-LOO; 'delta_elpd' below) relative to the best fitting model. Uncertainty around ∆ELPD-LOO is captured through the standard error (SE; 'delta_elpd_se' below) of the estimated pointwise ELPD differences. We consider an absolute change in ELPD < 1 SE as weak evidence for improved predictive accuracy. Below, "pareto_k" columns indicate the number of observations in acceptable ("good" and "ok") versus problematic ranges ("bad" and "very bad"; Vehtari, 2022). Pareto K diagnostics flag issues with the integrity of the LOO approximation and can also signal problems with the model itself.

For all groups, the winning model (“Model 10” hereafter) was one in which all parameters varied by diagnostic group and evidence accumulation (drift rate) was influenced by gaze direction, head orientation, and emotion expression of stimuli. It assumed that response caution (threshold separation), start point (expectancy bias), and NDT operated as trait-level processes that did not vary in response to stimulus changes. This is a reasonable model account because the drift rate is influenced by the physical qualities of the stimulus. Model 10 showed improvements in predictive accuracy relative to the other models (i.e., ∆ELPD-LOO) that far exceeded the uncertainty of those estimates (i.e., |∆ELPD-LOO| was 5 to 105 times the SE). 

## Model Comparison (Full Sample)

First, we performed model comparisons on the full sample N=100 combined. Results showed that models perform better when the drift rate is allowed to vary by gaze direction (e.g., Model 2 vs 1) and head orientation (e.g., Model 9,10 vs others). The models also perform better when drift rate varies by gaze direction, head orientation, AND emotion (Model 10), versus when drift rate only varies by gaze direction and head orientation (Model 9). The difference between the best fitting model (Model 10) and the next best fitting model (Model 9) based on the difference in ELPD LOO ("delta_elpd" below) was over 5x greater than the SE of the estimate ("delta_elpd_se" below). So, the improvement from Model 9 to Model 10 was considerable. 

Additionally, we noted some difficulties when start point is allowed to vary by emotion (Model 7 and 8), which have some high Pareto K values.

```{r id14, echo=FALSE,cache=TRUE}

# Code below calls summary data for fitted parameter recovery simulations that were run using separate 'run_hddm_*.R' scripts for each model (available on OSF and GitHub)

################################################################################

# Prep data: Extract loglik values from giant (>40GB) stanfit or cmdstanfit .csvs and calculate LOO-CV (overall, by group, and by subject)

################################################################################
# note: To perform model comparisons, models 1,2,5,6,7,8,9,10 were run in Stan with 8000 TOTAL postwarmup draws. But due to computing demands, we needed to change packages (from Rstan to cmdstanr) and computing environments. As such there is some variability in how these were run (which requires slightly different prep of models). Because the models were determined to have converged and so the variability should not impact the samples themselves. 

################################################################################

models<-c('m1','m2','m5','m6','m7','m8','m9','m10') #add m10 once it's done running

### Note: First section is for models 1-7 (run in RStan), last section is for Model 8,9,10 (run in cmdstanr)
for (g in 1:length(models)){
  model<-models[g]
  if (model!='m8' && model!='m9' && model!='m10'){ #if model run in rstan
    
    # set PATH to parent directory
    outpath <- '/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_'
    modelpath<-paste(outpath,model,'/log_lik/loglik_stanfit.RData',sep="")
    the_loglikfile<-paste(outpath,model,'/log_lik/loglik_group1.RData',sep="")
    
    if (file.exists(the_loglikfile)){ #if loglik file exists, do nothing
    }else{ #if loglik file doesnt exist, process the data
      load(modelpath)
      
      fit<-modelfit$outputs$stanfit
      rm(modelfit)
      invisible(gc())
      
      #load real observed data
      real_data<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv")
      real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 
      real_data<-subset(real_data,select=c(group,subj)) #grab group and subj cols
      n_groups<-length(unique(real_data$group)) #get number of groups
      
      ####################################################################
      # do sample level loglik calculation (all subjects combined)
      ####################################################################
      
      all_loglik<-extract_log_lik(fit,parameter_name = "log_lik",merge_chains = FALSE) #all_loglik 
      rm(fit)
      invisible(gc())
      reff<-relative_eff(exp(all_loglik))
      filename<-paste(outpath,model,'/log_lik/loglik_all.RData',sep="")
      loo_1 <- loo(all_loglik, r_eff = reff, cores = 8)
      save(loo_1,file = filename)
      
      ####################################################################
      # do group level loglik calculation
      ####################################################################
      
      for (i in 1:n_groups){
        filename<-paste(outpath,model,'/log_lik/loglik_group',i,'.RData',sep="")
        # if extracted loglik +  loo file doesn't exist for this model + group, generate it
        if (file.exists(filename)){
        }else{
          group_ids<-which(real_data$group==i) #find  indices for current group
          group_loglik<-all_loglik[,,group_ids] #get loglik data for current group
          group_reff<-reff[group_ids] #get loglik data for current group
          loo_1 <- loo(group_loglik, r_eff = group_reff, cores = 8)
          save(loo_1,file = filename)
        }
        text<-paste("*LOO RESULTS FOR GROUP: ",i, sep="")
        print(text)
        print(loo_1)
      }
      
      ####################################################################
      # do loglik calculation at the subject level (only save summary info because the file is massive)
      ####################################################################
      
      subjs<-unique(real_data$subj)
      
      # setup subj loo data frame (summary vals only)
      loo_headers<-c('model','group','subj','looic','looic_se','delta_loo', 'pareto_k_good','pareto_k_ok','pareto_k_bad','pareto_k_verybad')
      subj_loo<-data.frame(matrix(data=NA,nrow=length(subjs),ncol=length(loo_headers)))
      colnames(subj_loo)<-loo_headers
      
      filename<-paste(outpath,model,'/log_lik/loglik_subj.RData',sep="")
      
      # if extracted subj level loglik +loo file doesn't exist, generate it
      if (file.exists(filename)){
      }else{
        for (i in 1:length(subjs)){
          subj_ids<-which(real_data$subj==subjs[i]) #find all indices for current subj
          subj_loglik<-all_loglik[,,subj_ids] #get loglik data for current subj
          subj_reff<-reff[subj_ids] #get loglik data for current subj
          loo_1 <- loo(subj_loglik, r_eff = subj_reff, cores = 8)
      
          #save summary info to running subj_loo file and save
          subj_loo$model[i]<-model
          subj_loo$group[i]<-unique(subset(real_data$group,real_data$subj==subjs[i]))#get subj's group
          subj_loo$subj[i]<-subjs[i]
          subj_loo$looic[i]<-round(data.frame(loo_1$estimates)[3,1],2)
          subj_loo$looic_se[i]<-round(data.frame(loo_1$estimates)[3,2],2)
          subj_loo$pareto_k_good[i]<-sum(loo_1$diagnostics$pareto_k < .5) #good
          subj_loo$pareto_k_ok[i]<-sum(loo_1$diagnostics$pareto_k >= .5 & loo_1$diagnostics$pareto_k < .7) #ok
          subj_loo$pareto_k_bad[i]<-sum(loo_1$diagnostics$pareto_k >= .7 & loo_1$diagnostics$pareto_k < 1)#bad
          subj_loo$pareto_k_verybad[i]<-sum(loo_1$diagnostics$pareto_k >= 1)#very bad
      
          #save
          save(subj_loo,file = filename)
          text<-paste("*LOO RESULTS FOR SUBJ: ",i, sep="")
          print(text)
          print(loo_1)
        }
      }
    }
    
  }else{ #if model run in cmdstanr  (model 8,9,10 only). Note: model 8 ran with 40 chains, 2500 warmup, 200 iterations/ model9/10 ran with 36 chains, 2500 warmup, 225 postwarmup. we account for this below
    
    if(model=='m8'){# if model 8, specify 40 chains and 200 postwarmup iterations
      chains=40
      iter=200 #40 chains, 2500 warmup, 200 postwarmup draws/chains = 8000 postwarmup iterations
    }else{ #if model 9 or 10, specify 36 chains and 225 postwarmup
      chains=36
      iter=225 
    }
    
    modelpath<-paste(outpath,model,'/log_lik/loglik_raw.RData',sep="")
    
    if(file.exists(modelpath)){ #if raw extracted loglik file exists, do nothing
    }else{ #otherwise, process it.
      
      fitdir<-paste("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_",model,"/log_lik/",sep="")
      setwd(fitdir)
      
      #read in all .csv files
      files <- (Sys.glob("*.csv"))
      
      for (i in 1:length(files)){
        print(i)
        tmpfile<-paste(fitdir,files[i],sep="")
        
        #only grab the first 200 rows of samples.
        tmpdata<-fread(tmpfile,skip=50,nrows=iter)
        
        if (i==1){
            alldata<-tmpdata
        }else{
          alldata<-rbind(alldata,tmpdata)
        }
      }
      
      myheaders<-read.csv(tmpfile,skip=45,nrows=1)
      my_colnames <- colnames(myheaders) 
      colnames(alldata)<-my_colnames
      
      #save combined cmdstan fit object to .RData file
      fitname<-paste(fitdir,"loglik_stanfit_cmdstan",".RData",sep = "") 
      save(alldata, file = fitname)
      
      #grab only log_lik cols
      cols<-grep("log_lik", colnames(alldata)) 
      col1<-min(cols)
      col2<-max(cols)
      alldata2<-data.frame(alldata)
      alldata2<-alldata2[,cols]
      
      fitname<-paste(fitdir,"loglik_raw",".RData",sep = "") 
      save(alldata2, file = fitname)
    }
    
    the_loglikfile<-paste(outpath,model,'/log_lik/loglik_group1.RData',sep="")
    
    if(file.exists(the_loglikfile)){ #if extracted loglik file exists, do nothing
    }else{#if extracted loglik file doesnt exist, process it now
      
      #loads raw loglik values in a matrix called "alldata2" (#iterations [8000] * n_observations [50062])
      load(modelpath)
      
      #generate chain IDs
      for (i in 1:chains){
        tmp_chainID<-rep(i,iter)
        if(i==1){
          chain_id<-tmp_chainID
        }else{
          chain_id<-c(chain_id,tmp_chainID)
        }
      }
      
      ####################################################################
      # do loglik calculation for all subjects combined
      ####################################################################
      
      r_eff<-relative_eff(as.matrix(exp(alldata2)),chain_id,cores = 8)
      loo_1<-loo(as.matrix(alldata2),r_eff = r_eff, cores = 8)
      filename<-paste(outpath,model,'/log_lik/loglik_all.RData',sep="")
      save(loo_1,file = filename)
      
      ####################################################################
      # do loglik calculation at the group level 
      ####################################################################
      
      #load real data
      real_data<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv")
      real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 
      real_data<-subset(real_data,select=c(group,subj)) #grab only group and subj cols
      n_groups<-length(unique(real_data$group)) #get number of groups
          
      for (i in 1:n_groups){
        filename<-paste(outpath,model,'/log_lik/loglik_group',i,'.RData',sep="")
        # if extracted loglik +  loo file doesn't exist for this model + group, generate it
        if (file.exists(filename)){
        }else{
          group_ids<-which(real_data$group==i) #find all indices for current group
          group_loglik<-alldata2[,group_ids] #get loglik data for current group
          group_reff<-r_eff[group_ids] #get loglik data for current group
          loo_1<-loo(as.matrix(group_loglik),r_eff = group_reff, cores = 8)
          save(loo_1,file = filename)
        }
      }
      
      ####################################################################
      # do loglik calculation at the subject level (only save summary info)
      ####################################################################
      subjs<-unique(real_data$subj)
      
      #setup subj loo data frame (summary vals only)
      loo_headers<-c('model','group','subj','looic','looic_se','delta_loo', 'pareto_k_good','pareto_k_ok','pareto_k_bad','pareto_k_verybad')
      subj_loo<-data.frame(matrix(data=NA,nrow=length(subjs),ncol=length(loo_headers)))
      colnames(subj_loo)<-loo_headers
      
      filename<-paste(outpath,model,'/log_lik/loglik_subj.RData',sep="")
      
      # if extracted subj level loglik +loo file doesn't exist for model, generate it
      if (file.exists(filename)){
      }else{
        for (i in 1:length(subjs)){
          subj_ids<-which(real_data$subj==subjs[i]) #find all indices for current subj
          subj_loglik<-alldata2[,subj_ids] #get loglik data for current subj
          subj_reff<-r_eff[subj_ids] #get loglik data for current subj
          loo_1<-loo(as.matrix(subj_loglik),r_eff = subj_reff, cores = 8)
          #save summary info to running subj_loo file and save
          subj_loo$model[i]<-model
          subj_loo$group[i]<-unique(subset(real_data$group,real_data$subj==subjs[i]))#get subj's group
          subj_loo$subj[i]<-subjs[i]
          subj_loo$looic[i]<-round(data.frame(loo_1$estimates)[3,1],2)
          subj_loo$looic_se[i]<-round(data.frame(loo_1$estimates)[3,2],2)
          subj_loo$pareto_k_good[i]<-sum(loo_1$diagnostics$pareto_k < .5) #good
          subj_loo$pareto_k_ok[i]<-sum(loo_1$diagnostics$pareto_k >= .5 & loo_1$diagnostics$pareto_k < .7) #ok
          subj_loo$pareto_k_bad[i]<-sum(loo_1$diagnostics$pareto_k >= .7 & loo_1$diagnostics$pareto_k < 1)#bad
          subj_loo$pareto_k_verybad[i]<-sum(loo_1$diagnostics$pareto_k >= 1)#very bad
      
          #save
          save(subj_loo,file = filename)
        }
      }
    }
  }
}

```

```{r id15, echo=FALSE, fig.height=6, fig.show="hold", fig.width=11, message=FALSE, warning=FALSE, dpi=500, out.width='1500px', tidy=T,cache=TRUE}

# set PATH to parent directory
outpath <- '/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_'
models<-c('m1','m2','m5','m6','m7','m8','m9','m10')
loo_headers<-c('model','looic','looic_se','elpd_loo','elpd_loo_se','delta_elpd','delta_elpd_se', "pareto_k_good","pareto_k_ok","pareto_k_bad","pareto_k_verybad")
all_loo<-data.frame(matrix(data=NA,nrow=length(models),ncol=length(loo_headers)))
colnames(all_loo)<-loo_headers

# Pareto k diagnostic values:
#                         
# (-Inf, 0.5]   (good)         
#  (0.5, 0.7]   (ok)             
#    (0.7, 1]   (bad)    
#    (1, Inf)   (very bad)    

all_loo_objects<-list() #list to hold loo_psis objects for each model

for (i in 1:length(models)){
  load(paste(outpath,models[i],'/log_lik/loglik_all.RData',sep=""))
  all_loo$model[i]<-models[i]
  all_loo_objects[[models[i]]]<-loo_1
  all_loo$looic[i]<-round(data.frame(loo_1$estimates)[3,1],2)
  all_loo$looic_se[i]<-round(data.frame(loo_1$estimates)[3,2],2)
  all_loo$elpd_loo[i]<-round(data.frame(loo_1$estimates)[1,1],2)
  all_loo$elpd_loo_se[i]<-round(data.frame(loo_1$estimates)[1,2],2)
  all_loo$pareto_k_good[i]<-sum(loo_1$diagnostics$pareto_k < .5) #good 
  all_loo$pareto_k_ok[i]<-sum(loo_1$diagnostics$pareto_k >= .5 & loo_1$diagnostics$pareto_k < .7) #ok 
  all_loo$pareto_k_bad[i]<-sum(loo_1$diagnostics$pareto_k >= .7 & loo_1$diagnostics$pareto_k < 1)#bad 
  all_loo$pareto_k_verybad[i]<-sum(loo_1$diagnostics$pareto_k >= 1)#very bad 
}

#sort table by from lowest to highest loo val
all_loo <- all_loo[order(all_loo$looic,decreasing=FALSE),]

#calculate elpd loo diff 
for (i in 1:length(models)){
  
  tmp_model<-all_loo$model[i] #current model
  tmp_loo_object<-all_loo_objects[[tmp_model]]
  
  best_model<-all_loo$model[1] #best fitting model
  best_loo_object<-all_loo_objects[[best_model]]
  
  if (i > 1){
    compare<-data.frame(loo_compare(tmp_loo_object,best_loo_object))
    all_loo$delta_elpd[i]<-round(compare$elpd_diff[2],2)
    all_loo$delta_elpd_se[i]<-round(compare$se_diff[2],2)
    
  }else{
    all_loo$delta_elpd[i]<-"--"
    all_loo$delta_elpd_se[i]<-"--"
  }
}

#print 
rownames(all_loo)<-NULL

all_loo %>%
  kbl(caption = "Model Comparison in the Full Sample",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

rm(list=c('loo_1'))
invisible(gc())

```

## Model Comparison (by Group)

Second, we ensure that the model comparison results from the full sample hold within diagnostic groups. Results within groups show the same pattern of results: Model 10 is the best fitting model within SZ, BD, and HC. The improvement from Model 9 to Model 10 based on changes in LOO ELPD exceeded the SE of the differences for all groups, indicating Model 10 offered a considerably better account of the data. Based on these results, it was clear that Model 10 offered the best fit of the data based on LOO model comparisons.

```{r id16, echo=FALSE, fig.height=6, fig.show="hold", fig.width=11, message=FALSE, warning=FALSE, dpi=500, out.width='1500px', tidy=T,cache=TRUE}

# set PATH to parent directory
outpath <- '/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_'
models<-c('m1','m2','m5','m6','m7','m8','m9','m10')
groups<-c('hc','bd','sz')
loo_headers<-c('group','model','looic','looic_se','elpd_loo','elpd_loo_se','delta_elpd','delta_elpd_se', "pareto_k_good","pareto_k_ok","pareto_k_bad","pareto_k_verybad")
group_loo<-data.frame(matrix(data=NA,nrow=length(models),ncol=length(loo_headers)))
colnames(group_loo)<-loo_headers

# Pareto k diagnostic values:
# (-Inf, 0.5]   (good)         
#  (0.5, 0.7]   (ok)             
#    (0.7, 1]   (bad)    
#    (1, Inf)   (very bad)    

group_loo_objects<-list() #list to hold loo_psis objects for each model

for (h in 1:length(groups)){
  for (i in 1:length(models)){
    load(paste(outpath,models[i],'/log_lik/loglik_group',h,'.RData',sep=""))
    group_loo$group[i]<-groups[h]
    group_loo$model[i]<-models[i]
    group_loo_objects[[models[i]]]<-loo_1
    group_loo$looic[i]<-round(data.frame(loo_1$estimates)[3,1],2)
    group_loo$looic_se[i]<-round(data.frame(loo_1$estimates)[3,2],2)
    group_loo$elpd_loo[i]<-round(data.frame(loo_1$estimates)[1,1],2)
    group_loo$elpd_loo_se[i]<-round(data.frame(loo_1$estimates)[1,2],2)
    group_loo$pareto_k_good[i]<-sum(loo_1$diagnostics$pareto_k < .5) #good 
    group_loo$pareto_k_ok[i]<-sum(loo_1$diagnostics$pareto_k >= .5 & loo_1$diagnostics$pareto_k < .7) #ok 
    group_loo$pareto_k_bad[i]<-sum(loo_1$diagnostics$pareto_k >= .7 & loo_1$diagnostics$pareto_k < 1)#bad 
    group_loo$pareto_k_verybad[i]<-sum(loo_1$diagnostics$pareto_k >= 1)#very bad 
  }
  
  #sort table by from lowest to highest loo val
  group_loo <- group_loo[order(group_loo$looic,decreasing=FALSE),]
  
  #calculate elpd loo diff 
  for (j in 1:length(models)){
    
    tmp_model<-group_loo$model[j] #current model
    tmp_loo_object<-group_loo_objects[[tmp_model]]
    
    best_model<-group_loo$model[1] #best fitting model
    best_loo_object<-group_loo_objects[[best_model]]
    
    if (j > 1){
      compare<-data.frame(loo_compare(tmp_loo_object,best_loo_object))
      group_loo$delta_elpd[j]<-round(compare$elpd_diff[2],2)
      group_loo$delta_elpd_se[j]<-round(compare$se_diff[2],2)
      
    }else{
      group_loo$delta_elpd[j]<-"--"
      group_loo$delta_elpd_se[j]<-"--"
    }
  }
  
  #append to larger table
  if (h==1){
    all_group_loo<-group_loo
  }else{
    all_group_loo<-rbind(all_group_loo,group_loo)
  }
}

#print table
rownames(all_group_loo)<-NULL
all_group_loo$group<-""
all_group_loo %>%
  kbl(caption = "Model Comparison by Group",valign = "t") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  pack_rows("HC", 1, 8)%>%
  pack_rows("BD", 9, 16)%>%
  pack_rows("SZ", 17, 24)

rm(list=c('loo_1'))
invisible(gc())

```

# Confusion Matrix

We further evaluated whether it was possible to arbitrate between these 8 models (1, 2, 5, 6, 7, 8, 9, and 10) by generating a confusion matrix. This allowed us to determine whether data generated by each model could be identified as being best fit by the model that generated it.

We simulated 50 data sets using each of these 8 models, fit all 8 models to each simulated data set, generated LOO values for each model fit, and identified the best fitting model for each simulated data set (i.e., model with the lowest LOO value). Simulated data was generated using the same procedure described above for Parameter Recovery. Because of the computational resources this process demands, we took steps to reduce the overall run time: 1) we simulated only N=20 subjects for each data set; 2) we reduced the warm up/post warmup samples (500 warm up, 1000 post warm up over multiple chains); 3) we fixed NDT at 0.2; and 4) we performed this process in HC only (i.e., HC posterior values were used to generate simulated data).

Results are summarized below. This confusion matrix indicates the proportion of the N=50 data sets simulated using a given model (rows) were best fit by each of the 8 models (columns), where values in rows sum to 1. Ideally, the diagonal elements should equal 1 and off-diagonal elements should equal 0, meaning that the model that generated the data always fits the data best. Results show that data generated by Model 10---the best fitting model based on LOO model comparisons---was best fit by Model 10 in 98% of simulations. The same was true of Model 9---the second best-fitting model based on LOO model comparisons. There was more difficulty arbitrating between simpler models, especially Models 5, 7 and 8. It is noteworthy that for Models 7 and 8 (which allowed start point to vary based on emotion) we also noted sampling difficulties at other model evaluation steps. This further corroborates our decision to not explore those models further. Together, these results indicate that it was possible to arbitrate between these two best-fitting models, including Model 10--the winning model based on LOO model comparisons.

```{r id141, echo=FALSE,cache=TRUE, fig.height=4.5, fig.width=5,warning=FALSE,cache=TRUE}

models<-c("m1","m2","m5","m6","m7","m8","m9","m10")

# for (i in 1:length(models)){ #gen model
#   for(j in 1:length(models)){ #fit model
#     tmp_file<-paste("/N/slate/clasagn/gazeddm/output/hddm_",models[i],"_confusion_matrices/confusion_matrices/hddm_",models[j],"_confusion_matrices_fit/group1/confMatrices_groupLvl_summary.csv",sep="")
#     tmp_summary<-read.csv(tmp_file)
#     if(i==1&j==1){
#       all_summary<-tmp_summary
#     }else{
#       all_summary<-rbind(all_summary,tmp_summary)
#     }
#   }
# }
# counts<-as.data.frame(table(all_summary$fitmodel, all_summary$datasimmodel))
# #counts
# 
# outfile<-paste("/N/slate/clasagn/gazeddm/output/","confusion_matrices_summary.csv",sep="")
# write.csv(all_summary,outfile,row.names=FALSE)

confMat_file<-paste('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/','confusion_matrices_summary.csv',sep="")
all_summary<-read.csv(confMat_file)

#do confusion matrices
sims<-50

conf_mat<-as.data.frame(matrix(data=0,ncol=length(models),nrow=length(models)))
rownames(conf_mat)<-paste(models,"_gen",sep="")
colnames(conf_mat)<-paste(models,"",sep="")

for (i in 1:sims){
  for (j in 1:length(models)){ #gen model
    gen_model<-paste("hddm_",models[j],"_confusion_matrices",sep="")
    
    #get data for this gen model and this sim
    tmp_data<-subset(all_summary,all_summary$datasimmodel==gen_model&all_summary$sim==i)
    
    #find model with lowest looic value -- note other model comparisons were based 
    # on elpd loo but looic is just a linear transform of elpd so it gives the same result
    best_fit_model<-tmp_data$fitmodel[which.min(tmp_data$looic)] 
    best_fit_model_clean <- paste(gsub(".*(m(1|2|5|6|7|8|9|10)).*", "\\1", best_fit_model),"",sep="")
    conf_mat[[best_fit_model_clean]][j]<-conf_mat[[best_fit_model_clean]][j]+1
  }
}

conf_mat<-conf_mat/sims #convert to proportion
gen_mod<-data.frame(gen_mod=factor(models,levels=models))
colnames(conf_mat)<-models
conf_mat<-cbind(gen_mod,conf_mat)

# Convert dataframe to long format
df_long <- melt(conf_mat,id="gen_mod",varnames = c("gen_mod","value"))
colnames(df_long)<-c("gen_mod","fit_mod","Proportion")

# Create the plot
p<-ggplot(df_long, aes(x = fit_mod, y = gen_mod)) +
  geom_tile(aes(fill = Proportion), color = "black") +
  geom_text(aes(label = Proportion), color = "black") +
  scale_fill_gradient2(low = "white",high = "firebrick", midpoint = mean(df_long$Proportion), na.value = "transparent", guide = "legend") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Confusion Matrix\n(p fitted model | simulated model)",
       x="Fitted Model",y="Simulated Model",
       fill = "Proportion\nBest Fit")
print(p)

```

# Posterior Predictions

After model comparisons and confusion matrices above, it Model 10 was the best performing model. Before selecting it as our winning model to be used for analyses, we subjected Model 10 to additional posterior predictive checks. 

As such, Models 10 was run in cmdstanr with a larger number of samples to use to perform posterior predictive checks (i.e., 36 chains, 2500 warmup, and 2000 post warmup draws, resulting in 72,000 post warmup samples). Using means and SD's of posteriors for all parameters, we used an estimation-based method to generate predicted RT quantiles and predicted choice proportions for all task conditions, for all diagnostic groups.

```{r id18, echo=FALSE,cache=TRUE}

# Prep Data: loop through cmdstanr .csv files and combine into single data frame (postwarmup draws x # pars) where #postwarmup draws = rows of all draws from chain 1 (2000 rows), then all draws from chain 2 and so-on)

warmup<-2500
iter<-2000
models<-c('m10')
outpath<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/"

for (i in 1:length(models)){
  model<-paste("hddm_",models[i],sep="")
  the_fitfile<-paste(outpath,model,'/final_fit/stanfit_cmdstan.RData',sep="")
  if(file.exists(the_fitfile)){ #if extracted stanfit file exists, do nothing
  }else{
    fitdir<-paste(outpath,model,'/final_fit/',sep="")
    setwd(fitdir)

    #read in all .csv files
    files <- (Sys.glob("*.csv"))
    skip_rows<-50+warmup #csvs for m9 and m10 included 2500 warmup so we have to exclude those + initial 50 rows of cmdstan comments
    
    for (j in 1:length(files)){
      tmpfile<-paste(fitdir,files[j],sep="")
      tmpdata<-fread(tmpfile,skip=skip_rows,nrows=iter)
      if (j==1){
          alldata<-tmpdata
      }else{
        alldata<-rbind(alldata,tmpdata)
      }
    }
    
    myheaders<-read.csv(tmpfile,skip=45,nrows=1)
    my_colnames <- colnames(myheaders) 
    colnames(alldata)<-my_colnames
    
    #save combined cmdstan fit object to .RData file
    fitname<-paste(fitdir,"stanfit_cmdstan",".RData",sep = "") 
    save(alldata, file = fitname)
  }
}

```

## Predicted RT Quantiles (Model 10)

Results show that the predicted RT quantiles (data in pink below) for Model 10 matched the observed data well (data in teal below). This was true for all 3 groups in all conditions (left panel below) and when we marginalized over the emotion condition (right panel below). The exception was cases in which there were a lower number of trials (e.g., indrect/forward/neutral/acc=0). This indicates the model is capable of making accurate predictions about RT Quantiles.

Note: Posterior predicted RT quantiles were similarly accurate when we marginalized over the emotion condition. However, we do not include these below for brevity.

```{r id24, echo=FALSE, fig.height=3.5, fig.width=4, message=FALSE, warning=FALSE,cache=TRUE}

estimated_quantile_file<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/ppc/quantile_estimates_hdi.RData"

#load real observed data
real_data<-read.csv('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv',header=TRUE) #load real observed data
real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 

quantiles<-c(.1,.3,.5,.7,.9)
groups<-unique(real_data$group)
groups_names<-c("HC","BD","SZ")
subjs<-unique(real_data$subjID)
gaze_conds<-unique(real_data$cond)
gaze_conds_names<-c("direct","indirect")
emo_conds<-unique(real_data$cond_other_emo)
emo_conds_names<-c("neutral","fearful")
head_conds<-unique(real_data$cond_other_head)
head_conds_names<-c("forward","deviated")
emohead_conds<-unique(real_data$cond_other_emo_head)
emohead_conds_names<-c("forward_neutral","forward_fearful","deviated_neutral","deviated_fearful")
acc<-unique(real_data$acc)
choices<-unique(real_data$choice)
n_obs<-dim(real_data)[1]
cores<-10

if(file.exists(estimated_quantile_file)){ #if file exists,load it
  load(estimated_quantile_file)
}else{ #if file doesn't exist, run code to estimate quantiles

  load("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/stanfit_cmdstan.RData") #load posterior samples
  posterior<-data.frame(alldata)
  iterations=dim(posterior)[1] #number of posterwarmup draws
  
  rm(alldata)
  invisible(gc())
  
  #setup cluster on computer for parallel processing
  cluster <- makeCluster(cores) #Leave one core to avoid overload your computer
  registerDoParallel(cluster)

  # get subj level variables (group and minRT) and subj*cond*choice level varaibles (ie how many trials each subject has for each gaze condition, choice)
  for (i in 1:length(subjs)){
    tmpdata<-subset(real_data,real_data$subjID==i)
    #get min Rt and group for this subj
    tmp_subj_vars<-data.frame(subj=i,group=unique((tmpdata$group)),minRT=min(tmpdata$rt))
    if (i==1){
      subj_vars<-tmp_subj_vars
    }else{
      subj_vars<-rbind(subj_vars,tmp_subj_vars)
    }
    for (j in 1:length(gaze_conds)){
      for (k in 1:length(emohead_conds)){
        for (l in 1:length(choices)){
          tmpdata<-dim(subset(real_data,real_data$subjID==i & real_data$cond == j & real_data$other_cond_emo_head == k & real_data$choice == l))[1]
          tmp_cond_vars<-data.frame(subj=i,gaze_cond=j,emohead_cond=k,choice=l,trials=tmpdata)
          if (i==1 & j==1 & k==1 & l==1){
            cond_vars<-tmp_cond_vars
          }else{
            cond_vars<-rbind(cond_vars,tmp_cond_vars)
          }
        }
      }
    }
  }

  #get group level vars
  for (i in 1:length(groups)){
    tmpdata<-subset(real_data,real_data$group==i)
    tmp_group_vars<-data.frame(group=i,maxRT=max(tmpdata$rt))
    if (i==1){
      group_vars<-tmp_group_vars
    }else{
      group_vars<-rbind(group_vars,tmp_group_vars)
    }
  }

  #grab rand draws to subsample from full posterior to make the file size more managable
  set.seed(42)

  # prepare the posterior samples
  ## subject parameters
  sub_alpha_pr_samples<-posterior[,c(grep("sub_alpha_pr", colnames(posterior)))]
  sub_beta_pr_samples<-posterior[,c(grep("sub_beta_pr", colnames(posterior)))]
  sub_delta_present1_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,1:100]
  sub_delta_present2_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,101:200]
  sub_delta_present3_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,201:300]
  sub_delta_present4_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,301:400]
  sub_delta_absent1_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,1:100]
  sub_delta_absent2_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,101:200]
  sub_delta_absent3_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,201:300]
  sub_delta_absent4_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,301:400]
  sub_ndt_pr_samples<-posterior[,c(grep("sub_ndt_pr", colnames(posterior)))]

  ## group parameters (means)
  mu_grp_alpha_pr_samples<-posterior[,c(grep("mu_grp_alpha_pr", colnames(posterior)))]
  mu_grp_beta_pr_samples<-posterior[,c(grep("mu_grp_beta_pr", colnames(posterior)))]
  mu_grp_delta_present_pr_samples<-posterior[,c(grep("mu_grp_delta_present_pr", colnames(posterior)))]
  mu_grp_delta_absent_pr_samples<-posterior[,c(grep("mu_grp_delta_absent_pr", colnames(posterior)))]
  mu_grp_ndt_pr_samples<-posterior[,c(grep("mu_grp_ndt_pr", colnames(posterior)))]

  ## group parameters (variances)
  sig_grp_alpha_pr_samples<-posterior[,c(grep("sig_grp_alpha_pr", colnames(posterior)))]
  sig_grp_beta_pr_samples<-posterior[,c(grep("sig_grp_beta_pr", colnames(posterior)))]
  sig_grp_delta_pr_samples<-posterior[,c(grep("sig_grp_delta_pr", colnames(posterior)))]
  sig_grp_ndt_pr_samples<-posterior[,c(grep("sig_grp_ndt_pr", colnames(posterior)))]

  ## other condition parameters (group level effect; 2x emo conditions)
  cond_delta1_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,1:3]
  cond_delta2_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,4:6]
  cond_delta3_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,7:9]
  cond_delta4_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,10:12]

  subsample<-1000 #how many draws should we randomly subsample from the full posterior

  #create empty array for rt quantiles: #obs x #subsampled iterations x #quantiles
  estim_quantiles <- array(numeric(),c(n_obs,subsample,length(quantiles)))
  #create empty array for predicted choice proportions: #obs x #subsampled iterations
  estim_choice_props <- array(numeric(),c(n_obs,subsample))
  
  for (k in 1:n_obs){
    tmp_subj<-real_data$subjID[k]
    tmp_group<-real_data$group[k]
    tmp_gazecond<-real_data$cond[k]
    tmp_emoheadcond<-real_data$cond_other_emo_head[k]
    tmp_choice<-real_data$choice[k]
    tmp_minRT<-subj_vars$minRT[tmp_subj] #get this subj's min RT

    #rand subsample
    rand_draws<-sample(seq(from=1,to=iterations,by=1),subsample)

    #randomly draw group mean, variance values
    tmp_mu_grp_alpha_pr<-as.numeric(unlist(mu_grp_alpha_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_beta_pr<-as.numeric(unlist(mu_grp_beta_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_delta_present_pr<-as.numeric(unlist(mu_grp_delta_present_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_delta_absent_pr<-as.numeric(unlist(mu_grp_delta_absent_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_ndt_pr<-as.numeric(unlist(mu_grp_ndt_pr_samples[rand_draws,tmp_group]))

    tmp_sig_grp_alpha_pr<-as.numeric(unlist(sig_grp_alpha_pr_samples[rand_draws,tmp_group]))
    tmp_sig_grp_beta_pr<-as.numeric(unlist(sig_grp_beta_pr_samples[rand_draws,tmp_group]))
    tmp_sig_grp_delta_pr<-as.numeric(unlist(sig_grp_delta_pr_samples[rand_draws,tmp_group]))
    tmp_sig_grp_ndt_pr<-as.numeric(unlist(sig_grp_ndt_pr_samples[rand_draws,tmp_group]))

    # randomly draw condition level values
    tmp_cond_delta1_pr<-as.numeric(unlist(cond_delta1_pr_samples[rand_draws,tmp_group]))
    tmp_cond_delta2_pr<-as.numeric(unlist(cond_delta2_pr_samples[rand_draws,tmp_group]))
    tmp_cond_delta3_pr<-as.numeric(unlist(cond_delta3_pr_samples[rand_draws,tmp_group]))
    tmp_cond_delta4_pr<-as.numeric(unlist(cond_delta4_pr_samples[rand_draws,tmp_group]))

    #draw subj level values
    tmp_sub_alpha_pr<-as.numeric(unlist(sub_alpha_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_beta_pr<-as.numeric(unlist(sub_beta_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present1_pr<-as.numeric(unlist(sub_delta_present1_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present2_pr<-as.numeric(unlist(sub_delta_present2_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present3_pr<-as.numeric(unlist(sub_delta_present3_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present4_pr<-as.numeric(unlist(sub_delta_present4_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent1_pr<-as.numeric(unlist(sub_delta_absent1_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent2_pr<-as.numeric(unlist(sub_delta_absent2_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent3_pr<-as.numeric(unlist(sub_delta_absent3_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent4_pr<-as.numeric(unlist(sub_delta_absent4_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_ndt_pr<-as.numeric(unlist(sub_ndt_pr_samples[rand_draws,tmp_subj]))

    # do transformations for non centered parameterization (scale raw subject par by group mean and variance)
    tmp_alpha<-pnorm(tmp_mu_grp_alpha_pr + tmp_sub_alpha_pr*tmp_sig_grp_alpha_pr)*3.9+0.1
    tmp_beta<-pnorm(tmp_mu_grp_beta_pr + tmp_sub_beta_pr*tmp_sig_grp_beta_pr)
    tmp_delta_present1<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present1_pr*tmp_sig_grp_delta_pr+tmp_cond_delta1_pr)*8-4
    tmp_delta_present2<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present2_pr*tmp_sig_grp_delta_pr+tmp_cond_delta2_pr)*8-4
    tmp_delta_present3<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present3_pr*tmp_sig_grp_delta_pr+tmp_cond_delta3_pr)*8-4
    tmp_delta_present4<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present4_pr*tmp_sig_grp_delta_pr+tmp_cond_delta4_pr)*8-4
    tmp_delta_absent1<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent1_pr*tmp_sig_grp_delta_pr+tmp_cond_delta1_pr)*8-4
    tmp_delta_absent2<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent2_pr*tmp_sig_grp_delta_pr+tmp_cond_delta2_pr)*8-4
    tmp_delta_absent3<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent3_pr*tmp_sig_grp_delta_pr+tmp_cond_delta3_pr)*8-4
    tmp_delta_absent4<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent4_pr*tmp_sig_grp_delta_pr+tmp_cond_delta4_pr)*8-4
    tmp_ndt<-(pnorm(tmp_mu_grp_ndt_pr + tmp_sub_ndt_pr*tmp_sig_grp_ndt_pr)*tmp_minRT*0.9)/1000

    if(tmp_gazecond==1){#if gaze=direct
      if(tmp_choice==1){#if choice =YES (correct)
        resp<-"upper" #will use this variable for RWiener cdf function (pwiener) below)
        tmp_beta1<-tmp_beta
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_present1
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_present2
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_present3
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_present4
        }
      }else{#if choice=NO (incorrect)
        resp<-"lower"
        tmp_beta1<-tmp_beta #im deliberately NOT flipping the beta here (1-beta) like i would usually bc qwiener func does this internally based on the resp specified
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_present1 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_present2 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_present3 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_present4 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }
      }
    }else{ #if gaze=indirect
      if(tmp_choice==1){#if choice=YES (incorrect)
        resp<-"upper"
        tmp_beta1<-tmp_beta
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_absent1 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_absent2
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_absent3
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_absent4
        }
      }else{#if choice=NO (correct)
        resp<-"lower"
        tmp_beta1<-tmp_beta #im deliberately NOT flipping the beta here (1-beta) like i would usually bc qwiener func does this internally based on the resp specified
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_absent1 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_absent2
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_absent3
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_absent4
        }
      }
    }

    #define some variables that we will use to calculate the choice probs below
    sigma<-1 #diffusion coefficient. fixed at 1

    qc<-foreach(l=1:length(tmp_alpha),.combine='rbind',.packages = c("RWiener","dplyr")) %dopar% {

      #calculate choice probabilities (needed to dead with defective CDF)
      ## calc absolute start point
      z<-tmp_alpha[l]*tmp_beta1[l]

      boundary<-1 #1=upper, 0=lower
      m<-1 #iteration tracker
      tinc<-.01 #time increment
      t<-c(tinc+tmp_ndt[l]) #ndt plus some tolerance
      looking<-1 #status of routine within while statement
      pt<-c(NA)

      ## calculate choice probabilities for current trial, parameters (whether it's upper or lower depends on observed choice of current trial)
      if(tmp_choice==1){#if upper bound response, calculate prob of upper bound resp
        tmp_choice_prob <- (exp(2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2) - exp(2*tmp_delta_cond[l]*(tmp_alpha[l]-z)/sigma^2))/(exp(2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2)-1)
      }else{#if lower bound response, calculate prob of lower bound resp
        tmp_choice_prob <- (exp(-2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2) - exp(-2*tmp_delta_cond[l]*z/sigma^2))/(exp(-2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2)-1)
      }

      #loop thru all samples, calculating + fixing defective cdf, and terminating when prob from fixed CDF exceeds .9 + .01
      while (looking==1){
        #calculate DEFECTIVE cdf for the response given on this trial

        #im deliberately making rt here tinc+tmp_ndt bc the rwiener function subtracts out the ndt from the rt for us
        pt[m]<-pwiener(t[m],tmp_alpha[l],tmp_ndt[l],tmp_beta1[l],tmp_delta_cond[l],resp)
        #fix the defective CDF (by dividing current prob value by the prob of the current choice
        pt[m]<-(pt[m])/tmp_choice_prob
        test<-as.numeric(pt>max(quantiles)+.01)
        looking<-as.numeric(sum(test)<2)
        m<-m+1
        t[m]<-t[m-1]+tinc
      }
      qc<-c(NA)
      for (n in 1:length(quantiles)){
        o <- last(which(pt<=quantiles[n]))
        if (length(o)>0){#if it's not empty
          qc[n] <- (t[o+1] - t[o])*(quantiles[n]-pt[o])/(pt[o+1]-pt[o]) + t[o]
        }else{
          qc[n] <- tmp_ndt[l]
        }
      }
      return(qc)
    }

    estim_quantiles[k,,]<-qc

    #save quantile estimates every 1000 observations (in case things crash)
    if(as.numeric(endsWith(as.character(unlist(k)), "000"))==1){
      fitdir<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/ppc/"
      fitname<-paste(fitdir,"quantile_estimates_hdi",".RData",sep = "")
      save(estim_quantiles, file = fitname)
      print("saving data processed to this point")
    }
    invisible(gc())
    print(k)
  }

  # save full estimated quantile data to .RData file
  fitdir<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/ppc/"
  fitname<-paste(fitdir,"quantile_estimates_hdi",".RData",sep = "")
  save(estim_quantiles, file = fitname)

  parallel::stopCluster(cl = cluster)

}

###############################################################################
# average predictions across subjects for each condition, choice, acc
###############################################################################

subsample<-1000

#version1: plots across all conds (gaze, emo, head) separately
for (j in 1:length(gaze_conds)){
  for (k in 1:length(emohead_conds)){
    for (l in 1:length(acc)){
      for (m in 1:length(groups)){
        
        #get list of subjects in this group
        current_subj_list<-unique(subset(real_data,real_data$group==groups[m])$subjID)
        
        #create empty array: #obs x #iterations x #quantiles
        estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
        estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
        
        for (n in 1:length(current_subj_list)){
          
          #get real quantiles for this subj, condition,acc
          tmp_realdata<-subset(real_data,real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$acc==(l-1))
          
          estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$rt,type=8)
          
          #get predicted quantiles (means) and HDI's
          
          ## get cols from n_obs that correspond to current cond, subj, acc
          tmp_quant_cols<-which(real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$acc==(l-1))
          #use those to select observations from the predicted quantiles array
          tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
          
          #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
          if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
            estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
          }else if(length(tmp_quant_cols)==1){
            estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
          }else{ # if subj has 2 or more trials for this cond
            estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
          }
        }
        
        for (o in 1:length(quantiles)){
          
          tmp_hdi<-HDIofMCMC(estim_pred_quantiles_over_subjs[,,o])
          tmp_hdi<-data.frame(quantile=quantiles[o],
                              real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                              pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE)*1000,
                              hdi_lo=tmp_hdi[1]*1000,
                              hdi_hi=tmp_hdi[2]*1000)
          if(o==1){
            all_hdi<-tmp_hdi
          }else{
            all_hdi<-rbind(all_hdi,tmp_hdi)
          }
        }
        
        #setup data in long form
        quantile_data<-data.frame(
          type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
          quantile=c(quantiles,quantiles), # quantiles
          rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
          hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
          hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
        
        quantile_data$group<-groups_names[m]
        
        if(m==1){#if group 1
          grp_quantile_data<-quantile_data
          grp1_trials<-dim(subset(real_data,real_data$group==m & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$acc==(l-1)))[1]
        }else if(m==2){#if group 2
          tmp_grp_quantile_data<-quantile_data
          tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
          grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
          grp2_trials<-dim(subset(real_data,real_data$group==m & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$acc==(l-1)))[1]
        }else{#if group 3
          tmp_grp_quantile_data<-quantile_data
          tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.06 #shift slightly to right for plotting
          grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
          grp3_trials<-dim(subset(real_data,real_data$group==m & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$acc==(l-1)))[1]
        }
      }
      
      grp_quantile_data$group<-as.factor(grp_quantile_data$group)
      title<-paste("Model 10 quantile predictions:\n",gaze_conds_names[j],": ",emohead_conds_names[k],"\nacc=",(l-1),sep="")
      trials_text<-paste("Trials per group:\nHC=",grp1_trials," BD=",grp2_trials," SZ=",grp3_trials,sep="")
      
      #plot quantiles (xaxis against all quantiles colored by "types", shapes vary by group)
      plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
        geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
        geom_point(alpha=.7,aes(shape=group)) +
        ggtitle(title) +
        ylab("RT") +
        xlab("Quantile")+
        theme(plot.title = element_text(size=10),axis.text.y=element_text(size=7))+ 
        coord_cartesian(ylim = c(0,2500))+ annotate(geom = "text", x = .5, y = 2450, 
                                              label = trials_text, 
                                              hjust = "center",
                                              size=3.5,lineheight=.9)
      print(plot)
      
    }
  }
}

# version 2: plot by gaze and head conds but marginalizes over emotion conds 
for (j in 1:length(gaze_conds)){
  for (k in 1:length(head_conds)){
    for (l in 1:length(acc)){
      for (m in 1:length(groups)){
        
        #get list of subjects in this group
        current_subj_list<-unique(subset(real_data,real_data$group==groups[m])$subjID)
        
        #create empty array: #obs x #iterations x #quantiles
        estim_pred_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),subsample,length(quantiles))) 
        estim_real_quantiles_over_subjs <- array(numeric(),c(length(current_subj_list),length(quantiles))) 
        
        for (n in 1:length(current_subj_list)){
          
          #get real quantiles for this subj, condition,acc
          tmp_realdata<-subset(real_data,real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_head==k & real_data$acc==(l-1))
          
          estim_real_quantiles_over_subjs[n,]<-quantile(tmp_realdata$rt,type=8)
          
          #get predicted quantiles (means) and HDI's
          
          ## get cols from n_obs that correspond to current cond, subj, acc
          tmp_quant_cols<-which(real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_head==k & real_data$acc==(l-1))
          #use those to select observations from the predicted quantiles array
          tmp_quantile_est<-estim_quantiles[tmp_quant_cols,,]
          
          #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
          if(length(tmp_quant_cols)==0){ #if no trials for current condition for current subject, create array of NA
            estim_pred_quantiles_over_subjs[n,,]<-matrix(NA,nrow=subsample,ncol=length(quantiles))
          }else if(length(tmp_quant_cols)==1){
            estim_pred_quantiles_over_subjs[n,,]<-tmp_quantile_est
          }else{ # if subj has 2 or more trials for this cond
            estim_pred_quantiles_over_subjs[n,,]<-apply(tmp_quantile_est, c(2,3), mean,na.rm=TRUE) 
          }
        }
        
        for (o in 1:length(quantiles)){
          
          tmp_hdi<-HDIofMCMC(estim_pred_quantiles_over_subjs[,,o])
          tmp_hdi<-data.frame(quantile=quantiles[o],
                              real_quantile_mean=mean(estim_real_quantiles_over_subjs[,o],na.rm = TRUE),
                              pred_quantile_mean=mean(estim_pred_quantiles_over_subjs[,,o],na.rm = TRUE)*1000,
                              hdi_lo=tmp_hdi[1]*1000,
                              hdi_hi=tmp_hdi[2]*1000)
          if(o==1){
            all_hdi<-tmp_hdi
          }else{
            all_hdi<-rbind(all_hdi,tmp_hdi)
          }
        }
        
        #setup data in long form
        quantile_data<-data.frame(
          type=c(rep("real",length(quantiles)),rep("pred",length(quantiles))), # type of data (long form)
          quantile=c(quantiles,quantiles), # quantiles
          rt=c(all_hdi$real_quantile_mean,all_hdi$pred_quantile_mean), #type of quantile data
          hdi_lo=c(all_hdi$real_quantile_mean,all_hdi$hdi_lo),
          hdi_hi=c(all_hdi$real_quantile_mean, all_hdi$hdi_hi))
        
        quantile_data$group<-groups_names[m]
        
        if(m==1){#if group 1
          grp_quantile_data<-quantile_data
          grp1_trials<-dim(subset(real_data,real_data$group==m & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$acc==(l-1)))[1]
        }else if(m==2){#if group 2
          tmp_grp_quantile_data<-quantile_data
          tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.03 #shift slightly to right for plotting
          grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
          grp2_trials<-dim(subset(real_data,real_data$group==m & real_data$cond==j & real_data$cond_other_head==k & real_data$acc==(l-1)))[1]
        }else{#if group 3
          tmp_grp_quantile_data<-quantile_data
          tmp_grp_quantile_data$quantile<-tmp_grp_quantile_data$quantile+.06 #shift slightly to right for plotting
          grp_quantile_data<-rbind(tmp_grp_quantile_data,grp_quantile_data)
          grp3_trials<-dim(subset(real_data,real_data$group==m & real_data$cond==j & real_data$cond_other_head==k & real_data$acc==(l-1)))[1]
        }
      }
      grp_quantile_data$group<-as.factor(grp_quantile_data$group)
      title<-paste("Model 10 quantile predictions:\n",gaze_conds_names[j],": ",head_conds_names[k],"\nacc=",(l-1),sep="")
      trials_text<-paste("Trials per group:\nHC=",grp1_trials," BD=",grp2_trials," SZ=",grp3_trials,sep="")
      
      #plot quantiles 
      plot<-ggplot(grp_quantile_data, aes(x=quantile, y=rt, color=type)) +
        geom_errorbar(aes(ymin=hdi_lo,ymax=hdi_hi),width=0,alpha=.7) +
        geom_point(alpha=.7,aes(shape=group)) +
        ggtitle(title) +
        ylab("RT") +
        xlab("Quantile")+
        theme(plot.title = element_text(size=12),axis.text.y=element_text(size=7))+ 
        coord_cartesian(ylim = c(0,2500))+ annotate(geom = "text", x = .5, y = 2450, 
                                              label = trials_text, 
                                              hjust = "center",
                                              size=3.5,lineheight=.9)
      print(plot)
    }
  }
}

rm(list=c('estim_pred_quantiles_over_subjs','estim_quantiles','posterior','mudata','subdata','sigdata','tmp_quantile_est','tmp_data','tmp_matrix','quantile_data','fit_summary','p','plot','real_data'))
invisible(gc())

```

## Predicted Choice % (Model 10)

Results show that the predicted proportion of "yes" responses (data in pink below) for Model 10 matched the observed data well (data in teal below). This was true for all 3 groups in all conditions (left panel below) and when we marginalized over the emotion condition (right panel below). This indicates the model is capable of making accurate predictions about choice proportions.

```{r id26, echo=FALSE, fig.height=6.5, fig.width=4.25, message=FALSE, warning=FALSE,fig.show="hold",cache=TRUE}

estimated_choiceprops_file<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/ppc/choice_prop_estimates_hdi.RData"

#load real observed data
real_data<-read.csv('/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv',header=TRUE) #load real observed data
real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 

quantiles<-c(.1,.3,.5,.7,.9)
groups<-unique(real_data$group)
subjs<-unique(real_data$subjID)
gaze_conds<-unique(real_data$cond)
emo_conds<-unique(real_data$cond_other_emo)
head_conds<-unique(real_data$cond_other_head)
emohead_conds<-unique(real_data$cond_other_emo_head)
choices<-unique(real_data$choice)
acc<-unique(real_data$acc)
n_obs<-dim(real_data)[1]
cores<-10

groups_names<-c("HC","BD","SZ")
gaze_conds_names<-c("direct","indirect")
emo_conds_names<-c("neutral","fearful")
head_conds_names<-c("forward","deviated")
emohead_conds_names<-c("forward_neutral","forward_fearful","deviated_neutral","deviated_fearful")

if(file.exists(estimated_choiceprops_file)){ #if file exists,load it
  load(estimated_choiceprops_file)
}else{ #if file doesn't exist, run code to estimate quantiles
  
  load("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/stanfit_cmdstan.RData") #load posterior samples
  iterations=dim(alldata)[1] #number of posterwarmup draws
  posterior<-data.frame(alldata)
  
  rm(alldata)
  invisible(gc())
  
  #setup cluster on computer for parallel processing
  cluster <- makeCluster(cores) #Leave one core to avoid overload your computer
  registerDoParallel(cluster)
  
  # get subj level variables (group and minRT) and subj*cond*choice level varaibles (ie how many trials each subject has for each gaze condition, choice)
  for (i in 1:length(subjs)){
    tmpdata<-subset(real_data,real_data$subjID==i) 
    #get min Rt and group for this subj
    tmp_subj_vars<-data.frame(subj=i,group=unique((tmpdata$group)),minRT=min(tmpdata$rt))
    if (i==1){
      subj_vars<-tmp_subj_vars
    }else{
      subj_vars<-rbind(subj_vars,tmp_subj_vars)
    }
    for (j in 1:length(gaze_conds)){
      for (k in 1:length(emohead_conds)){
        for (l in 1:length(choices)){
          tmpdata<-dim(subset(real_data,real_data$subjID==i & real_data$cond == j & real_data$other_cond_emo_head == k & real_data$choice == l))[1]
          tmp_cond_vars<-data.frame(subj=i,gaze_cond=j,emohead_cond=k,choice=l,trials=tmpdata)
          if (i==1 & j==1 & k==1 & l==1){
            cond_vars<-tmp_cond_vars
          }else{
            cond_vars<-rbind(cond_vars,tmp_cond_vars)
          }
        }
      }
    }
  }
  
  #get group level vars
  for (i in 1:length(groups)){
    tmpdata<-subset(real_data,real_data$group==i)
    tmp_group_vars<-data.frame(group=i,maxRT=max(tmpdata$rt))
    if (i==1){
      group_vars<-tmp_group_vars
    }else{
      group_vars<-rbind(group_vars,tmp_group_vars)
    }
  }
  
  #grab rand draws to subsample from full posterior to make the file size more managable
  set.seed(42)
  
  # prepare the posterior samples
  ## subject parameters
  sub_alpha_pr_samples<-posterior[,c(grep("sub_alpha_pr", colnames(posterior)))]
  sub_beta_pr_samples<-posterior[,c(grep("sub_beta_pr", colnames(posterior)))] 
  sub_delta_present1_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,1:100]
  sub_delta_present2_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,101:200] 
  sub_delta_present3_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,201:300]
  sub_delta_present4_pr_samples<-posterior[,c(grep("sub_delta_present_pr", colnames(posterior)))][,301:400] 
  sub_delta_absent1_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,1:100]
  sub_delta_absent2_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,101:200]
  sub_delta_absent3_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,201:300]
  sub_delta_absent4_pr_samples<-posterior[,c(grep("sub_delta_absent_pr", colnames(posterior)))][,301:400]
  sub_ndt_pr_samples<-posterior[,c(grep("sub_ndt_pr", colnames(posterior)))] 
  
  ## group parameters (means)
  mu_grp_alpha_pr_samples<-posterior[,c(grep("mu_grp_alpha_pr", colnames(posterior)))] 
  mu_grp_beta_pr_samples<-posterior[,c(grep("mu_grp_beta_pr", colnames(posterior)))] 
  mu_grp_delta_present_pr_samples<-posterior[,c(grep("mu_grp_delta_present_pr", colnames(posterior)))] 
  mu_grp_delta_absent_pr_samples<-posterior[,c(grep("mu_grp_delta_absent_pr", colnames(posterior)))] 
  mu_grp_ndt_pr_samples<-posterior[,c(grep("mu_grp_ndt_pr", colnames(posterior)))] 
  
  ## group parameters (variances)
  sig_grp_alpha_pr_samples<-posterior[,c(grep("sig_grp_alpha_pr", colnames(posterior)))] 
  sig_grp_beta_pr_samples<-posterior[,c(grep("sig_grp_beta_pr", colnames(posterior)))] 
  sig_grp_delta_pr_samples<-posterior[,c(grep("sig_grp_delta_pr", colnames(posterior)))] 
  sig_grp_ndt_pr_samples<-posterior[,c(grep("sig_grp_ndt_pr", colnames(posterior)))] 
  
  ## other condition parameters (group level effect; 2x emo conditions)
  cond_delta1_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,1:3]
  cond_delta2_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,4:6]
  cond_delta3_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,7:9]
  cond_delta4_pr_samples<-(posterior[,c(grep("cond_delta_pr", colnames(posterior)))])[,10:12]
  
  subsample<-1000 #how many draws should we randomly subsample from the full posterior 
  
  #create empty array for predicted choice proportions: #obs x #subsampled iterations
  estim_choice_props <- array(numeric(),c(n_obs,subsample)) 
  
  for (k in 1:n_obs){
    tmp_subj<-real_data$subjID[k]
    tmp_group<-real_data$group[k]
    tmp_gazecond<-real_data$cond[k]
    tmp_emoheadcond<-real_data$cond_other_emo_head[k]
    tmp_choice<-real_data$choice[k]
    tmp_minRT<-subj_vars$minRT[tmp_subj] #get this subj's min RT
    
    #rand subsample
    rand_draws<-sample(seq(from=1,to=iterations,by=1),subsample)
    
    #randomly draw group mean, variance values 
    tmp_mu_grp_alpha_pr<-as.numeric(unlist(mu_grp_alpha_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_beta_pr<-as.numeric(unlist(mu_grp_beta_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_delta_present_pr<-as.numeric(unlist(mu_grp_delta_present_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_delta_absent_pr<-as.numeric(unlist(mu_grp_delta_absent_pr_samples[rand_draws,tmp_group]))
    tmp_mu_grp_ndt_pr<-as.numeric(unlist(mu_grp_ndt_pr_samples[rand_draws,tmp_group]))
    
    tmp_sig_grp_alpha_pr<-as.numeric(unlist(sig_grp_alpha_pr_samples[rand_draws,tmp_group]))
    tmp_sig_grp_beta_pr<-as.numeric(unlist(sig_grp_beta_pr_samples[rand_draws,tmp_group]))
    tmp_sig_grp_delta_pr<-as.numeric(unlist(sig_grp_delta_pr_samples[rand_draws,tmp_group]))
    tmp_sig_grp_ndt_pr<-as.numeric(unlist(sig_grp_ndt_pr_samples[rand_draws,tmp_group]))
    
    # randomly draw condition level values
    tmp_cond_delta1_pr<-as.numeric(unlist(cond_delta1_pr_samples[rand_draws,tmp_group]))
    tmp_cond_delta2_pr<-as.numeric(unlist(cond_delta2_pr_samples[rand_draws,tmp_group]))
    tmp_cond_delta3_pr<-as.numeric(unlist(cond_delta3_pr_samples[rand_draws,tmp_group]))
    tmp_cond_delta4_pr<-as.numeric(unlist(cond_delta4_pr_samples[rand_draws,tmp_group]))
    
    #draw subj level values
    tmp_sub_alpha_pr<-as.numeric(unlist(sub_alpha_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_beta_pr<-as.numeric(unlist(sub_beta_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present1_pr<-as.numeric(unlist(sub_delta_present1_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present2_pr<-as.numeric(unlist(sub_delta_present2_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present3_pr<-as.numeric(unlist(sub_delta_present3_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_present4_pr<-as.numeric(unlist(sub_delta_present4_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent1_pr<-as.numeric(unlist(sub_delta_absent1_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent2_pr<-as.numeric(unlist(sub_delta_absent2_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent3_pr<-as.numeric(unlist(sub_delta_absent3_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_delta_absent4_pr<-as.numeric(unlist(sub_delta_absent4_pr_samples[rand_draws,tmp_subj]))
    tmp_sub_ndt_pr<-as.numeric(unlist(sub_ndt_pr_samples[rand_draws,tmp_subj]))
    
    # do transformations for non centered parameterization (scale raw subject par by group mean and variance)
    tmp_alpha<-pnorm(tmp_mu_grp_alpha_pr + tmp_sub_alpha_pr*tmp_sig_grp_alpha_pr)*3.9+0.1
    tmp_beta<-pnorm(tmp_mu_grp_beta_pr + tmp_sub_beta_pr*tmp_sig_grp_beta_pr)
    tmp_delta_present1<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present1_pr*tmp_sig_grp_delta_pr+tmp_cond_delta1_pr)*8-4
    tmp_delta_present2<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present2_pr*tmp_sig_grp_delta_pr+tmp_cond_delta2_pr)*8-4
    tmp_delta_present3<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present3_pr*tmp_sig_grp_delta_pr+tmp_cond_delta3_pr)*8-4
    tmp_delta_present4<-pnorm(tmp_mu_grp_delta_present_pr + tmp_sub_delta_present4_pr*tmp_sig_grp_delta_pr+tmp_cond_delta4_pr)*8-4
    tmp_delta_absent1<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent1_pr*tmp_sig_grp_delta_pr+tmp_cond_delta1_pr)*8-4
    tmp_delta_absent2<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent2_pr*tmp_sig_grp_delta_pr+tmp_cond_delta2_pr)*8-4
    tmp_delta_absent3<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent3_pr*tmp_sig_grp_delta_pr+tmp_cond_delta3_pr)*8-4
    tmp_delta_absent4<-pnorm(tmp_mu_grp_delta_absent_pr + tmp_sub_delta_absent4_pr*tmp_sig_grp_delta_pr+tmp_cond_delta4_pr)*8-4
    tmp_ndt<-(pnorm(tmp_mu_grp_ndt_pr + tmp_sub_ndt_pr*tmp_sig_grp_ndt_pr)*tmp_minRT*0.9)/1000
    
    if(tmp_gazecond==1){#if gaze=direct
      if(tmp_choice==1){#if choice =YES (correct)
        resp<-"upper" #will use this variable for RWiener cdf function (pwiener) below)
        tmp_beta1<-tmp_beta
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_present1
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_present2
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_present3
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_present4
        }
      }else{#if choice=NO (incorrect)
        resp<-"lower"
        tmp_beta1<-tmp_beta #im deliberately NOT flipping the beta here (1-beta) like i would usually bc qwiener func does this internally based on the resp specified
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_present1 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_present2 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_present3 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_present4 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }
      }
    }else{ #if gaze=indirect
      if(tmp_choice==1){#if choice=YES (incorrect)
        resp<-"upper"
        tmp_beta1<-tmp_beta
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_absent1 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_absent2 
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_absent3
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_absent4
        }
      }else{#if choice=NO (correct)
        resp<-"lower"
        tmp_beta1<-tmp_beta #im deliberately NOT flipping the beta here (1-beta) like i would usually bc qwiener func does this internally based on the resp specified
        if(tmp_emoheadcond==1){
          tmp_delta_cond<-tmp_delta_absent1 #im deliberately NOT inverting the delta here (-1*delta) like i would usually bc qwiener func does this internally based on the resp specified
        }else if(tmp_emoheadcond==2){
          tmp_delta_cond<-tmp_delta_absent2 
        }else if(tmp_emoheadcond==3){
          tmp_delta_cond<-tmp_delta_absent3
        }else if(tmp_emoheadcond==4){
          tmp_delta_cond<-tmp_delta_absent4
        }
      }
    }
    
    #define some variables that we will use to calculate the choice probs below
    sigma<-1 #diffusion coefficient. fixed at 1
    
    pred_choice_prop<-foreach(l=1:length(tmp_alpha),.combine='rbind',.packages = c("RWiener","dplyr")) %dopar% {
      
      #calculate choice probabilities (needed to dead with defective CDF)
      ## calc absolute start point
      z<-tmp_alpha[l]*tmp_beta1[l] 
      
      ## calculate choice probabilities for current trial, parameters (whether it's upper or lower depends on observed choice of current trial)
      if(tmp_choice==1){#if upper bound response, calculate prob of upper bound resp
        pred_choice_prop <- (exp(2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2) - exp(2*tmp_delta_cond[l]*(tmp_alpha[l]-z)/sigma^2))/(exp(2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2)-1)
      }else{#if lower bound response, calculate prob of lower bound resp
        pred_choice_prop <- (exp(-2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2) - exp(-2*tmp_delta_cond[l]*z/sigma^2))/(exp(-2*tmp_delta_cond[l]*tmp_alpha[l]/sigma^2)-1)
      }
      return(pred_choice_prop)
    }
    
    estim_choice_props[k,]<-pred_choice_prop
    print(k)
    
    #save quantile estimates every 1000 observations (in case things crash)
    if(as.numeric(endsWith(as.character(unlist(k)), "000"))==1){
      # save current estimated quantile data to .RData file
      fitdir<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m9/ppc/"
      fitname<-paste(fitdir,"choice_prop_estimates_hdi",".RData",sep = "") 
      save(estim_choice_props, file = fitname)
      print("saving data processed to this point")
      invisible(gc())
    }
  }
  
  # save full estimated quantile data to .RData file
  fitdir<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m9/ppc/"
  fitname<-paste(fitdir,"choice_prop_estimates_hdi",".RData",sep = "") 
  save(estim_choice_props, file = fitname)
  
  parallel::stopCluster(cl = cluster)
  
}

# before calculating predicted choice proportions -- bc of how we calculated predicted proportions (prob of upper or lower bound resp) -- 
# we need to go back through the predicted proportions and flip them based on the current trial choice, so 
# it is simply predicting the probability of an upper bound response only.

for (i in 1:n_obs){# for i in n_observations
  tmp_gazecond<-real_data$cond[i] #get current gaze condition
  tmp_choice<-real_data$choice[i] #get current choice
  if(tmp_choice==2){ #if current choice is lower bound response, we need to flip the predicted choice prob
    estim_choice_props[i,] <- 1-(estim_choice_props[i,])
  }
}

subsample<-1000

#version 1: this way plots ALL condtions separate (gaze, emo, head)
for (j in 1:length(gaze_conds)){
  for (k in 1:length(emohead_conds)){
    for (m in 1:length(groups)){
      
      #get list of subjects in this group
      current_subj_list<-unique(subset(real_data,real_data$group==groups[m])$subjID)
      
      #create empty array: #subjs in this group x #iterations
      estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample)) 
      estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1)) 
      
      for (n in 1:length(current_subj_list)){
        
        #calculate real YES choice proportion
        tmp_realdata<-subset(real_data,real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_emo_head==k) # grab real # of yes reponses
        total_choices<-dim(tmp_realdata)[1] #total choices made for this gaze condition, head condition, for this subj
        total_yes_choice<-length(which(tmp_realdata$choice==1))
        estim_real_props_over_subjs[n,1]<-total_yes_choice/total_choices
        
        #calculate predicted YES choice proportion + HDIs for this condition/subj
        
        ## first, need to flip choice proportions to be upper bound only (we calculated prob of upper or lower bound depending on the choice in a given trial)
        tmp_prop_cols<-which(real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_emo_head==k & real_data$choice==1) #YES coded as 1 in data
        tmp_prop_est<-estim_choice_props[tmp_prop_cols,]
        
        #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
        if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
          estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
        }else if(length(tmp_prop_cols)==1){
          estim_pred_props_over_subjs[n,]<-tmp_prop_est
        }else{ # if subj has 2 or more trials for this cond, average over trials
          estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE) 
        }
      }
      
      #calculate predicted YES choice proportion + HDIs
      tmp_hdi_pred<-HDIofMCMC(as.vector(estim_pred_props_over_subjs))
      tmp_hdi_real<-HDIofMCMC(as.vector(estim_real_props_over_subjs))
      
      #tmp_cond<-paste(groups_names[m]," | ",gaze_conds_names[j]," | ",emohead_conds_names[k],sep="")
      tmp_cond<-paste(m," | ",j," | ",k,sep="")
      tmp_cond<-paste(groups_names[m]," | ",gaze_conds_names[j]," | ",emohead_conds_names[k],sep="")
      
      #setup data in long form (for plots)
      choiceprop_data<-data.frame(
        type=as.factor(c("real","pred")), # type of data (long form)
        mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
        pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
        pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))
      
      choiceprop_data$group<-as.factor(m)
      choiceprop_data$cond<-tmp_cond
      
      if(j==1&k==1&m==1){ #if first iteration
        grp_choiceprop_data<-choiceprop_data   
      }else{
        grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
      }
    }
  }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]

real_data$x_emohead<-paste(tolower(real_data$x_head),"_",tolower(real_data$x_emo),sep="")
real_data$x_group<- factor(real_data$group, levels = c("1", "2","3"), labels = c("HC", "BD","SZ"))

real_data$plot_cond<-paste(real_data$x_group," | ",tolower(real_data$x_gaze)," | ",real_data$x_emohead,sep="")
plot_real_data <- real_data[order(real_data$plot_cond),]
plot_real_data$choice[plot_real_data$choice == 2] <- 0

plot_real_data <- plot_real_data %>% 
  dplyr::group_by(plot_cond,subjID) %>% 
  dplyr::summarize(choice = mean(choice))

plot_real_data$type<-"real"

grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("1", "2","3"), labels = c("HC", "BD","SZ"))

title<-paste("Model 10 choice pred (% yes):\n All conditions",sep="")
plot<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_real_data,aes(x=plot_cond,y=choice,color=type),
    size=1.5,
    side = "r", ## draw jitter on the right
    range_scale = .8, ## control range of jitter
    alpha = .2## add some transparency
  ) +
  coord_flip()
print(plot)

#version2: this way plots head and gaze condityions but marginalizes over emo cond
for (j in 1:length(gaze_conds)){
  for (k in 1:length(head_conds)){
    for (m in 1:length(groups)){

      #get list of subjects in this group
      current_subj_list<-unique(subset(real_data,real_data$group==groups[m])$subjID)

      #create empty array: #subjs in this group x #iterations
      estim_pred_props_over_subjs <- array(numeric(),c(length(current_subj_list),subsample))
      estim_real_props_over_subjs <- array(numeric(),c(length(current_subj_list),1))

      for (n in 1:length(current_subj_list)){

        #calculate real YES choice proportion
        tmp_realdata<-subset(real_data,real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_head==k) # grab real # of yes reponses
        total_choices<-dim(tmp_realdata)[1] #total choices made for this gaze condition, head condition, for this subj
        total_yes_choice<-length(which(tmp_realdata$choice==1))
        estim_real_props_over_subjs[n,1]<-total_yes_choice/total_choices

        #calculate predicted YES choice proportion + HDIs for this condition/subj

        ## first, need to flip choice proportions to be upper bound only (we calculated prob of upper or lower bound depending on the choice in a given trial)
        tmp_prop_cols<-which(real_data$subjID==current_subj_list[n] & real_data$cond==j & real_data$cond_other_head==k & real_data$choice==1) #YES coded as 1 in data
        tmp_prop_est<-estim_choice_props[tmp_prop_cols,]

        #average estimates over trials for this subject (gives matrix = #subsamples*#quantiles)
        if(length(tmp_prop_cols)==0){ #if no trials for current condition for current subject, create array of NA
          estim_pred_props_over_subjs[n,]<-as.numeric(matrix(NA,nrow=1,ncol=subsample))
        }else if(length(tmp_prop_cols)==1){
          estim_pred_props_over_subjs[n,]<-tmp_prop_est
        }else{ # if subj has 2 or more trials for this cond, average over trials
          estim_pred_props_over_subjs[n,]<-apply(tmp_prop_est, c(2), mean,na.rm=TRUE)
        }
      }

      #calculate predicted YES choice proportion + HDIs
      tmp_hdi_pred<-HDIofMCMC(as.vector(estim_pred_props_over_subjs))
      tmp_hdi_real<-HDIofMCMC(as.vector(estim_real_props_over_subjs))

      tmp_cond<-paste(groups_names[m]," | ",gaze_conds_names[j]," | ",head_conds_names[k],sep="")

      #setup data in long form (for plots)
        choiceprop_data<-data.frame(
        type=as.factor(c("real","pred")), # type of data (long form)
        mean_choiceprop=as.numeric(c(mean(estim_real_props_over_subjs),mean(estim_pred_props_over_subjs,na.rm = TRUE))),
        pred_hdi_lo=as.numeric(c(tmp_hdi_real[1],tmp_hdi_pred[1])),
        pred_hdi_hi=as.numeric(c(tmp_hdi_real[2],tmp_hdi_pred[2])))

      choiceprop_data$group<-as.factor(m)
      choiceprop_data$cond<-tmp_cond

      if(j==1&k==1&m==1){ #if first iteration
        grp_choiceprop_data<-choiceprop_data
      }else{
        grp_choiceprop_data<-rbind(grp_choiceprop_data,choiceprop_data)
      }
    }
  }
}

grp_choiceprop_data <- grp_choiceprop_data[order(grp_choiceprop_data$cond),]

real_data$plot_cond<-paste(real_data$x_group," | ",tolower(real_data$x_gaze)," | ",tolower(real_data$x_head),sep="")
plot_real_data <- real_data[order(real_data$plot_cond),]
plot_real_data$choice[plot_real_data$choice == 2] <- 0

plot_real_data <- plot_real_data %>%
  dplyr::group_by(plot_cond,subjID) %>%
  dplyr::summarize(choice = mean(choice))

plot_real_data$type<-"real"
grp_choiceprop_data$group_name<-factor(grp_choiceprop_data$group, levels = c("1", "2","3"), labels = c("HC", "BD","SZ"))

title<-paste("Model 10 choice pred (% yes):\n Marginalized Emotion",sep="")
plot<-ggplot(data=grp_choiceprop_data, aes(x=cond, y=mean_choiceprop,color=type)) +
  geom_point(data=grp_choiceprop_data,alpha=1,aes(shape=group_name),position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=grp_choiceprop_data,aes(ymin=pred_hdi_lo,ymax=pred_hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  scale_fill_manual(values=c('#F8766D','#00BFC4'))+
  coord_cartesian(ylim = c(0,1))+
  ggtitle(title) + xlab("") +
  ylab("Choice Proportions")+
  theme(plot.title = element_text(size=10),axis.text.y = element_text(size=7),axis.text.x = element_text(size=7,angle = 90,vjust=1,hjust=1))+
  gghalves::geom_half_point(data=plot_real_data,aes(x=factor(plot_cond),y=choice,color=type),
    size=1.5,
    side = "r", ## draw jitter on the right
    range_scale = .8, ## control range of jitter
    alpha = .2## add some transparency
  ) +
  coord_flip()
print(plot)

rm(estim_pred_props_over_subjs)
invisible(gc())

```

# Winning Model Selection

To summarize the previous model evaluation steps:
* Both in the full sample and separate diagnostic groups, Model 10 was the winning model based on LOO model comparisons. It fit the data considerably better than the next best-fitting model (Model 9) based on differences in ELPD.
* A confusion matrix indicated that it was possible to arbitrate between Model 10 and other models.
* Model 10 showed good recovery of group- and subject-level parameters.
* Model 10 made accurate posterior predictions of RT quantiles and choice proportions across task conditions and diagnostic groups.

Considering all of the previous model evaluation steps, __Model 10 was selected as the winning model__. This model allowed all parameters to vary by diagnostic group and also allowed drift rate to vary in response to changes in gaze direction, head orientation, and facial emotion of stimuli. This is a plausible model account of behavior as the drift rate is known to be sensitive to the physical features of stimuli.

# Final Fit of Winning Model

After selecting Model 10 as our winning model, we ran a "final fit" of this model using a larger number of samples. We did this to achieve an effective sample size (ESS) of \>10,000 for all parameters, to ensure parameters we would be interpreting were sufficiently stable. The final fit of Model 10 was run in cmdstanr with 2,500 warm up samples and a total of 216,000 post warm up draws (obtained over 36 chains). Convergence checks are reported below and indicate that chains converged to target distributions.

```{r id27, echo=FALSE}

### Prep Data: loop through cmdstanr .csv files and combine into single data frame (postwarmup draws x # pars) where #postwarmup draws = rows of all draws from chain 1 (6000 rows), then all draws from chain 2 and so-on) 

knitr::opts_chunk$set(echo = TRUE)

warmup<-2500
postwarmup_draws<-6000
n_chains<-36
models<-c('m10')
outpath<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/"

final_fit_file<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/stanfit_cmdstan.RData"
if(file.exists(final_fit_file)){
  load(final_fit_file)
  alldata<-data.frame(alldata)
}else{
  for (i in 1:length(models)){
    model<-paste("hddm_",models[i],sep="")
    the_fitfile<-paste(outpath,model,'/final_fit/stanfit_cmdstan.RData',sep="")
    if(file.exists(the_fitfile)){ #if extracted stanfit file exists, do nothing
    }else{
      fitdir<-paste(outpath,model,'/final_fit/',sep="")
      setwd(fitdir)
  
      #read in all .csv files
      files <- (Sys.glob("*.csv"))
      skip_rows<-50 
      
      for (j in 1:length(files)){
        print(j)
        tmpfile<-paste(fitdir,files[j],sep="")
        
        #only grab the actual samples (not extra comments cmdstan outputs after)
        tmpdata<-fread(tmpfile,skip=skip_rows,nrows=postwarmup_draws)
        
        if (j==1){
            alldata<-tmpdata
        }else{
          alldata<-rbind(alldata,tmpdata)
        }
      }
      
      myheaders<-read.csv(tmpfile,skip=45,nrows=1)
      my_colnames <- colnames(myheaders) 
      colnames(alldata)<-my_colnames
      
      #save combined cmdstan fit object to .RData file
      fitname<-paste(fitdir,"stanfit_cmdstan",".RData",sep = "") 
      save(alldata, file = fitname)
      alldata<-data.frame(alldata)
    }
  }
}

alldata<-data.frame(alldata)

# narrow down parameters to examine

#grab sigma data 
tmp_col<-grep("sig_", colnames(alldata),fixed = TRUE) 
sigdata<-alldata[,tmp_col] 

#grab group data 
tmp_col<-grep("mu_", colnames(alldata),fixed = TRUE) 
mudata<-alldata[,tmp_col]
## remove untransformed params
tmp_col<-grep("_pr.", colnames(mudata),fixed = TRUE) 
mudata<-mudata[,-tmp_col] 

#grab subdata 
tmp_col<-grep("sub_", colnames(alldata),fixed = TRUE) 
subdata<-alldata[,tmp_col]

## remove untransformed params
tmp_col<-grep("_pr.", colnames(subdata),fixed = TRUE) 
subdata<-subdata[,-tmp_col] 

#combine
alldata<-cbind(mudata,sigdata,subdata)
pars<-colnames(alldata)

#function to rename param names in output to match manuscript
rename_m10<-function(pars){
  pars <- gsub("alpha", "threshold_sep", pars)
  pars <- gsub("beta", "start_point", pars)
  pars <- gsub("delta", "drift_rate", pars)
  pars <- gsub("_present", "_direct", pars)
  pars <- gsub("_absent", "_indirect", pars)
  pars <- gsub("mu_", "grp_", pars)
  pars <- gsub("_pr.", ".", pars)
  
  #1=neutral-forward, 2=fearful-forward, 3=neutral-deviated, 4=fearful-deviated
  # Define the patterns to match
  pattern_forward_neutral <- "\\.(\\d{1,3})\\.1$"  # Pattern for ".1"
  pattern_forward_fearful <- "\\.(\\d{1,3})\\.2$"  # Pattern for ".2"
  pattern_deviated_neutral <- "\\.(\\d{1,3})\\.3$"  # Pattern for ".3"
  pattern_deviated_fearful <- "\\.(\\d{1,3})\\.4$"  # Pattern for ".4"
  
  # Get the indices of columns matching the patterns
  matching_cols_forward_neutral <- grep(pattern_forward_neutral, pars)
  matching_cols_forward_fearful <- grep(pattern_forward_fearful, pars)
  matching_cols_deviated_neutral <- grep(pattern_deviated_neutral, pars)
  matching_cols_deviated_fearful <- grep(pattern_deviated_fearful, pars)
  
  # Replace matching column names
  pars[matching_cols_forward_neutral] <- gsub(pattern_forward_neutral, ".\\1.forw_fear", pars[matching_cols_forward_neutral])
  pars[matching_cols_forward_fearful] <- gsub(pattern_forward_fearful, ".\\1.forw_fear", pars[matching_cols_forward_fearful])
  pars[matching_cols_deviated_neutral] <- gsub(pattern_deviated_neutral, ".\\1.forw_fear", pars[matching_cols_deviated_neutral])
  pars[matching_cols_deviated_fearful] <- gsub(pattern_deviated_fearful, ".\\1.forw_fear", pars[matching_cols_deviated_fearful])
  
  first_sub_col <- which(grepl("^sub_", pars))[1]
  
  pars[1:(first_sub_col - 1)] <- gsub("\\.1$", "_hc", pars[1:(first_sub_col - 1)])
  pars[1:(first_sub_col - 1)] <- gsub("\\.2$", "_bd", pars[1:(first_sub_col - 1)])
  pars[1:(first_sub_col - 1)] <- gsub("\\.3$", "_sz", pars[1:(first_sub_col - 1)])
  
  pars <- gsub("cond.1.", "hc_", pars)
  pars <- gsub("cond.2.", "bd_", pars)
  pars <- gsub("cond.3.", "sz_", pars)
  
  return(pars)
  
}

#rename parameters to match manuscript
params<-rename_m10(colnames(alldata))
colnames(alldata)<-params

# check for divergent transitions
num_div<-sum(alldata$divergent__)

# adjust the scale of mu NDT samples (i.e., group level mean NDT only; subj level is fine). these were scaled by the max minRT across all groups in the stan code. this is not what we want, so we need to transform back to a proportion. (note: this was done in the generated quantities and, thus, doesn't have an effect on the sampling or integrity of the samples. the scale is just off and must be adjusted).

#load real observed data
real_data<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv")
real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 
subjs<-unique(real_data$subj)

#get min RT for all subjs
for (i in 1:length(subjs)){
  tmpdata<-subset(real_data,real_data$subj==subjs[i]) 
  tmp_minRT<-data.frame(subj=i,minRT=min(tmpdata$rt))
  if (i==1){
    minRT<-tmp_minRT
  }else{
    minRT<-rbind(minRT,tmp_minRT)
  }
}

tmp_minrt<-cbind(data.frame(subjs),minRT[,2])
rescale_mu_ndt<-max(minRT[,2])*0.9/1000 #do what was done in model and scale by max minRT*.9

# rescale all group NDTs back to proportion and scale by 0.9
alldata$grp_ndt_hc<-(alldata$grp_ndt_hc/rescale_mu_ndt)*0.9
alldata$grp_ndt_bd<-(alldata$grp_ndt_bd/rescale_mu_ndt)*0.9
alldata$grp_ndt_sz<-(alldata$grp_ndt_sz/rescale_mu_ndt)*0.9

```

## Convergence Check (Model 10)

In the final fit of the winning model (Model 10), there were 0 divergent transitions, the ESS was \>10,000 for all variables, trace plots were well-mixed, Rhat values were close to 1 (and all \>1.1), and autocorrelation was low but a lag of \~30. Together this indicated that posteriors were stable and chains had converged to target distributions.

### Rhat/ESS

```{r id21, echo=FALSE, fig.height=2.5, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

# calculate rhat, ESS
fit_summaryfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/fit_summary.RData"

if(file.exists(fit_summaryfile)){ #only calculate if fit summary hasn't been processed yet
  load(fit_summaryfile)
}else{
  for (i in 1:length(params)){
    tmp_data<-alldata[,i]
    tmp_matrix<-matrix(tmp_data,nrow=postwarmup_draws,ncol=n_chains,byrow=FALSE) #make this back into matrix (#draws * #chains)
    tmp_summary<-data.frame(
      parameter=params[i],
      rhat=round(rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
  }
  save(fit_summary,file=fit_summaryfile)
}

fit_summary$parameter<-rename_m10(fit_summary$parameter)
fit_summary

```

### Trace Plots

```{r id211, echo=FALSE, fig.height=1.5, fig.width=3.5, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

#Trace plots for group parameters

#keep only group parameters
tmp_col<-grep("sub_", params,fixed = TRUE) 
params<-params[-tmp_col] 

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))
for (i in 1:length(params)){
  tmp_data<-alldata[,i]
  tmp_matrix<-matrix(tmp_data,nrow=postwarmup_draws,ncol=n_chains,byrow=FALSE) #make this back into matrix (#draws * #chains)
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=postwarmup_draws,by=1),
                             Chain=seq(from=1,to=n_chains,by=1),
                             Parameter=params[i])
  title<-paste("Model 10:\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
}

```

### Autocorrelation

Note: We originally examined all 36 chains for autocorrelation. But here we randomly select 6 and display those (for brevity)

```{r id22, echo=FALSE, fig.height=2, fig.width=3, message=FALSE, warning=FALSE, cache=TRUE, tidy=TRUE}

#Autocorrelation for group parameters
chain_nums<-seq(from=1,to=n_chains,by=1)
subsample_chains<-6 #of of chains to subsample and display autocorr for

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:length(params)){
  tmp_data<-alldata[,i]
  tmp_matrix<-matrix(tmp_data,nrow=postwarmup_draws,ncol=n_chains,byrow=FALSE) #make this back into matrix (#draws * #chains)
  rand_chains<-sample(seq(from=1,to=n_chains,by=1),subsample_chains,replace=FALSE) #randomly sample some chains
  tmp_matrix<-tmp_matrix[,rand_chains] #get only data for those chains
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=postwarmup_draws,by=1),
                             Chain=rand_chains,
                             Parameter=params[i])
  title<-paste("Model 10: Autocorrelation\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)
}

rm(list=c('alldata'))
invisible(gc())

```

# [Analysis Supplement]{.underline} {.unnumbered}

```{r id1111, message=FALSE, warning=FALSE, include=FALSE}

# __Coding key for groups, task conditions, and task responses:__
# 
# *Diagnostic groups:* 1=HC, 2=BD, 3=SZ
# 
# *DDM parameters:* alpha=threshold separation, beta=start point, ndt=non-decision time, delta=drift rate, delta bias=drift bias
# 
# *Task stimuli:* 
# 
# * gaze direction: 1=direct, 2=indirect 
# * head orientation: 1=forward, 2=deviated 
# * emotion: 1=neutral, 2=fearful 
# * emotion*head: 1=neutral-forward, 2=fearful-forward, 3=neutral-deviated, 4=fearful-deviated
# 
# *Task responses:* 
# 
# * response: 1=yes (looking at me), 2=no (not looking at me)
# * accuracy: 0=incorrect, 1=correct

# Setup: load packages and custom functions

rm(list=ls()) #clear environment
cat("\f") #clear console
set.seed(42) #clear previous seed

options(width = 10000)
options(max.print=10000)
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(rstan)
library(ggplot2)
library(ggpubr)
library(tidyr)
library(plyr)
library(dplyr)
library(formatR)
library(loo)
library(bayesplot)
library(misty)
library(stats)
library(shredder)
library(RWiener)
library(hrbrthemes)
library(viridis)
library(ggridges)
library(HDInterval)
library(boot)
library(pkgcond)
library(cmdstanr)
library(posterior)
library(data.table)
library(parallel)
library(doParallel)
library(foreach)
library(psych)
library(arsenal)
library(GGally)
library(chlorpromazineR)
library(pscl)
library(lmtest)
library(kableExtra)
library(apaTables)
library(BayesFactor)
library(bayestestR)

#from HBayesDM toolbox. Ahn et al 2017
HDIofMCMC <- function(sampleVec,credMass = 0.9) {#from hbayesdm toolbox (Ahn et al 2014)
  sortedPts = sort(sampleVec)
  ciIdxInc = floor(credMass * length(sortedPts))
  nCIs = length(sortedPts) - ciIdxInc
  ciWidth = rep(0 , nCIs)
  for (i in 1:nCIs) {
    ciWidth[i] = sortedPts[i + ciIdxInc] - sortedPts[i]
  }
  HDImin = sortedPts[which.min(ciWidth)]
  HDImax = sortedPts[which.min(ciWidth) + ciIdxInc]
  HDIlim = c(HDImin , HDImax)
  return(HDIlim)
}

#custom function to print hierarchical linear regression outputs in a not-so-chaotic way
apaRegress <- function(models){#models is list of lm() models 
  n_models<-length(models)
  
  for (i in 1:n_models){
    tempmod<-models[[i]]
    tempsum<-data.frame(summary(tempmod)[["coefficients"]])
    
    if (i==1){
      refmod<-models[[1]]#begin with model 1 as reference model
      refmod_n<-1
      R2_change<-"--"
      F_change<-"--"
      p_change<-"--"
    }else{
      R2_change<-round(summary(tempmod)$r.squared - summary(refmod)$r.squared,2) #calc R2 change
      F_change<-round(anova(refmod,tempmod)$F[2],2)
      p_change<-round(anova(refmod,tempmod)$`Pr(>F)`[2],3)
    }
    
    temp_fitdata<-data.frame(Model=i,
                         Predictor="",
                         Versus=refmod_n,
                         R2=round(summary(tempmod)[["r.squared"]],3),
                         R2_change=R2_change,
                         F_change=F_change,
                         p_change=p_change,
                         b="",
                         SE="",
                         CI="",
                         t="",
                         p="")
    temp_preddata<-data.frame(Model="",
                         Predictor=rownames(tempsum),
                         Versus="",
                         R2="",
                         R2_change="",
                         F_change="",
                         p_change="",
                         b=round(tempsum$Estimate,2),
                         SE=round(tempsum$Std..Error,2),
                         CI=paste("[",round(confint(tempmod),2)[,1],", ",round(confint(tempmod),2)[,2],"]",sep=""),
                         t=round(tempsum$t.value,2),
                         p=round(tempsum$Pr...t..,3))
    tempdata<-rbind(temp_fitdata,temp_preddata)
    colnames(tempdata)<-c("Model","Predictor","Reference Model","R2",paste("R2","\u0394",sep=""),paste("F","\u0394",sep=""),paste("p","\u0394",sep=""),
                          "b","SE","CI","t","p")
    if (i==1){
      output<-tempdata
    }else{
      output<-rbind(output,tempdata)
    }
    if (is.na(p_change)==FALSE){
      if (p_change<.05){
        refmod<-models[[i]]
        refmod_n<-i
      }
    }
  }
  return(output)
}

```

```{r id21111, include=FALSE,cache=TRUE}

#Note: Final winning model (Model 10) run on HPC in cmdstanr with 36 chains over 36 cores, with 2500 warmup, 6000 postwarmup draws = 216000 postwarmup samples

## Prep Data: loop through cmdstanr .csv files and combine into single data frame (postwarmup draws x # pars) where #postwarmup draws = rows of all draws from chain 1 (6000 rows), then all draws from chain 2 and so-on)

warmup<-2500
postwarmup_draws<-6000
n_chains<-36
models<-c('m10')
outpath<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/"

final_fit_file<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/stanfit_cmdstan.RData"
if(file.exists(final_fit_file)){
  load(final_fit_file)
  alldata<-data.frame(alldata)
}else{
  for (i in 1:length(models)){
    model<-paste("hddm_",models[i],sep="")
    the_fitfile<-paste(outpath,model,'/final_fit/stanfit_cmdstan.RData',sep="")
    if(file.exists(the_fitfile)){ #if extracted stanfit file exists, do nothing
    }else{
      fitdir<-paste(outpath,model,'/final_fit/',sep="")
      setwd(fitdir)
  
      #read in all .csv files
      files <- (Sys.glob("*.csv"))
      skip_rows<-50 
      
      for (j in 1:length(files)){
        print(j)
        tmpfile<-paste(fitdir,files[j],sep="")
        
        #only grab the actual samples (not extra comments cmdstan outputs after)
        tmpdata<-fread(tmpfile,skip=skip_rows,nrows=postwarmup_draws)
        
        if (j==1){
            alldata<-tmpdata
        }else{
          alldata<-rbind(alldata,tmpdata)
        }
      }
      
      myheaders<-read.csv(tmpfile,skip=45,nrows=1)
      my_colnames <- colnames(myheaders) 
      colnames(alldata)<-my_colnames
      
      #save combined cmdstan fit object to .RData file
      fitname<-paste(fitdir,"stanfit_cmdstan",".RData",sep = "") 
      save(alldata, file = fitname)
      alldata<-data.frame(alldata)
    }
  }
}

# check for divergent transitions
divmsg<-paste('There were ',sum(alldata$divergent__),' divergent transitions after warmup',sep='')
print(divmsg)

# narrow down parameters to examine

#grab sigma data 
tmp_col<-grep("sig_", colnames(alldata),fixed = TRUE) 
sigdata<-alldata[,tmp_col] 

#grab mu (group) data and remove untransformed parameters
tmp_col<-grep("mu_", colnames(alldata),fixed = TRUE) 
mudata<-alldata[,tmp_col]

## remove untransformed params
tmp_col<-grep("_pr.", colnames(mudata),fixed = TRUE) 
mudata<-mudata[,-tmp_col] 

#grab subdata and remove untransformed parameters
tmp_col<-grep("sub_", colnames(alldata),fixed = TRUE) 
subdata<-alldata[,tmp_col]

## remove untransformed params
tmp_col<-grep("_pr.", colnames(subdata),fixed = TRUE) 
subdata<-subdata[,-tmp_col] 

#combine
alldata<-cbind(mudata,sigdata,subdata)
params<-colnames(alldata)

# calculate rhat, ESS
fit_summaryfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/fit_summary.RData"
if(file.exists(fit_summaryfile)){ 
  load(fit_summaryfile)
}else{
  for (i in 1:length(params)){
    tmp_data<-alldata[,i]
    tmp_matrix<-matrix(tmp_data,nrow=postwarmup_draws,ncol=n_chains,byrow=FALSE) #make into matrix (#draws * #chains)
    tmp_summary<-data.frame(
      parameter=params[i],
      rhat=round(rhat(tmp_matrix),2),
      bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
      tail_ess=round(posterior::ess_tail(tmp_matrix),0))
    
    if(i==1){
      fit_summary<-tmp_summary
    }else{
      fit_summary<-rbind(fit_summary,tmp_summary)
    }
  }
  save(fit_summary,file=fit_summaryfile)
}

rownames(fit_summary)<-fit_summary[,1]

#confirm that ESS of all parameters is >10,000
fit_summary

rm(list=c("fit_summary"))
invisible(gc())

#load real data
real_data<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv")
real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 

groups<-unique(real_data$group)
subjs<-unique(real_data$subj)

#group samples
cols<-grep("mu_alpha.", colnames(alldata),fixed=TRUE)
mu_alpha_samples<-alldata[,cols]
cols<-grep("mu_beta.", colnames(alldata),fixed=TRUE)
mu_beta_samples<-alldata[,cols]
cols<-grep("mu_ndt.", colnames(alldata),fixed=TRUE) #ndt samples were scaled by max minRT of each group in stan. we need to back transform this to proportion and scale by .9 later
mu_ndt_samples<-alldata[,cols]

# emo * head condition
## 1 = neutral forward
## 2 = fearful forward
## 3 = neutral deviated
## 4 = fearful deviated

cols<-grep("mu_delta_present_cond.", colnames(alldata),fixed=TRUE)[1:3]
mu_delta_present_cond1_samples<-alldata[,cols]
cols<-grep("mu_delta_present_cond.", colnames(alldata),fixed=TRUE)[4:6]
mu_delta_present_cond2_samples<-alldata[,cols] 
cols<-grep("mu_delta_present_cond.", colnames(alldata),fixed=TRUE)[7:9]
mu_delta_present_cond3_samples<-alldata[,cols] 
cols<-grep("mu_delta_present_cond.", colnames(alldata),fixed=TRUE)[10:12]
mu_delta_present_cond4_samples<-alldata[,cols] 
cols<-grep("mu_delta_absent_cond.", colnames(alldata),fixed=TRUE)[1:3]
mu_delta_absent_cond1_samples<-alldata[,cols]
cols<-grep("mu_delta_absent_cond.", colnames(alldata),fixed=TRUE)[4:6]
mu_delta_absent_cond2_samples<-alldata[,cols]
cols<-grep("mu_delta_absent_cond.", colnames(alldata),fixed=TRUE)[7:9]
mu_delta_absent_cond3_samples<-alldata[,cols]
cols<-grep("mu_delta_absent_cond.", colnames(alldata),fixed=TRUE)[10:12]
mu_delta_absent_cond4_samples<-alldata[,cols]

# group variances
cols<-grep("sig_grp_alpha_pr", colnames(alldata),fixed=TRUE)
sig_grp_alpha_pr_samples<-alldata[,cols]
cols<-grep("sig_grp_beta_pr", colnames(alldata),fixed=TRUE)
sig_grp_beta_pr_samples<-alldata[,cols]
cols<-grep("sig_grp_delta_pr", colnames(alldata),fixed=TRUE)
sig_grp_delta_pr_samples<-alldata[,cols]
cols<-grep("sig_grp_ndt_pr", colnames(alldata),fixed=TRUE)
sig_grp_ndt_pr_samples<-alldata[,cols]

# we also need to adjust the mu NDT samples. these were scaled by the max minRT across all groups in the stan code. this is not what we want, so we need to transform back to a proportion
for (i in 1:length(subjs)){
  tmpdata<-subset(real_data,real_data$subj==subjs[i]) 
  #get min Rt for this subj
  tmp_minRT<-data.frame(subj=i,minRT=min(tmpdata$rt))
  if (i==1){
    minRT<-tmp_minRT
  }else{
    minRT<-rbind(minRT,tmp_minRT)
  }
}

tmp_minrt<-cbind(data.frame(subjs),minRT[,2])
rescale_mu_ndt<-max(minRT[,2])*0.9/1000 #do what was done in model and scale by max minRT*.9

for (i in 1:length(groups)){
  
  delta_marginal_direct_forward_samples<-rowMeans(cbind(mu_delta_present_cond1_samples[,i],mu_delta_present_cond2_samples[,i])) 
  delta_marginal_direct_deviated_samples<-rowMeans(cbind(mu_delta_present_cond3_samples[,i],mu_delta_present_cond4_samples[,i])) 
  delta_marginal_indirect_forward_samples<-rowMeans(cbind(mu_delta_absent_cond1_samples[,i],mu_delta_absent_cond2_samples[,i])) 
  delta_marginal_indirect_deviated_samples<-rowMeans(cbind(mu_delta_absent_cond3_samples[,i],mu_delta_absent_cond4_samples[,i])) 
  
  delta_marginal_direct_samples<-rowMeans(cbind(delta_marginal_direct_forward_samples,delta_marginal_direct_deviated_samples))
  delta_marginal_indirect_samples<-rowMeans(cbind(delta_marginal_indirect_forward_samples,delta_marginal_indirect_deviated_samples))
               
  tmp_samples<-data.frame(group=rep(i,postwarmup_draws),
                          beta=mu_beta_samples[,i],
                          ndt=(mu_ndt_samples[,i]/rescale_mu_ndt)*0.9, # 1) rescale ndt back to proportion;  2) scale by .9 to match subject-level parameters
                          alpha=mu_alpha_samples[,i],
                          delta_direct_forward_neutral=mu_delta_present_cond1_samples[,i],
                          delta_direct_forward_fearful=mu_delta_present_cond2_samples[,i],
                          delta_direct_deviated_neutral=mu_delta_present_cond3_samples[,i],
                          delta_direct_deviated_fearful=mu_delta_present_cond4_samples[,i],
                          delta_indirect_forward_neutral=mu_delta_absent_cond1_samples[,i],
                          delta_indirect_forward_fearful=mu_delta_absent_cond2_samples[,i],
                          delta_indirect_deviated_neutral=mu_delta_absent_cond3_samples[,i],
                          delta_indirect_deviated_fearful=mu_delta_absent_cond4_samples[,i],
                          delta_marginal_direct_forward=delta_marginal_direct_forward_samples,
                          delta_marginal_direct_deviated=delta_marginal_direct_deviated_samples,
                          delta_marginal_indirect_forward=delta_marginal_indirect_forward_samples,
                          delta_marginal_indirect_deviated=delta_marginal_indirect_deviated_samples,
                          delta_marginal_bias=rowMeans(cbind(delta_marginal_direct_samples,delta_marginal_indirect_samples)),
                          delta_marginal_bias_forward=rowMeans(cbind(delta_marginal_direct_forward_samples,delta_marginal_indirect_forward_samples)),
                          delta_marginal_bias_deviated=rowMeans(cbind(delta_marginal_direct_deviated_samples,delta_marginal_indirect_deviated_samples)),
                          delta_marginal=rowMeans(cbind(delta_marginal_direct_samples,(delta_marginal_indirect_samples*-1))),
                          sigma_alpha=sig_grp_alpha_pr_samples[,i],
                          sigma_beta=sig_grp_beta_pr_samples[,i],
                          sigma_delta=sig_grp_delta_pr_samples[,i],
                          sigma_ndt=sig_grp_ndt_pr_samples[,i])
  if (i==1){
    all_mu_samples<-tmp_samples
  }else{
    all_mu_samples<-rbind(all_mu_samples,tmp_samples)
  }
}

all_mu_samples$group<-as.factor(all_mu_samples$group)

rm(list=c("mu_beta_samples","mu_ndt_samples","mu_alpha_samples","sig_grp_alpha_pr_samples",
          "sig_grp_beta_pr_samples","sig_grp_ndt_pr_samples","sig_grp_delta_pr_samples",
          "mu_delta_present_cond1_samples","mu_delta_present_cond2_samples",
          "mu_delta_present_cond3_samples","mu_delta_present_cond4_samples",
          "mu_delta_absent_cond1_samples","mu_delta_absent_cond2_samples",
          "mu_delta_absent_cond3_samples","mu_delta_absent_cond4_samples"))
invisible(gc())

levels(all_mu_samples$group)[levels(all_mu_samples$group) == "1"] = "HC"
levels(all_mu_samples$group)[levels(all_mu_samples$group) == "2"] = "BD"
levels(all_mu_samples$group)[levels(all_mu_samples$group) == "3"] = "SZ"

```

# Group/Condition Effects

We examined group and condition effects using group-level posteriors for all parameters. Between group differences were tested in a pairwise manner by subtracting the posterior for given parameter for one group from another. If the 90% highest density interval (HDI) of the posterior difference did not contain zero, then the difference was considered 'credible'. The same process was followed to evaluate the credibility of task condition effects within groups. This approach was appropriate because the parameterization of the models inherently accounted for the shared group-level variability of within-group effects.

Results presented below and detail group differences in threshold separation, start point, NDT, and drift rate parameters. Because the winning model (Model 10) accounted for within-group condition effects on drift rates--for different gaze, head, and emotion conditions--we also present task condition effects on drift rate below (both in isolation [marginalized across group] and in combination with group effects).

## Threshold Separation, Start Point, NDT (Group Effects)

Results show that threshold separation is credibly higher in SZ than HC, but no other group differences are credible for threshold separation. The three groups exhibited comparable values for start point and NDT parameters.

```{r id33333, echo=FALSE, fig.height=3, fig.show="hold", fig.width=3.5, message=FALSE, warning=FALSE,cache=TRUE}

#alpha
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$alpha)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$alpha)),3)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$alpha)-(subset(all_mu_samples,all_mu_samples$group=="BD")$alpha)),3)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$alpha)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$alpha)),3)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$alpha)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$alpha)),2)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$alpha)-(subset(all_mu_samples,all_mu_samples$group=="BD")$alpha)),2)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$alpha)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$alpha)),2)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$alpha)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$alpha)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$alpha)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$alpha)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$alpha)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$alpha)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1.5,1.9))+
  ggtitle("Threshold Separation") + xlab("") +
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 2, y = 1.87, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)

#beta
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$beta)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$beta)),3)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$beta)-(subset(all_mu_samples,all_mu_samples$group=="BD")$beta)),3)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$beta)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$beta)),3)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$beta)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$beta)),3)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$beta)-(subset(all_mu_samples,all_mu_samples$group=="BD")$beta)),3)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$beta)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$beta)),3)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$beta)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$beta)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$beta)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$beta)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$beta)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$beta)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(0.5,.57))+
  ggtitle("Start Point") + xlab("") +
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 2, y = .565, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)

#ndt
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$ndt)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$ndt)),4)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$ndt)-(subset(all_mu_samples,all_mu_samples$group=="BD")$ndt)),4)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$ndt)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$ndt)),4)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$ndt)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$ndt)),4)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$ndt)-(subset(all_mu_samples,all_mu_samples$group=="BD")$ndt)),4)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$ndt)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$ndt)),4)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$ndt)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$ndt)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$ndt)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$ndt)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$ndt)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$ndt)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(0.8995,.9002))+
  ggtitle("NDT") + xlab("") +
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 2, y = .90015, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)


```

## Drift Rate (Group/Gaze/Head/Emo Effects)

We looked at group differences in drift rates broken down by gaze and head conditions. This was done separately by neutral and fearful emotions. Results showed credible group differences in drift rates within gaze and head conditions. None of the groups exhibited group differences when gaze was direct and heads were deviated (this was true for both fearful and neutral emotions). When heads were forward and gaze was direct, HC had credibly higher drift rates than BD in both emotion conditions. When heads were forward and gaze was direct, HC had credibly higher drift rates than SZ only in the fearful emotion condition.

```{r id433333, echo=FALSE, fig.height=7.5, fig.show="hold", fig.width=3.5, message=FALSE, warning=FALSE,cache=TRUE}

##############################################################################
# head * gaze conditions for neutral trials
##############################################################################

#delta direct forward neutral
hc_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)),3)
hc_minus_bd1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)),3)
bd_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)),3)

hc_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)),3)
hc_minus_bd1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)),3)
bd_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)),3)

#delta direct deviated neutral
hc_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)),3)
hc_minus_bd2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)),3)
bd_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)),3)

hc_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)),3)
hc_minus_bd2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)),3)
bd_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)),3)

#delta INdirect forward neutral
hc_minus_sz3<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral)),3)
hc_minus_bd3<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral)),3)
bd_minus_sz3<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral)),3)

hc_minus_sz3_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral)),3)
hc_minus_bd3_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral)),3)
bd_minus_sz3_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral)),3)

#delta INdirect deviated neutral
hc_minus_sz4<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral)),3)
hc_minus_bd4<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral)),3)
bd_minus_sz4<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral)),3)

hc_minus_sz4_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral)),3)
hc_minus_bd4_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral)),3)
bd_minus_sz4_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral)),3)

texta<-paste("Direct, forward:\n",
             "HC-SZ: [",hc_minus_sz1[1],",",hc_minus_sz1[2],"], M = ",hc_minus_sz1_mean,"\n",
             "HC-BD: [",hc_minus_bd1[1],",",hc_minus_bd1[2],"], M = ",hc_minus_bd1_mean,"\n",
             "BD-SZ: [",bd_minus_sz1[1],",",bd_minus_sz1[2],"], M = ",bd_minus_sz1_mean,"\n",
             "Direct, deviated:\n",
             "HC-SZ: [",hc_minus_sz2[1],",",hc_minus_sz2[2],"], M = ",hc_minus_sz2_mean,"\n",
             "HC-BD: [",hc_minus_bd2[1],",",hc_minus_bd2[2],"], M = ",hc_minus_bd2_mean,"\n",
             "BD-SZ: [",bd_minus_sz2[1],",",bd_minus_sz2[2],"], M = ",bd_minus_sz2_mean,"\n",
             "Indirect, forward:\n",
             "HC-SZ: [",hc_minus_sz3[1],",",hc_minus_sz3[2],"], M = ",hc_minus_sz3_mean,"\n",
             "HC-BD: [",hc_minus_bd3[1],",",hc_minus_bd3[2],"], M = ",hc_minus_bd3_mean,"\n",
             "BD-SZ: [",bd_minus_sz3[1],",",bd_minus_sz3[2],"], M = ",bd_minus_sz3_mean,"\n",
             "Indirect, deviated:\n",
             "HC-SZ: [",hc_minus_sz4[1],",",hc_minus_sz4[2],"], M = ",hc_minus_sz4_mean,"\n",
             "HC-BD: [",hc_minus_bd4[1],",",hc_minus_bd4[2],"], M = ",hc_minus_bd4_mean,"\n",
             "BD-SZ: [",bd_minus_sz4[1],",",bd_minus_sz4[2],"], M = ",bd_minus_sz4_mean,
             sep="")

hc_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)
bd_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)
sz_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)

hc_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)
bd_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)
sz_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)

hc_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)
bd_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)
sz_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)

hc_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)
bd_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)
sz_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)

hc_hdi3<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral)
bd_hdi3<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral)
sz_hdi3<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral)

hc_mean3<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral)
bd_mean3<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral)
sz_mean3<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral)

hc_hdi4<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral)
bd_hdi4<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral)
sz_hdi4<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral)

hc_mean4<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral)
bd_mean4<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral)
sz_mean4<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral)

plot_data<-data.frame(
  group=rep(c("HC","BD","SZ"),4),
  cond=c(rep("Direct-Forward",3),rep("Direct-Deviated",3),rep("Indirect-Forward",3),rep("Indirect-Deviated",3)),
  mean=c(hc_mean1,bd_mean1,sz_mean1,hc_mean2,bd_mean2,sz_mean2,hc_mean3,bd_mean3,sz_mean3,hc_mean4,bd_mean4,sz_mean4),
  hdi_lo=c(hc_hdi1[1],bd_hdi1[1],sz_hdi1[1],hc_hdi2[1],bd_hdi2[1],sz_hdi2[1],hc_hdi3[1],bd_hdi3[1],sz_hdi3[1],hc_hdi4[1],bd_hdi4[1],sz_hdi4[1]),
  hdi_hi=c(hc_hdi1[2],bd_hdi1[2],sz_hdi1[2],hc_hdi2[2],bd_hdi2[2],sz_hdi2[2],hc_hdi3[2],bd_hdi3[2],sz_hdi3[2],hc_hdi4[2],bd_hdi4[2],sz_hdi4[2]))

p<-ggplot(data=plot_data, aes(x=factor(cond,levels=c("Direct-Forward","Direct-Deviated","Indirect-Forward","Indirect-Deviated")), y=mean,color=factor(group,levels=c("HC","BD","SZ")),alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-3.5,6.5))+
  scale_color_manual(values=c('#482475','#287c8e','#addc30'))+
  ggtitle("Model 10: Drift Rate (Neutral Emo)") + xlab("") +
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 4.5, y = 5, label = texta, hjust = "right",size=3,lineheight=.9)+
  labs(colour="group")
print(p)

##############################################################################
# head * gaze conditions for fearful trials
##############################################################################

#delta direct forward fearful
hc_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)),3)
hc_minus_bd1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)),3)
bd_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)),3)

hc_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)),3)
hc_minus_bd1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)),3)
bd_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)),3)

#delta direct deviated fearful
hc_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)),3)
hc_minus_bd2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)),3)
bd_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)),3)

hc_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)),3)
hc_minus_bd2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)),3)
bd_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)),3)

#delta INdirect forward fearful
hc_minus_sz3<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful)),3)
hc_minus_bd3<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful)),3)
bd_minus_sz3<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful)),3)

hc_minus_sz3_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful)),3)
hc_minus_bd3_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful)),3)
bd_minus_sz3_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful)),3)

#delta INdirect deviated fearful
hc_minus_sz4<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful)),3)
hc_minus_bd4<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful)),3)
bd_minus_sz4<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful)),3)

hc_minus_sz4_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful)),3)
hc_minus_bd4_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful)),3)
bd_minus_sz4_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful)),3)

texta<-paste("Direct, forward:\n",
             "HC-SZ: [",hc_minus_sz1[1],",",hc_minus_sz1[2],"], M = ",hc_minus_sz1_mean,"\n",
             "HC-BD: [",hc_minus_bd1[1],",",hc_minus_bd1[2],"], M = ",hc_minus_bd1_mean,"\n",
             "BD-SZ: [",bd_minus_sz1[1],",",bd_minus_sz1[2],"], M = ",bd_minus_sz1_mean,"\n",
             "Direct, deviated:\n",
             "HC-SZ: [",hc_minus_sz2[1],",",hc_minus_sz2[2],"], M = ",hc_minus_sz2_mean,"\n",
             "HC-BD: [",hc_minus_bd2[1],",",hc_minus_bd2[2],"], M = ",hc_minus_bd2_mean,"\n",
             "BD-SZ: [",bd_minus_sz2[1],",",bd_minus_sz2[2],"], M = ",bd_minus_sz2_mean,"\n",
             "Indirect, forward:\n",
             "HC-SZ: [",hc_minus_sz3[1],",",hc_minus_sz3[2],"], M = ",hc_minus_sz3_mean,"\n",
             "HC-BD: [",hc_minus_bd3[1],",",hc_minus_bd3[2],"], M = ",hc_minus_bd3_mean,"\n",
             "BD-SZ: [",bd_minus_sz3[1],",",bd_minus_sz3[2],"], M = ",bd_minus_sz3_mean,"\n",
             "Indirect, deviated:\n",
             "HC-SZ: [",hc_minus_sz4[1],",",hc_minus_sz4[2],"], M = ",hc_minus_sz4_mean,"\n",
             "HC-BD: [",hc_minus_bd4[1],",",hc_minus_bd4[2],"], M = ",hc_minus_bd4_mean,"\n",
             "BD-SZ: [",bd_minus_sz4[1],",",bd_minus_sz4[2],"], M = ",bd_minus_sz4_mean,
             sep="")

hc_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)
bd_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)
sz_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)

hc_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)
bd_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)
sz_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)

hc_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)
bd_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)
sz_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)

hc_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)
bd_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)
sz_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)

hc_hdi3<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful)
bd_hdi3<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful)
sz_hdi3<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful)

hc_mean3<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful)
bd_mean3<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful)
sz_mean3<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful)

hc_hdi4<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful)
bd_hdi4<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful)
sz_hdi4<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful)

hc_mean4<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful)
bd_mean4<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful)
sz_mean4<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful)

plot_data<-data.frame(
  group=rep(c("HC","BD","SZ"),4),
  cond=c(rep("Direct-Forward",3),rep("Direct-Deviated",3),rep("Indirect-Forward",3),rep("Indirect-Deviated",3)),
  mean=c(hc_mean1,bd_mean1,sz_mean1,hc_mean2,bd_mean2,sz_mean2,hc_mean3,bd_mean3,sz_mean3,hc_mean4,bd_mean4,sz_mean4),
  hdi_lo=c(hc_hdi1[1],bd_hdi1[1],sz_hdi1[1],hc_hdi2[1],bd_hdi2[1],sz_hdi2[1],hc_hdi3[1],bd_hdi3[1],sz_hdi3[1],hc_hdi4[1],bd_hdi4[1],sz_hdi4[1]),
  hdi_hi=c(hc_hdi1[2],bd_hdi1[2],sz_hdi1[2],hc_hdi2[2],bd_hdi2[2],sz_hdi2[2],hc_hdi3[2],bd_hdi3[2],sz_hdi3[2],hc_hdi4[2],bd_hdi4[2],sz_hdi4[2]))

p<-ggplot(data=plot_data, aes(x=factor(cond,levels=c("Direct-Forward","Direct-Deviated","Indirect-Forward","Indirect-Deviated")), y=mean,color=factor(group,levels=c("HC","BD","SZ")),alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-3.5,6.5))+
  scale_color_manual(values=c('#482475','#287c8e','#addc30'))+
  ggtitle("Model 10: Drift Rate (Fearful Emo)") + xlab("") +
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 4.5, y = 5, label = texta, hjust = "right",size=3,lineheight=.9)+
  labs(colour="group")
print(p)

```

## Drift Rate (Group/Gaze/Head Effects)

These results are reported in the main text. Here, we look at group differences in group-level drift rate parameters in a pairwise manner. This is done separately for drift rates in gaze and head conditions, after marginalizing over the emotion conditions.

```{r id633333, echo=FALSE, fig.height=6, fig.show="hold", fig.width=3.5, message=FALSE, warning=FALSE,tidy=T,cache=TRUE}

#delta direct forward
hc_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)),3)
hc_minus_bd1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)),3)
bd_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)),3)

hc_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)),3)
hc_minus_bd1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)),3)
bd_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)),3)

#delta indirect forward
hc_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward)),3)
hc_minus_bd2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward)),3)
bd_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward)),3)

hc_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward)),3)
hc_minus_bd2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward)),3)
bd_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward)),3)

texta<-paste("Direct ",
      "\nHC-SZ: [",hc_minus_sz1[1],",",hc_minus_sz1[2],"], M = ",hc_minus_sz1_mean,"\n",
      "HC-BD: [",hc_minus_bd1[1],",",hc_minus_bd1[2],"], M = ",hc_minus_bd1_mean,"\n",
      "BD-SZ: [",bd_minus_sz1[1],",",bd_minus_sz1[2],"], M = ",bd_minus_sz1_mean,"\n",
  "Indirect",
  "\nHC-SZ: [",hc_minus_sz2[1],",",hc_minus_sz2[2],"], M = ",hc_minus_sz2_mean,"\n",
      "HC-BD: [",hc_minus_bd2[1],",",hc_minus_bd2[2],"], M = ",hc_minus_bd2_mean,"\n",
      "BD-SZ: [",bd_minus_sz2[1],",",bd_minus_sz2[2],"], M = ",bd_minus_sz2_mean,
  sep="")

hc_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)
bd_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)
sz_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)

hc_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)
bd_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)
sz_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)

hc_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward)
bd_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward)
sz_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward)

hc_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward)
bd_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward)
sz_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward)

plot_data<-data.frame(
  group=rep(c("HC","BD","SZ"),2),
  cond=c(rep("Direct",3),rep("Indirect",3)),
  mean=c(hc_mean1,bd_mean1,sz_mean1,hc_mean2,bd_mean2,sz_mean2),
  hdi_lo=c(hc_hdi1[1],bd_hdi1[1],sz_hdi1[1],hc_hdi2[1],bd_hdi2[1],sz_hdi2[1]),
  hdi_hi=c(hc_hdi1[2],bd_hdi1[2],sz_hdi1[2],hc_hdi2[2],bd_hdi2[2],sz_hdi2[2]))

p<-ggplot(data=plot_data, aes(x=cond, y=mean,color=factor(group,levels=c("HC","BD","SZ")),alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-3.5,4))+
  scale_color_manual(values=c('#482475','#287c8e','#addc30'))+
  ggtitle("Drift Rate: Forward Heads") + xlab("") +
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 1.5, y = 3.2, label = texta, hjust = "center",size=3,lineheight=.9)+
  labs(colour="group")
print(p)

# now do deviated heads

#delta direct deviated
hc_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)),3)
hc_minus_bd1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)),3)
bd_minus_sz1<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)),3)

hc_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)),3)
hc_minus_bd1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)),3)
bd_minus_sz1_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)),3)

#delta indirect deviated
hc_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated)),3)
hc_minus_bd2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated)),3)
bd_minus_sz2<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated)),3)

hc_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated)),3)
hc_minus_bd2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated)),3)
bd_minus_sz2_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated)),3)

texta<-paste("Direct ",
      "\nHC-SZ: [",hc_minus_sz1[1],",",hc_minus_sz1[2],"], M = ",hc_minus_sz1_mean,"\n",
      "HC-BD: [",hc_minus_bd1[1],",",hc_minus_bd1[2],"], M = ",hc_minus_bd1_mean,"\n",
      "BD-SZ: [",bd_minus_sz1[1],",",bd_minus_sz1[2],"], M = ",bd_minus_sz1_mean,"\n",
  "Indirect",
  "\nHC-SZ: [",hc_minus_sz2[1],",",hc_minus_sz2[2],"], M = ",hc_minus_sz2_mean,"\n",
      "HC-BD: [",hc_minus_bd2[1],",",hc_minus_bd2[2],"], M = ",hc_minus_bd2_mean,"\n",
      "BD-SZ: [",bd_minus_sz2[1],",",bd_minus_sz2[2],"], M = ",bd_minus_sz2_mean,
  sep="")

hc_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)
bd_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)
sz_hdi1<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)

hc_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)
bd_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)
sz_mean1<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)

hc_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated)
bd_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated)
sz_hdi2<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated)

hc_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated)
bd_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated)
sz_mean2<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated)

plot_data<-data.frame(
  group=rep(c("HC","BD","SZ"),2),
  cond=c(rep("Direct",3),rep("Indirect",3)),
  mean=c(hc_mean1,bd_mean1,sz_mean1,hc_mean2,bd_mean2,sz_mean2),
  hdi_lo=c(hc_hdi1[1],bd_hdi1[1],sz_hdi1[1],hc_hdi2[1],bd_hdi2[1],sz_hdi2[1]),
  hdi_hi=c(hc_hdi1[2],bd_hdi1[2],sz_hdi1[2],hc_hdi2[2],bd_hdi2[2],sz_hdi2[2]))

p<-ggplot(data=plot_data, aes(x=cond, y=mean,color=factor(group,levels=c("HC","BD","SZ")),alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-3.5,4))+
  scale_color_manual(values=c('#482475','#287c8e','#addc30'))+
  ggtitle("Drift Rate: Deviated Heads") + xlab("") +
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 1.5, y = 3.22, label = texta, hjust = "center",size=3,lineheight=.9)+
  labs(colour="group")
print(p)

```

## Drift Rate (Group Effects)

We marginalized over all task conditions (emo, then head, then gaze) for drift rates to obtain a measure of overall evidence accumulation efficiency for all three groups. Results showed that HC had credibly higher drift rates than SZ and BD when samples were marginalized over all task conditions.

```{r id61333, echo=FALSE, fig.height=3, fig.show="hold", fig.width=3.5, message=FALSE, warning=FALSE,tidy=T,cache=TRUE}

# delta (across all conditions)
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal)),3)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal)),3)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal)),3)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal)),2)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal)),2)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal)),2)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(1,2.5))+
  ggtitle("Delta (Overall)") + xlab("") +
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+
  annotate(geom = "text", x = 2, y = 2.3, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)

```

## Drift Rate (Emo Effects)

We examined the influence of emotion on drift rates within gaze/head conditions and diagnostic groups to get an overall indicator of the effect of emotion on drift rates. Results show that there are not credible differences in drift rates between emotion conditions (i.e., all of the 90% HDI's below contain zero) in any of the groups or head/gaze conditions. Because of this, we chose to marginalize over emotions conditions for drift rates and drift bias to simplify the analyses in the main text. We chose not to marginalize over gaze and head orientation because we did find at least some credible influence of gaze/head cues (reported below) on drift rates within groups. Additionally, for correlation analyses, we chose to further marginalize over gaze direction for drift rates to reduce the number of comparisons.

__Marginalizing over emotion conditions:__ Marginalizing over emotion conditions may appear counterintuitive when the winning model—accounting for the influences of gaze, head, and emotion on drift rates—outperformed other models that did not also account for the influence of emotion on drift rates. This is an open question, but it is likely that Model 10 captured subtle nuances in how emotion influenced evidence accumulation that helped improve out-of-sample predictions but were not sufficiently large to yield credible condition-level effects, when tested at the group-level as we did. 

One explanation is that Model 10 better captured variability at the subject-level and not the group-level as our tests of condition-level effects examined. In other words, the lack of within-group differences in drift rates based on emotion may not have occurred universally within the subjects of each group. Rather, condition-level emotions effects may have occurred for a subset of individuals within each group. In this case, the added variability introduced by emotion effects for some—but not all—subjects would have been best captured by Model 10. However, if this occurred in just a subset of subjects, these effects would not be sufficiently influential to produce group-level emotion effects in the group-level hyperparameters for the drift rates. 

Although we chose to marginalize over samples in the analyses in the main text, for completeness and transparency we also report the tests of group differences in drift rates before marginalizing over the emotion condition in the supplement section named "Drift Rate (Group/Gaze/Head/Emo Effects)".

```{r id533333, echo=FALSE, fig.height=2, fig.show="hold", fig.width=8, message=FALSE, warning=FALSE, dpi=300, out.width='1500px',cache=TRUE}

#Note: we multiply indirect delta samples by -1 so the direction of the param values for direct/indirect is equivalent and, therefore more easily interpretable (ie for all conditions/contrast, high drift rates=better evidence accumulation)

#make data.frame to hold results
within_effects<-setNames(data.frame(matrix(ncol = 7, nrow = 0)), c("group","cond_gaze","cond_head","emo_contrast", "HDI_lo", "HDI_hi","mean"))

#delta neutral minus fearful emo for FORWARD heads and DIRECT gaze
within_effects[1,]<-c("hc","direct","forward","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_forward_fearful)))
within_effects[2,]<-c("bd","direct","forward","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_forward_fearful)))
within_effects[3,]<-c("sz","direct","forward","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_forward_fearful)))

#delta neutral minus fearful emo for FORWARD heads and INDIRECT gaze (marginalized over emo)
within_effects[4,]<-c("hc","indirect","forward","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_forward_fearful*-1)))
within_effects[5,]<-c("bd","indirect","forward","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_forward_fearful*-1)))
within_effects[6,]<-c("sz","indirect","forward","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_forward_fearful*-1)))

#delta neutral minus fearful emo for deviated heads and DIRECT gaze (marginalized over emo)
within_effects[7,]<-c("hc","direct","deviated","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)),
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_direct_deviated_fearful)))
within_effects[8,]<-c("bd","direct","deviated","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_direct_deviated_fearful)))
within_effects[9,]<-c("sz","direct","deviated","neutral-fearful",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_neutral)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_direct_deviated_fearful)))

#delta neutral minus fearful emo for deviated heads and INDIRECT gaze (marginalized over emo)
within_effects[10,]<-c("hc","indirect","deviated","neutral-fearful",
                       HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful*-1)),
                       mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_indirect_deviated_fearful*-1)))
within_effects[11,]<-c("bd","indirect","deviated","neutral-fearful",
                       HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful*-1)),
                       mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_indirect_deviated_fearful*-1)))
within_effects[12,]<-c("sz","indirect","deviated","neutral-fearful",
                       HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful*-1)),
                       mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_neutral*-1)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_indirect_deviated_fearful*-1)))

#round values
within_effects$HDI_lo<-round(as.numeric(within_effects$HDI_lo),3)
within_effects$HDI_hi<-round(as.numeric(within_effects$HDI_hi),3)
within_effects$mean<-round(as.numeric(within_effects$mean),3)

within_effects %>%
  kbl(caption = "Posterior Differences in Drift Rates Between Emotion Conditions (by Dx Group, Gaze Direction, Head Orientation)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

## Drift Rate (Gaze Effects)

After marginalizing over emotion conditions, we examined the influence of gaze direction on drift rates in all three groups. Results show facilitation effects of gaze direction in all three groups for deviated heads (higher drift rates for indirect gaze when heads are deviated). However, we don't see this as consistently for forwards heads.

```{r id103333, echo=FALSE, fig.height=6, fig.show="hold", fig.width=6, message=FALSE, warning=FALSE, dpi=300, out.width='1500px',cache=TRUE}

#make data.frame to hold results
within_effects<-setNames(data.frame(matrix(ncol = 6, nrow = 0)), c("group","cond_head","gaze_contrast", "HDI_lo", "HDI_hi","mean"))

#Note: here, we multiply indirect delta samples by -1 so the direction of the param values for direct/indirect is equivalent and, therefore more easily interpretable (ie for all conditions/contrast, faster drift rates=better)

#delta direct minus indirect gaze for FORWARD heads (marginalized over emo)
within_effects[1,]<-c("hc","forward","direct-indirect",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward*-1)))
within_effects[2,]<-c("bd","forward","direct-indirect",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward*-1)))
within_effects[3,]<-c("sz","forward","direct-indirect",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward*-1)))

#delta direct minus indirect gaze for DEVIATED heads (marginalized over emo)
within_effects[4,]<-c("hc","deviated","direct-indirect",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated*-1)))
within_effects[5,]<-c("bd","deviated","direct-indirect",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated*-1)))
within_effects[6,]<-c("sz","deviated","direct-indirect",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated*-1)))

#round values
within_effects$HDI_lo<-round(as.numeric(within_effects$HDI_lo),3)
within_effects$HDI_hi<-round(as.numeric(within_effects$HDI_hi),3)
within_effects$mean<-round(as.numeric(within_effects$mean),3)

within_effects %>%
  kbl(caption = "Posterior Differences in Drift Rates Between Gaze Direction Conditions (by Dx Group, Head Orientation)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

## Drift Rate (Head Effects)

After marginalizing over emotion conditions, we examined the influence of head orientation on drift rates in all three groups. Results show credible differences in drift rate based on the head orientation direction of stimuli, such that individuals show more efficient evidence accumulation for forward vs deviated heads. This is found in all three groups.

```{r id113333, echo=FALSE, fig.height=2, fig.show="hold", fig.width=6, message=FALSE, warning=FALSE, dpi=300, out.width='1500px',cache=TRUE}

#Note: here, we multiply indirect delta samples by -1 so the direction of the param values for direct/indirect is equivalent and, therefore more easily interpretable (ie for all conditions/contrast, faster drift rates=better)

#make data.frame to hold results
within_effects<-setNames(data.frame(matrix(ncol = 6, nrow = 0)), c("group","cond_gaze","head_contrast", "HDI_lo", "HDI_hi","mean"))

#delta forward minus deviated head for DIRECT gaze (marginalized over emo)
within_effects[1,]<-c("hc","direct","forward-deviated",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_direct_deviated)))
within_effects[2,]<-c("bd","direct","forward-deviated",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_direct_deviated)))
within_effects[3,]<-c("sz","direct","forward-deviated",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_direct_deviated)))

#delta forward minus deviated head for INDIRECT gaze (marginalized over emo)
within_effects[4,]<-c("hc","indirect","forward-deviated",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward*-1)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_forward*-1)-(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_indirect_deviated*-1)))
within_effects[5,]<-c("bd","indirect","forward-deviated",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward*-1)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_forward*-1)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_indirect_deviated*-1)))
within_effects[6,]<-c("sz","indirect","forward-deviated",
                      HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward*-1)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated*-1)),
                      mean((subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_forward*-1)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_indirect_deviated*-1)))

#round values
within_effects$HDI_lo<-round(as.numeric(within_effects$HDI_lo),3)
within_effects$HDI_hi<-round(as.numeric(within_effects$HDI_hi),3)
within_effects$mean<-round(as.numeric(within_effects$mean),3)

within_effects %>%
  kbl(caption = "Posterior Differences in Drift Rates Between Head Orientation Conditions (by Dx Group, Gaze Direction)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

## Drift Bias (Group/Head Effects)

These results are presented in the main text. After marginalizing over emotion conditions, we examined the influence of diagnostic group on drift bias within forward and deviated head orientations. Results showed that HC had credibly lower drift bias than BD in forward and deviated head conditions. HC only showed credibly lower drift bias than SZ in deviated head conditions.

```{r id83333, echo=FALSE, fig.height=3, fig.show="hold", fig.width=3.5, message=FALSE, warning=FALSE,cache=TRUE}

#delta bias - forward
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_forward)),3)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_forward)),3)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_forward)),3)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_forward)),3)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_forward)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_forward)),3)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_forward)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_forward)),3)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_forward)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_forward)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_forward)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_forward)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_forward)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_forward)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-1.5,.5))+
  ggtitle("Drift Bias: Forward Heads") + xlab("") +
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 2, y = -1, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)

#delta bias - deviated
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_deviated)),3)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_deviated)),3)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_deviated)),3)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_deviated)),3)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_deviated)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_deviated)),3)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_deviated)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_deviated)),3)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_deviated)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_deviated)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_deviated)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias_deviated)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias_deviated)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias_deviated)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-1.5,.5))+
  ggtitle("Drift Bias: Deviated Heads") + xlab("") +
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+ 
  annotate(geom = "text", x = 2, y = -.05, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)

```

## Drift Bias (Group Effects)

We examined the influence of diagnostic group on drift bias after marginalizing over emotion and head conditions to get an an overall indicator of the effect of group on drift bias. Results showed that, in general, HC had credibly lower drift bias than BD and SZ.

```{r id81333, echo=FALSE, fig.height=3, fig.show="hold", fig.width=3.5, message=FALSE, warning=FALSE,cache=TRUE}

#delta bias - overall
hc_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias)),3)
hc_minus_bd<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias)),3)
bd_minus_sz<-round(HDIofMCMC((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias)),3)

hc_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias)),3)
hc_minus_bd_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias)-(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias)),3)
bd_minus_sz_mean<-round(mean((subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias)-(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias)),3)

texta<-paste("HC-SZ: [",hc_minus_sz[1],",",hc_minus_sz[2],"], M = ",hc_minus_sz_mean,"\n",
      "HC-BD: [",hc_minus_bd[1],",",hc_minus_bd[2],"], M = ",hc_minus_bd_mean,"\n",
      "BD-SZ: [",bd_minus_sz[1],",",bd_minus_sz[2],"], M = ",bd_minus_sz_mean,sep="")

hc_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias)
bd_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias)
sz_hdi<-HDIofMCMC(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias)

hc_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="HC")$delta_marginal_bias)
bd_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="BD")$delta_marginal_bias)
sz_mean<-mean(subset(all_mu_samples,all_mu_samples$group=="SZ")$delta_marginal_bias)

plot_data<-data.frame(
  group=c("HC","BD","SZ"),
  mean=c(hc_mean,bd_mean,sz_mean),
  hdi_lo=c(hc_hdi[1],bd_hdi[1],sz_hdi[1]),
  hdi_hi=c(hc_hdi[2],bd_hdi[2],sz_hdi[2]))

p<-ggplot(data=plot_data, aes(x=group, y=mean,color=group,alpha=.4)) +
  geom_point(data=plot_data,alpha=1,position=position_dodge(width = .5),size=3) + theme_bw()+
  geom_errorbar(data=plot_data,aes(ymin=hdi_lo,ymax=hdi_hi),linewidth=.8,
                width=0,alpha=1,position=position_dodge(width = .5)) +
  coord_cartesian(ylim = c(-.9,0.03))+
  ggtitle("Drift Bias (Overall)") + xlab("") +
  scale_x_discrete(limits=c("HC","BD","SZ"))+
  scale_color_manual(values=c('#287c8e','#482475','#addc30'))+
  ylab("Parameter Estimate")+
  theme(axis.text.x = element_text(angle = 90,vjust=1,hjust=1))+
  annotate(geom = "text", x = 2, y = -.05, label = texta, hjust = "center",size=3,lineheight=.9)
print(p)

```

## Drift Bias (Head Effects)

We calculated 90% HDIs for drift biases within both head orientation conditions after marginalizing over emotion conditions and groups. Results showed that biases toward self referential choices tended to be higher when heads were forward and lower when heads were deviated.

```{r id933333, echo=FALSE, fig.height=3, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE,cache=TRUE}

temp_for<-rowMeans(cbind(all_mu_samples$delta_marginal_bias_forward[which(all_mu_samples$group=="HC")],
                         all_mu_samples$delta_marginal_bias_forward[which(all_mu_samples$group=="BD")],
                         all_mu_samples$delta_marginal_bias_forward[which(all_mu_samples$group=="SZ")]))
temp_dev<-rowMeans(cbind(all_mu_samples$delta_marginal_bias_deviated[which(all_mu_samples$group=="HC")],
                         all_mu_samples$delta_marginal_bias_deviated[which(all_mu_samples$group=="BD")],
                         all_mu_samples$delta_marginal_bias_deviated[which(all_mu_samples$group=="SZ")]))

output<-data.frame(head=c('forward','deviated'),
                     hdi_lo=c(HDIofMCMC(temp_for)[1],HDIofMCMC(temp_dev)[1]),
                     hdi_hi=c(HDIofMCMC(temp_for)[2],HDIofMCMC(temp_dev)[2]),
                     mean=c(mean(temp_for),mean(temp_dev)))

output %>%
  kbl(caption = "Drift Bias by Head Orientation Marginalized Over Group",valign = "t",digits=3) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

# Preprocessing Correlates

Before conducting analyses on individual differences (correlations, regressions), we performed necessary preprocessing steps on our variables of interest.

1.  For subject-level drift rates and drift biases, we marginalized over emotion conditions (as was done for group-level parameters). To reduce the number of tests/comparisons correlations and regressions, we also marginalized over gaze conditions for drift rates. This was done by flipping the sign of drift rates for indirect gaze (which were originally negative-going) and then averaging over subject-level posteriors for direct and indirect gaze. This was done separately for both forward and deviated head conditions. As a result, for all subejcts, we had measures of drift rates and drift biases in both forward and deviated head conditions.

2.  To retain as much data as possible, we also winsorized the outermost .01 of data for variables that contained outliers.

```{r id1233333, fig.height=9, fig.show="hold", fig.width=9, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, include=FALSE, out.width='1500px'}

###############################################################################
# PART 1a - Process traditional gaze task metrics (Acc/RT)
###############################################################################

#process gaze task RT,  acc, and % yes summary values
real_data<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/gaze_beh.csv")
real_data$subjID<-match(real_data$subj, unique(real_data$subj)) #subj IDs to sequential indexes 

groups<-unique(real_data$group)
subjs<-unique(real_data$subj)

acc_rt_summary<-data.frame(subj=subjs)

## overall acc and RT
for (i in 1:length(subjs)){
  tmpdata<-subset(real_data,real_data$subj==subjs[i])
  tmp_summary<-data.frame(acc_all=mean(tmpdata$acc,na.rm=TRUE),
                          rt_all=mean(tmpdata$rt,na.rm=TRUE),
                          yes_all=length(which(tmpdata$choice == 1))/length(tmpdata$choice))
  if(i==1){
    running_summary<-tmp_summary
  }else{
    running_summary<-rbind(running_summary,tmp_summary)
  }
}

acc_rt_summary<-cbind(acc_rt_summary,running_summary)

#find forward head (cond_other_head=1), calculate acc, rt, yes for that
find<-which(real_data$cond_other_head==1)
tmp_real_data<-real_data[-find,]

for (i in 1:length(subjs)){
  tmpdata<-subset(tmp_real_data,tmp_real_data$subj==subjs[i])
  tmp_summary<-data.frame(acc_for=mean(tmpdata$acc,na.rm=TRUE),
                          rt_for=mean(tmpdata$rt,na.rm=TRUE),
                          yes_for=length(which(tmpdata$choice == 1))/length(tmpdata$choice))
  if(i==1){
    running_summary<-tmp_summary
  }else{
    running_summary<-rbind(running_summary,tmp_summary)
  }
}

acc_rt_summary<-cbind(acc_rt_summary,running_summary)

#find deviated head (cond_other_head=2), calculate acc, rt, yes for that
find<-which(real_data$cond_other_head==2)
tmp_real_data<-real_data[-find,]

for (i in 1:length(subjs)){
  tmpdata<-subset(tmp_real_data,tmp_real_data$subj==subjs[i])
  tmp_summary<-data.frame(acc_dev=mean(tmpdata$acc,na.rm=TRUE),
                          rt_dev=mean(tmpdata$rt,na.rm=TRUE),
                          yes_dev=length(which(tmpdata$choice == 1))/length(tmpdata$choice))
  if(i==1){
    running_summary<-tmp_summary
  }else{
    running_summary<-rbind(running_summary,tmp_summary)
  }
}

acc_rt_summary<-cbind(acc_rt_summary,running_summary)

write.csv(acc_rt_summary,file="/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/correlates/gaze_acc_rt_performance.csv",row.names = FALSE)

###############################################################################
# PART 1b - Process traditional signal detection gaze task metrics 
###############################################################################

real_data$hits<-ifelse(real_data$cond==1&real_data$choice==1,1,0)
real_data$miss<-ifelse(real_data$cond==1&real_data$choice==2,1,0)
real_data$fa<-ifelse(real_data$cond==2&real_data$choice==1,1,0) #false alarm
real_data$cr<-ifelse(real_data$cond==2&real_data$choice==2,1,0) #correct rejection

# sum SDT metrics within each subj, head condition
sdt_summary <- real_data %>%
  dplyr::group_by(group, subj, cond_other_head) %>%
  dplyr::summarise(n_hits = sum(hits),
            n_fa = sum(fa), #false alarm
            n_miss = sum(miss),
            n_cr = sum(cr), #corr rejection
            n_present_trials = n_hits + n_miss,
            n_absent_trials = n_fa + n_cr,
            hit_rate = n_hits / n_present_trials,
            fa_rate = n_fa / n_absent_trials)

# CALCULATE DPRIME
# Note: replace rates of 0 with 0.5/𝑛and rates of 1 with (𝑛−0.5)/𝑛where 𝑛is the number of signal or noise trials 
 
get_dprime <- function(hit_rate, false_alarm_rate,n_present_trials,n_absent_trials) { #rate should be calculated at subject level in proportion
  
  # Avoid perfect hit or false alarm rates (Macmillan & Kaplan, 1985)
  if (hit_rate == 1) hit_rate <- 1 - (1 / (2 * n_present_trials))
  if (hit_rate == 0) hit_rate <- 1 / (2 * n_present_trials)
  if (false_alarm_rate == 1) false_alarm_rate <- 1 - (1 / (2 * n_absent_trials))
  if (false_alarm_rate == 0) false_alarm_rate <- 1 / (2 * n_absent_trials)
  
  # Convert hit rate and false alarm rate to probabilities and calculate dprime
  d_prime <- qnorm(hit_rate) - qnorm(false_alarm_rate)
  
  return(d_prime)
}

library(tidyr)

# summarize in wide format by head orientation conditions
full_sdt_summary <- sdt_summary %>%
  dplyr::group_by(subj,cond_other_head, hit_rate,fa_rate,n_hits,n_miss,n_fa,n_cr) %>%
  dplyr::summarise(d_prime = get_dprime(hit_rate, fa_rate, n_present_trials,n_absent_trials))

full_sdt_summary <- pivot_wider(sdt_summary, id_cols = subj, names_from = cond_other_head, values_from = c("hit_rate","fa_rate","n_hits","n_miss","n_fa","n_cr"))

#do the same across head orientation conditions
sdt_summary_all <- real_data %>%
  dplyr::group_by(group, subj) %>%
  dplyr::summarise(n_hits_all = sum(hits),
            n_fa_all = sum(fa), #false alarm
            n_miss_all = sum(miss),
            n_cr_all = sum(cr), #corr rejection
            n_present_trials_all = n_hits_all + n_miss_all,
            n_absent_trials_all = n_fa_all + n_cr_all,
            hit_rate_all = n_hits_all / n_present_trials_all,
            fa_rate_all = n_fa_all / n_absent_trials_all)

full_sdt_summary_all <- sdt_summary_all %>%
  dplyr::group_by(subj, hit_rate_all,fa_rate_all,n_hits_all,n_miss_all,n_fa_all,n_cr_all) %>%
  dplyr::summarise(d_prime_all = get_dprime(hit_rate_all, fa_rate_all, n_present_trials_all,n_absent_trials_all))

full_sdt_summary_merged <- merge(full_sdt_summary_all, full_sdt_summary,
                              all.x=TRUE, by.y = "subj",no.dups = T)

write.csv(full_sdt_summary_merged,file="/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/correlates/sdt_performance.csv",row.names = FALSE)

###############################################################################
# PART 1c - MODEL traditional signal detection gaze task metrics in Stan
###############################################################################

stan_sdt_file<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/correlates/sdt_performance_stan.csv"

sdt_chains<-4
sdt_warmup_draws<-1000 #postwarmup draws per chain
sdt_postwarmup_draws<-10000 #postwarmup draws per chain
sdt_path<-'/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/correlates/sdt_stan/'

if(!file.exists(stan_sdt_file)){
  
  # source scripts
  stanmodelname<-'/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/scripts/gaze_sdt.stan'
  
  # Compile the Stan model
  model <- cmdstan_model(stanmodelname)
  divs<-c()
  
  for (i in groups){
    tmp_sdt_summary<-subset(sdt_summary,sdt_summary$group==i)
    full_sdt_summary <- pivot_wider(tmp_sdt_summary, id_cols = subj, names_from = cond_other_head, values_from =
                                      c("n_hits","n_fa","n_present_trials","n_absent_trials"))
    tmp_subj<-full_sdt_summary$subj
    
    full_sdt_summary<-t(full_sdt_summary)
    row_names <- rownames(full_sdt_summary)
    
    data_stan_sdt <- list(
      nWCon = 2, #num head conditions
      nSub = ncol(full_sdt_summary),
      s = full_sdt_summary[which(grepl("^n_present", row_names)),], #num signal trials
      n = full_sdt_summary[which(grepl("^n_absent", row_names)),], #num noise trials
      h = full_sdt_summary[which(grepl("^n_hits", row_names)),],  # Add hits as observed data
      f = full_sdt_summary[which(grepl("^n_fa", row_names)),]   # Add false alarms as observed data
    )
    
    # Run the model
    fit <- model$sample(data=data_stan_sdt, 
                        iter_warmup=sdt_warmup_draws, 
                        iter_sampling=sdt_postwarmup_draws,
                        init=0, 
                        chains=sdt_chains, 
                        parallel_chains=sdt_chains, #num cores
                        save_warmup = FALSE,
                        adapt_delta=0.95,
                        max_treedepth=10,
                        step_size=1,
                        refresh=100,
                        seed=42,
                        output_dir=sdt_path,
                        diagnostics = c("divergences", "treedepth", "ebfmi"))
    
    fitname<-paste(sdt_path,"stanfit_group",i,".RData",sep = "") #save workspace as .RData file
    fit$save_object(fitname)
    draws<-fit$draws(variables = c("c","d"),format = "df")
    
    # get samples separate for two different head conditions
    c1_samples<-draws[,grep("^c\\[1", colnames(draws))]
    c2_samples<-draws[,grep("^c\\[2", colnames(draws))]
    d1_samples<-draws[,grep("^d\\[1", colnames(draws))]
    d2_samples<-draws[,grep("^d\\[2", colnames(draws))]
    
    c_avg<-data.frame(mean=colMeans((c1_samples+c2_samples)/2))
    d_avg<-data.frame(mean=colMeans((d1_samples+d2_samples)/2))
    
    c_names<-rownames(c_avg)
    d_names<-rownames(d_avg)
    
    combined_estimates_allCond<-data.frame(subj=tmp_subj,
                                           c_stan_all=c_avg[grep("^c", c_names),],
                                           d_stan_all=d_avg[grep("^d", d_names),]) 
    
    # Get sampler parameters
    sampler_params <- fit$sampler_diagnostics(format = "df")
    divs[i]<-sum(sampler_params$divergent__)
    
    colnames(draws)<-paste("grp",i,"_",colnames(draws),sep="")
    
    if(i==1){
      stan_samples_all<-draws
      all_estimates_allCond<-combined_estimates_allCond
    }else{
      stan_samples_all<-cbind(stan_samples_all,draws)
      all_estimates_allCond<-rbind(all_estimates_allCond,combined_estimates_allCond)
    }
  }
  
  stan_samples_file<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/correlates/sdt_stan_samples.csv"
  write.csv(stan_samples_all,stan_samples_file,row.names = FALSE)
  write.csv(all_estimates_allCond,file=stan_sdt_file,row.names = FALSE)
}

# Note: Stan and JAGS model estimates are highly similar. They are almost perfectly postively correlated.

###############################################################################
# PART 2 - Process the external correlates (single observation for each subject)
#
# Measures:
# Demographics: age, sex years of education
# 
# General Cognition = BACS
# Emotion-Related Social Cognition = MSCEIT
# Real world social functioning = SASSR Social/Leisure Scale
# Positive and Negative Symptoms = SAPS, SANS
# Other clinical metrics: Meds (CPZeq), duration of illness
#
###############################################################################

datadir<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/correlates/"
setwd(datadir)

#read in all .csv files
files <- (Sys.glob("*.csv"))
start<-data.frame(subj=subjs)
rownames(start)<-start$subj

for (i in 1:length(files)){
  if(files[i]!="sdt_stan_samples.csv"){ #skip stan mcmc samples file
    this_file<-data.frame(read.csv(files[i]))
    num_cols<-dim(this_file)[2]
    tmp_data<-merge(start, this_file,all.x=TRUE,by.x = names(start), by.y = "subj",no.dups = T)
  
    if(i==1){
      all_correlates<-tmp_data
    }else{
      all_correlates<-cbind(all_correlates,tmp_data)
    }
  }
}

#get rid of extra subject columns
tmp_col<-grep("subj", colnames(all_correlates),fixed = TRUE) 
tmp_col<-tmp_col[-1]
all_correlates<-all_correlates[,-tmp_col]

# score real world functional data (SASSR). 

## invert scores so higher scores = better functioning (for interpretation) (because raw data (scores range from 1 to 5 where 1 is better social functioning))
all_correlates$SASSR_Social_Leisure<-(6-all_correlates$SASSR_Social_Leisure)

# calculate illness duration
all_correlates$clin_illness_duration<-all_correlates$age-all_correlates$clin_age_onset

###############################################################################
# PART 3 - Calculate CPZeq for patients on antipsychotic medications
###############################################################################

round1<-dplyr::select(all_correlates,c('subj','antipsychotic1','dose1','q1','route1'))
find_na<-which(is.na(round1$antipsychotic1))
round1<-round1[-find_na,]

round1<-to_cpz(round1, ap_label = "antipsychotic1", dose_label = "dose1", route = "mixed", route_label = "route1", q_label = "q1", key=gardner2010) 
round1<-dplyr::select(round1,c('subj','cpz_conv_factor','cpz_eq'))
colnames(round1)<-c('subj','cpz_conv_factor1','cpz_eq1')
all_correlates<-merge(all_correlates, round1,all.x=TRUE,by.x = "subj", by.y = "subj",no.dups = T)

round2<-dplyr::select(all_correlates,c('subj','antipsychotic2','dose2','q2','route2'))
find_na<-which(is.na(round2$antipsychotic2))
round2<-round2[-find_na,]
round2$q2<-as.numeric(round2$q2)

round2<-to_cpz(round2, ap_label = "antipsychotic2", dose_label = "dose2", route = "mixed", route_label = "route2", q_label = "q2", key=gardner2010) 
round2<-dplyr::select(round2,c('subj','cpz_conv_factor','cpz_eq'))
colnames(round2)<-c('subj','cpz_conv_factor2','cpz_eq2')
all_correlates<-merge(all_correlates, round2,all.x=TRUE,by.x = "subj", by.y = "subj",no.dups = T)

#Note: SANS scored based on factor scores of Sayers, Curran, and Mueser, (1996): Diminished expression (EXP) and MAP (Social Amotivation factor). Items were multipled by factor weights for factors that they loaded significantly onto and then averaged across items within each factor. we did not include the inattention-alogia factor because the SANS inattention scale data was not collected (poor reliability)

for (i in 1:dim(all_correlates)[1]){
  if(is.na(all_correlates$meds_antipsychotic_user[i])){
  }else{
    if(all_correlates$meds_antipsychotic_user[i]==0){#if we know the person was not taking antipsychotics, change any NA CPZeq values to 0
      if(is.na(all_correlates$cpz_eq1[i])){
        all_correlates$cpz_eq1[i]<-0
      }
      if(is.na(all_correlates$cpz_eq2[i])){
        all_correlates$cpz_eq2[i]<-0
      }
    }
  }
}

all_correlates$meds_CPZeq_wHC<-rowSums(cbind(all_correlates$cpz_eq1,all_correlates$cpz_eq2),na.rm=T)
all_correlates$meds_CPZeq_noHC<-rowSums(cbind(all_correlates$cpz_eq1,all_correlates$cpz_eq2),na.rm=T)

for (i in 1:dim(all_correlates)[1]){
  if(all_correlates$group_name[i]==1){ #if person is control, change CPZ to 0 for meds_CPZeq_wHC and change CPZ to NA for meds_CPZeq_noHC
    all_correlates$meds_CPZeq_wHC[i]<-0
    all_correlates$meds_CPZeq_noHC[i]<-NA
  }else{
    if(is.na(all_correlates$meds_antipsychotic_user[i])){ #if data on antipsychotic use not collected, mark CPZeq as NA
      all_correlates$meds_CPZeq_wHC[i]<-NA
      all_correlates$meds_CPZeq_noHC[i]<-NA
    }else{
      if(all_correlates$meds_has_antipsychotic_dose_data[i]==0){ #if antipsychotic names, but not doses were collected, mark CPZ as NA
        all_correlates$meds_CPZeq_wHC[i]<-NA
        all_correlates$meds_CPZeq_noHC[i]<-NA
      }
    }
  }
}

write.csv(all_correlates,file="/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/data/all_correlates.csv",row.names = F)

###############################################################################
# PART 4 - Extract subject-level parameters from posterior samples of DDM
###############################################################################

#grab subject data 
tmp_col<-grep("sub_", colnames(alldata),fixed = TRUE) 
subdata<-alldata[,tmp_col] 

# grab alpha data
tmp_col<-grep("alpha", colnames(subdata),fixed = TRUE) 
alpha_data<-colMeans(subdata[,tmp_col])

#grab beta data
tmp_col<-grep("beta", colnames(subdata),fixed = TRUE) 
beta_data<-colMeans(subdata[,tmp_col])

#grab ndt data
tmp_col<-grep("ndt", colnames(subdata),fixed = TRUE) 
ndt_data<-colMeans(subdata[,tmp_col])

#grab delta present data
tmp_col<-grep("delta_present", colnames(subdata),fixed = TRUE) 

delta_present_forward_neutral_samples<-subdata[,tmp_col][1:100]
delta_present_forward_fearful_samples<-subdata[,tmp_col][101:200]
delta_present_deviated_neutral_samples<-subdata[,tmp_col][201:300]
delta_present_deviated_fearful_samples<-subdata[,tmp_col][301:400]

delta_present_forward_neutral_data<-colMeans(delta_present_forward_neutral_samples)
delta_present_forward_fearful_data<-colMeans(delta_present_forward_fearful_samples)
delta_present_deviated_neutral_data<-colMeans(delta_present_deviated_neutral_samples)
delta_present_deviated_fearful_data<-colMeans(delta_present_deviated_fearful_samples)

#grab delta absent data
tmp_col<-grep("delta_absent", colnames(subdata),fixed = TRUE) 

delta_absent_forward_neutral_samples<-subdata[,tmp_col][1:100]
delta_absent_forward_fearful_samples<-subdata[,tmp_col][101:200]
delta_absent_deviated_neutral_samples<-subdata[,tmp_col][201:300]
delta_absent_deviated_fearful_samples<-subdata[,tmp_col][301:400]

delta_absent_forward_neutral_data<-colMeans(delta_absent_forward_neutral_samples)
delta_absent_forward_fearful_data<-colMeans(delta_absent_forward_fearful_samples)
delta_absent_deviated_neutral_data<-colMeans(delta_absent_deviated_neutral_samples)
delta_absent_deviated_fearful_data<-colMeans(delta_absent_deviated_fearful_samples)

###########################################################################
# PART 4a - calculate overall drift rate for forward and deviated heads
###########################################################################

##marginalize over emo conditions
delta_marginal_present_forward_samples<-(delta_present_forward_neutral_samples+delta_present_forward_fearful_samples)/2
delta_marginal_present_deviated_samples<-(delta_present_deviated_neutral_samples+delta_present_deviated_fearful_samples)/2

delta_marginal_absent_forward_samples<-(delta_absent_forward_neutral_samples+delta_absent_forward_fearful_samples)/2
delta_marginal_absent_deviated_samples<-(delta_absent_deviated_neutral_samples+delta_absent_deviated_fearful_samples)/2

delta_marginal_present_forward_data<-colMeans(delta_marginal_present_forward_samples)
delta_marginal_present_deviated_data<-colMeans(delta_marginal_present_deviated_samples)

delta_marginal_absent_forward_data<-colMeans(delta_marginal_absent_forward_samples)
delta_marginal_absent_deviated_data<-colMeans(delta_marginal_absent_deviated_samples)

#Now marginalize over gaze direction

#marginalize over gaze direction for FORWARD heads - need to invert the indirect gaze condition samples (ie multiply by -1 so higher=better for all)
delta_marginal_forward_samples<-(delta_marginal_present_forward_samples+(delta_marginal_absent_forward_samples*-1))/2

#marginalize over gaze direction for DEVIATED heads - need to invert the indirect gaze condition samples (ie multiply by -1 so higher=better for all)
delta_marginal_deviated_samples<-(delta_marginal_present_deviated_samples+(delta_marginal_absent_deviated_samples*-1))/2

delta_marginal_forward_data<-colMeans(delta_marginal_forward_samples)
delta_marginal_deviated_data<-colMeans(delta_marginal_deviated_samples)

#Finally, let's marginalize over forward and deviated head
delta_marginal_samples<-(delta_marginal_forward_samples+delta_marginal_deviated_samples)/2
delta_marginal_data<-colMeans(delta_marginal_samples)
 
###########################################################################
# PART 4b - calculate bias in drift rate (bias towards direct gaze)
###########################################################################

#calculate drift bias within head conditions only

##marginalize over emo conditions
delta_marginal_present_forward_samples<-(delta_present_forward_neutral_samples+delta_present_forward_fearful_samples)/2
delta_marginal_present_deviated_samples<-(delta_present_deviated_neutral_samples+delta_present_deviated_fearful_samples)/2

delta_marginal_absent_forward_samples<-(delta_absent_forward_neutral_samples+delta_absent_forward_fearful_samples)/2
delta_marginal_absent_deviated_samples<-(delta_absent_deviated_neutral_samples+delta_absent_deviated_fearful_samples)/2

#here, we do NOT flip indirect gaze samples because we want to account for bias in drift rates toward direct gaze

#marginalize over gaze direction for FORWARD heads
delta_marginal_forward_samples<-(delta_marginal_present_forward_samples+delta_marginal_absent_forward_samples)/2

#marginalize over gaze direction for DEVIATED heads
delta_marginal_deviated_samples<-(delta_marginal_present_deviated_samples+delta_marginal_absent_deviated_samples)/2

delta_bias_marginal_forward_data<-colMeans(delta_marginal_forward_samples)
delta_bias_marginal_deviated_data<-colMeans(delta_marginal_deviated_samples)

# marginalize over forward and deviated heads -- to get drift bias -- and average over all samples.
delta_bias_marginal_samples<-(delta_marginal_forward_samples+delta_marginal_deviated_samples)/2
delta_bias_marginal_data<-colMeans(delta_bias_marginal_samples)

#calculate drift bias within head and emo conditions

##here, we do NOT flip indirect gaze samples because we want to account for bias in drift rates toward direct gaze

#marginalize over gaze direction for FORWARD heads / NEUTRAL emo
delta_marginal_forward_neutral_samples<-(delta_present_forward_neutral_samples+delta_absent_forward_neutral_samples)/2
#marginalize over gaze direction for FORWARD heads / FEARFUL emo
delta_marginal_forward_fearful_samples<-(delta_present_forward_fearful_samples+delta_absent_forward_fearful_samples)/2
#marginalize over gaze direction for DEVIATED heads / NEUTRAL emo
delta_marginal_deviated_neutral_samples<-(delta_present_deviated_neutral_samples+delta_absent_deviated_neutral_samples)/2
#marginalize over gaze direction for DEVIATED heads / NEUTRAL emo
delta_marginal_deviated_fearful_samples<-(delta_present_deviated_fearful_samples+delta_absent_deviated_fearful_samples)/2

delta_bias_marginal_forward_neutral_data<-colMeans(delta_marginal_forward_neutral_samples)
delta_bias_marginal_forward_fearful_data<-colMeans(delta_marginal_forward_fearful_samples)
delta_bias_marginal_deviated_neutral_data<-colMeans(delta_marginal_deviated_neutral_samples)
delta_bias_marginal_deviated_fearful_data<-colMeans(delta_marginal_deviated_fearful_samples)

############################################################################

#get the group that each subj belongs to
grp_data<-real_data %>% distinct(subj,.keep_all=TRUE)

#combine the data
all_sub_ddm_data<-data.frame(subj=unique(real_data$subj),
                         group=grp_data$group,
                         alpha=alpha_data,
                         beta=beta_data,
                         delta=delta_marginal_data,
                         delta_bias=delta_bias_marginal_data,
                         delta_direct_forward_neutral=delta_present_forward_neutral_data,
                         delta_direct_forward_fearful=delta_present_forward_fearful_data,
                         delta_direct_deviated_neutral=delta_present_deviated_neutral_data,
                         delta_direct_deviated_fearful=delta_present_deviated_fearful_data,
                         delta_indirect_forward_neutral=delta_absent_forward_neutral_data*-1,
                         delta_indirect_forward_fearful=delta_absent_forward_fearful_data*-1,
                         delta_indirect_deviated_neutral=delta_absent_deviated_neutral_data*-1,
                         delta_indirect_deviated_fearful=delta_absent_deviated_fearful_data*-1,
                         delta_bias_for_neut=delta_bias_marginal_forward_neutral_data,
                         delta_bias_for_fear=delta_bias_marginal_forward_fearful_data,
                         delta_bias_dev_neut=delta_bias_marginal_deviated_neutral_data,
                         delta_bias_dev_fear=delta_bias_marginal_deviated_fearful_data,
                         delta_for=delta_marginal_forward_data,
                         delta_dev=delta_marginal_deviated_data,
                         delta_bias_for=delta_bias_marginal_forward_data,
                         delta_bias_dev=delta_bias_marginal_deviated_data,
                         delta_direct_for=delta_marginal_present_forward_data,
                         delta_direct_dev=delta_marginal_present_deviated_data,
                         delta_indirect_for=delta_marginal_absent_forward_data*-1,
                         delta_indirect_dev=delta_marginal_absent_deviated_data*-1,
                         ndt=ndt_data)

#merge DDM param data and correlates based on matching subj ID's
all_sub_data<-merge(all_sub_ddm_data, all_correlates,all.x=TRUE,by.x = "subj", by.y = "subj",no.dups = T)

#save to final_fit directory
filename<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/sub_pars_w_correlates.RData"
save(all_sub_data,file = filename)
filename<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/sub_pars_w_correlates.csv"
write.csv(all_sub_data,file=filename,row.names=F)

#############################################################################
# Clear large variables and reload concise version of data
#############################################################################

rm(list=grep("*samples",ls(),value=TRUE,invert=FALSE))
rm(list=grep("tmp_*",ls(),value=TRUE,invert=FALSE))

invisible(gc())

#load individual subj params combined with subj-level correlates
all_sub_data<-read.csv("/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/modeling/ddm/output/hddm_m10/final_fit/sub_pars_w_correlates.csv")

all_sub_data<-dplyr::select(all_sub_data,c('subj','alpha','beta','ndt',
                                           'delta_for','delta_dev',
                                           'delta_direct_forward_neutral','delta_indirect_forward_neutral',
                                           'delta_direct_forward_fearful','delta_indirect_forward_fearful',
                                           'delta_direct_deviated_neutral','delta_indirect_deviated_neutral',
                                           'delta_direct_deviated_fearful','delta_indirect_deviated_fearful',
                                           'delta_bias_for','delta_bias_dev',
                                           'delta_bias_for_neut','delta_bias_dev_neut',
                                           'delta_bias_for_fear','delta_bias_dev_fear',
                                           'rt_all','acc_all','yes_all',
                                           'rt_for','acc_for','yes_for',
                                           'rt_dev','acc_dev','yes_dev',
                                           'hit_rate_all',
                                           'hit_rate_1','hit_rate_2',
                                           'fa_rate_all',
                                           'fa_rate_1','fa_rate_2',
                                           'c_stan_all','d_stan_all',
                                           'BACS','MSCEIT','group',
                                           'clin_illness_duration','saps_Hallucination',
                                           'saps_Delusion','saps_PersecDelusion','saps_RefDelusion',
                                           'diagnosis','asrm','bdi',
                                           'sans_MAP','sans_EXP',
                                           'SASSR_Social_Leisure',
                                           'meds_antipsychotic_user',
                                           'meds_antidepress_user','meds_mood_stab_user',
                                           'meds_stimulant_user','meds_hypnotic_user',
                                           'meds_anxiolytic_user','meds_anticholinergic_user',
                                           'meds_CPZeq_wHC','meds_CPZeq_noHC',
                                           'age','sex','edu','parent_edu','race'))

#dummy code categorical variables
race_dummy_code<-data.frame(dummy.code(all_sub_data$race))
dx_dummy_code<-data.frame(dummy.code(all_sub_data$diagnosis))
sex_dummy_code<-data.frame(dummy.code(all_sub_data$sex))
all_sub_data<-cbind(all_sub_data,race_dummy_code,dx_dummy_code,sex_dummy_code)
all_sub_data$group<-as.factor(all_sub_data$group)

# winsorize extreme acc and delta bias value (due to 1 subject P06 with extreme but valid data)
all_sub_data$acc_all<-winsor(all_sub_data$acc_all,trim=.01)
all_sub_data$rt_all<-winsor(all_sub_data$rt_all,trim=.01)
all_sub_data$yes_all<-winsor(all_sub_data$yes_all,trim=.01)
all_sub_data$delta_bias_for<-winsor(all_sub_data$delta_bias_for,trim=.01) 
all_sub_data$delta_bias_dev<-winsor(all_sub_data$delta_bias_dev,trim=.01) 

all_sub_data$delta_bias_for_neut<-winsor(all_sub_data$delta_bias_for_neut,trim=.01) 
all_sub_data$delta_bias_dev_neut<-winsor(all_sub_data$delta_bias_dev_neut,trim=.01) 
all_sub_data$delta_bias_for_fear<-winsor(all_sub_data$delta_bias_for_fear,trim=.01) 
all_sub_data$delta_bias_dev_fear<-winsor(all_sub_data$delta_bias_dev_fear,trim=.01) 

all_sub_data$delta_direct_forward_neutral<-winsor(all_sub_data$delta_direct_forward_neutral,trim=.01) 
all_sub_data$delta_direct_forward_fearful<-winsor(all_sub_data$delta_direct_forward_fearful,trim=.01) 
all_sub_data$delta_indirect_forward_neutral<-winsor(all_sub_data$delta_indirect_forward_neutral,trim=.01) 
all_sub_data$delta_indirect_forward_fearful<-winsor(all_sub_data$delta_indirect_forward_fearful,trim=.01) 
all_sub_data$delta_direct_deviated_neutral<-winsor(all_sub_data$delta_direct_deviated_neutral,trim=.01) 
all_sub_data$delta_direct_deviated_fearful<-winsor(all_sub_data$delta_direct_deviated_fearful,trim=.01) 
all_sub_data$delta_indirect_deviated_neutral<-winsor(all_sub_data$delta_indirect_deviated_neutral,trim=.01) 
all_sub_data$delta_indirect_deviated_fearful<-winsor(all_sub_data$delta_indirect_deviated_fearful,trim=.01) 

all_sub_data$delta_for<-winsor(all_sub_data$delta_for,trim=.01) 
all_sub_data$delta_dev<-winsor(all_sub_data$delta_dev,trim=.01)

all_sub_data$meds_CPZeq_noHC<-winsor(all_sub_data$meds_CPZeq_noHC,trim=.01) 
all_sub_data$c_stan_all<-winsor(all_sub_data$c_stan_all,trim=.01) 
all_sub_data$fa_rate_all<-winsor(all_sub_data$fa_rate_all,trim=.01) 

levels(all_sub_data$group)[levels(all_sub_data$group) == "1"] = "HC"
levels(all_sub_data$group)[levels(all_sub_data$group) == "2"] = "BD"
levels(all_sub_data$group)[levels(all_sub_data$group) == "3"] = "SZ"

```

# SDT Analysis

We performed additional signal detection analyses to extract measures of sensitivity (discriminability parameter) and bias (criterion parameter) from participants' choices on the gaze task. To do so, we programmed an equal variance gaussian SDT model in Stan that accounted for effects of head orientation on participant's discriminability and criterion values. The SDT model was a hierarchical Bayesian model with weakly informative priors. These were fit separately to participants in SZ, BD, and HC groups (equivalent to how between-group effects were programmed in DDM's). Models were run with 1,000 warmup samples and a total of 40,000 postwarmup samples obtained over several chains. As with DDM's convergence checks were performed to ensure models had converged. There were no divergences for any model fits. Moreover, for all parameters of all SDT models fits, Rhat values were close to 1 (all were \<1.1), trace plots were well-mixed, autocorrelation was low by a lag of \~30, and bulk and tail ESS were \>10,000. Together, this suggested that chains had converged to their target distributions.

*Note: For brevity we do not include outputs of convergence checks for SDT models here.*

```{r id12333332, eval=FALSE, fig.height=1.5, fig.show="hold", fig.width=2.5, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, include=FALSE, out.width='1500px'}

sdt_stan_sample_file<-paste(sdt_path,'sdt_stan_samples.csv',sep="")
all_draws<-read.csv(sdt_stan_sample_file)
params<-colnames(all_draws)

bayesplot_theme_set(theme_default(base_size = 8, base_family = "sans"))

for (i in 1:length(params)){
  tmp_data<-all_draws[,i]
  tmp_matrix<-matrix(tmp_data,nrow=sdt_postwarmup_draws,ncol=sdt_chains,byrow=FALSE) # convert to matrix (draws*chains)
  tmp_summary<-data.frame(
    parameter=params[i],
    rhat=round(posterior::rhat(tmp_matrix),2),
    bulk_ess=round(posterior::ess_bulk(tmp_matrix),0),
    tail_ess=round(posterior::ess_tail(tmp_matrix),0))
  
  if(i==1){
    fit_summary<-tmp_summary
  }else{
    fit_summary<-rbind(fit_summary,tmp_summary)
  }
  
  # trace plot
  post_draws<-simplify2array(list(tmp_matrix))
  dimnames(post_draws)<-list(Iteration=seq(from=1,to=sdt_postwarmup_draws,by=1),
                             Chain=seq(from=1,to=sdt_chains,by=1),
                             Parameter=params[i])
  title<-paste("Stan SDT Trace Plot:\n", params[i], sep="")
  p<-mcmc_trace(post_draws,pars=params[i])+legend_none()+ggtitle(title)+ ylab("Estimate")
  print(p)
  
  # autocorrelation plots
  title<-paste("Stan SDT Autocorrelation:\n", params[i], sep="")
  p<-mcmc_acf(post_draws[,,1], lags=30)+ggtitle(title)
  print(p)

}

#get rid of chain, iteration, and draw rows
fit_summary<-fit_summary[grepl("X|chain|iteration|draw", fit_summary$parameter)==FALSE,]

# do additional convergence checks
divs
min(fit_summary$bulk_ess)
min(fit_summary$tail_ess)
min(fit_summary$rhat)
max(fit_summary$rhat)
 
```

# Interpreting Bayes Factors

For all statistical tests reported below, we calculate Bayes Factors (BF) and use those as an additional piece of information indicating the strength of an observed result. BF values index the evidence for the alternative hypothesis compared against that of the null hypothesis. BF > 1 favors the alternative hypothesis and BF < 1 favors the null hypothesis.

* For demographic group differences: alternative hypothesis = difference between groups is not zero; null hypothesis = difference between groups is zero.
* For correlations: alternative hypothesis = association is not zero; null hypothesis = association is zero.
* For regression model comparisons: alternative hypothesis = favors the full model; null hypothsis = favors the reduced model.
* For regression predictors: alternative hypothesis = value of predictor is not zero; null hypothesis = value of predictor is zero.

We then interpret each BF using the ranges below, from  Lee and Wagenmakers (2014):

```{r id99999, echo=FALSE}

bf_ranges<-data.frame(BF_Value=c(">100","30-100","10-30","3-10","1-3","1","0.33-1",
                                 "0.1-0.33","0.03-0.1","0.01-0.03","<0.01"),
                      Evidence_Favors=c("Alternative","Alternative","Alternative","Alternative","Alternative","","Null","Null","Null","Null","Null"),
                      Strength_of_Evidence=c("Extreme","Very Strong","Strong","Moderate",
                                             "Anecdotal","No Evidence","Anecdotal","Moderate",
                                             "Strong","Very Strong","Extreme"))

colnames(bf_ranges)<-c("BF Value","Evidence Favors","Strength of Evidence")

bf_ranges %>%
  kbl(caption = "Bayes Factor Interpretation Scheme from Lee and Wagenmakers (2014)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

# Sample Demographics

Tests below are Bayesian t-tests, ANOVA, and proportion analyses run using ttestBF, anovaBF, and proportion BF in the 'BayesFactor' R package (Morey, 2022). 

```{r id1433333, echo=FALSE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1500px'}

#Demographics
demo_table<-dplyr::select(all_sub_data,c('subj','group','age','F','edu','parent_edu',
                                         'White','Black.or.African.American',
                                         'Multiracial','American.Indian.or.Alaska.Native',
                                         'Asian','Hispanic',
                                         'clin_illness_duration',
                                         'Schizophrenia','Schizoaffective','Bipolar.I',
                                         'asrm','bdi',
                                         'saps_Hallucination',
                                         'saps_Delusion',
                                         'sans_MAP','sans_EXP',
                                         'meds_antipsychotic_user',
                                         'meds_antidepress_user','meds_mood_stab_user',
                                         'meds_stimulant_user','meds_hypnotic_user',
                                         'meds_anxiolytic_user','meds_anticholinergic_user',
                                         'meds_CPZeq_noHC',
                                         'BACS','MSCEIT',
                                         'rt_all','acc_all',
                                         'c_stan_all','d_stan_all',
                                         'SASSR_Social_Leisure'))

colnames(demo_table)<-c('Subj','Group','Age','Sex (% Female)','Education (Years)','Parental Education (Years)',
                        'White (%)','Black or African American (%)','Multiracial (%)',
                        'American Indian or Alaska Native (%)',
                        'Asian (%)','Hispanic (%)',
                        'Illness Duration',
                        'Schizophrenia (%)','Schizoaffective (%)','Bipolar I (%)',
                        'ASRM','BDI',
                        'SAPS-Hallucination',
                        'SAPS-Delusion',
                        'SANS-Motivation',
                        'SANS-Expressive',
                        'Antipsychotic User (%)',
                        'Antidepressant User (%)',
                        'Mood Stabilizer User (%)',
                        'Stimulant User (%)',
                        'Hypnotic User (%)',
                        'Anxiolytic User (%)',
                        'Anticholinergic User (%)',
                        'CPZeq',
                        'Cognition-General: BACS',
                        'Cognition-Social: MSCEIT',
                        'Gaze Task: RT (ms)',
                        'Gaze Task: Accuracy',
                        'Gaze Task: Criterion',
                        'Gaze Task: Discriminability',
                        'Social Functioning: SAS-SR')

demo_table_hc<-subset(demo_table,demo_table$Group=="HC")
demo_table_bd<-subset(demo_table,demo_table$Group=="BD")
demo_table_sz<-subset(demo_table,demo_table$Group=="SZ")

demo_table_hc_sd<-apply((demo_table_hc[,3:ncol(demo_table_hc)]),2,sd,na.rm=T)
demo_table_bd_sd<-apply((demo_table_bd[,3:ncol(demo_table_bd)]),2,sd,na.rm=T)
demo_table_sz_sd<-apply((demo_table_sz[,3:ncol(demo_table_sz)]),2,sd,na.rm=T)

demo_table_hc_n<-colSums(!is.na(demo_table_hc[,3:ncol(demo_table_hc)]))
demo_table_bd_n<-colSums(!is.na(demo_table_bd[,3:ncol(demo_table_bd)]))
demo_table_sz_n<-colSums(!is.na(demo_table_sz[,3:ncol(demo_table_sz)]))

demo_table_hc<-colMeans(demo_table_hc[,3:ncol(demo_table_hc)],na.rm=T)
demo_table_bd<-colMeans(demo_table_bd[,3:ncol(demo_table_bd)],na.rm=T)
demo_table_sz<-colMeans(demo_table_sz[,3:ncol(demo_table_sz)],na.rm=T)

demo_table_all<-data.frame(cbind(demo_table_hc,demo_table_hc_sd,demo_table_hc_n,
                                 demo_table_bd,demo_table_bd_sd,demo_table_bd_n,
                                 demo_table_sz,demo_table_sz_sd,demo_table_sz_n))

colnames(demo_table_all)<-c("HC (M)","HC (SD)","HC (N)",
                            "BD (M)","BD (SD)","BD (N)",
                            "SZ (M)","SZ (SD)","HC (N)")

demo_table_all<-round(demo_table_all,2)

demo_table_all[demo_table_all=="NaN"]<-"--"
demo_table_all[is.na(demo_table_all)]<-"--"

# remove SD's for % variables
rows_to_replace <- grep("%", rownames(demo_table_all))
cols_to_replace <- grep("SD", colnames(demo_table_all))
demo_table_all[rows_to_replace, cols_to_replace] <- "--"

#add in group differences (frequentist and bayes functions)
anova_summary<-function(group,dv){
  tmp_data<-data.frame(iv=as.factor(group),dv=dv)
  tmp_groups<-unique(tmp_data$iv)
  anova_result <- aov(dv ~ iv, data = tmp_data)
  summary<-summary(anova_result)

  # Extract p-value, df, and F-value
  p_value <-summary[[1]]$Pr[1]
  df <- summary[[1]]$Df[1]
  f_value <- summary[[1]]$F[1]
  sig<-ifelse(p_value<.05,'*',ifelse(p_value<.01,'**',ifelse(p_value<.001,"***","")))

  # Report in APA format
  apa_format <- paste("F(", df, ")=", format(round(f_value, 2), nsmall = 2), ", p=", round(p_value, 3),sig,sep="")
  
  if(sig==""){
    posthoc_output_all<-"--"
  }else{
    posthoc<-data.frame(TukeyHSD(anova_result)[["iv"]])
    posthoc_rows<-rownames(posthoc)
  
    for (i in 1:nrow(posthoc)){#post hoc pairwise
      tmp_compare<-posthoc_rows[i]
      grp1 <- str_extract(tmp_compare, "^[^-]*")
      grp2 <- str_extract(tmp_compare, "(?<=-)[^-]*$")
      if(posthoc$p.adj[i]>.05){
        tmp_operator<-"≈"
      }else{
        if(posthoc$diff[i]<0){
          tmp_operator<-"<"
        }else{
          tmp_operator<-">"
        }
      }
      posthoc_output<-paste(grp1,tmp_operator,grp2,sep="")
      if(i==1){
        posthoc_output_all<-posthoc_output
      }else{
        posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
      }
    }
  }
  
  anova_output<-list(apa_format,posthoc_output_all)
  return(anova_output)
}

BFanova_summary<-function(group,dv,credMass){
  
  library(BayesFactor)
  library(bayestestR)
  
  tmp_data<-data.frame(iv=group,dv=dv)
  tmp_data<-na.omit(tmp_data)
  tmp_groups<-unique(tmp_data$iv)
  
  #anova output 
  result<-anovaBF(dv~iv,data=tmp_data,progress=FALSE) #calculate anova BF
  output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
  group_bf<-round(exp(result@bayesFactor[["bf"]]),2) #have to exp transform bc bayesFactor is in log space
  
  #direction of BF evidence
  if(group_bf==1){
    group_bf_direction<-"No Evidence"
  }else if(group_bf>1){
    group_bf_direction<-"Alternative"
  }else{
    group_bf_direction<-"Null"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((group_bf >= 100)){
    group_bf_range <- "Extreme"
  }else if(group_bf >= 30 && group_bf < 100){
    group_bf_range <- "Very Strong"
  }else if(group_bf >= 10 && group_bf < 30){
    group_bf_range <- "Strong"
  }else if(group_bf >= 3 && group_bf < 10){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 1 && group_bf < 3){
    group_bf_range <- "Anecdotal"
  }else if(group_bf == 1){
    group_bf_range <- "No Evidence"
  }else if(group_bf >= 0.33 && group_bf < 1){
    group_bf_range <- "Anecdotal"
  }else if(group_bf >= 0.1 && group_bf < 0.33){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 0.03 && group_bf < 0.1){
    group_bf_range <- "Strong"
  }else if(group_bf >= 0.01 && group_bf < 0.03){
    group_bf_range <- "Very Strong"
  }else if(group_bf < 0.01){
    group_bf_range <- "Extreme"
  }else{
    group_bf_range <- ""
  }
  
  apa_format<-paste("BF=",group_bf," (",group_bf_direction,"/",group_bf_range,")",sep="")
  
  ### POST HOC TESTS
  
  # Get unique levels of the factor
  unique_levels <- unique(tmp_data$iv)
  
  # Create all unique combinations, conver to string, filter out duplicate combinations
  combinations <- t(combn(unique_levels, 2))
  combinations_strings <- apply(combinations, 1, paste, collapse = "")
  combinations_filtered <- data.frame(combo=combinations_strings[!duplicated(combinations_strings)])
  
  combinations_filtered$group1 <- substr(combinations_filtered$combo, 1, 1)
  combinations_filtered$group2 <- substr(combinations_filtered$combo, 2, 2)
  combinations_filtered<-combinations_filtered[,-1]
  
  factor_levels <- levels(tmp_data$iv) # Get levels of group factor
  
  for (j in 1:nrow(combinations_filtered)){
    
    groupidx1<-as.numeric(combinations_filtered$group1[j])
    groupidx2<-as.numeric(combinations_filtered$group2[j])
    
    group1<-factor_levels[groupidx1]
    group2<-factor_levels[groupidx2]
    
    result <- ttestBF(tmp_data$dv[tmp_data$iv == group1], tmp_data$dv[tmp_data$iv == group2])
    output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
    
    post_bf<-round(output$BF[which(output$Parameter=="Difference")],2)
    post_ci_lo<-round(output$CI_lo[which(output$Parameter=="Difference")],2)
    post_ci_hi<-round(output$CI_hi[which(output$Parameter=="Difference")],2)
    post_ci<-paste("[",post_ci_lo,", ",post_ci_hi,"]",sep="")
    post_mean<-round(output$Mean[which(output$Parameter=="Difference")],2)
    
    if((post_ci_lo>0&&post_ci_hi>0)|(post_ci_lo<0&&post_ci_hi<0)){ #if difference is credible
      if(post_mean<0){
        tmp_operator<-"<"
      }else{
        tmp_operator<-">"
      }
    }else{
      tmp_operator<-"≈"
    }
    
    posthoc_output<-paste(group1,tmp_operator,group2,sep="")
    
    if(j==1){
      posthoc_output_all<-posthoc_output
    }else{
      posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
    }
  }
  
  anova_output<-list(apa_format,posthoc_output_all)
  return(anova_output)
}
    
chisq_summary<-function(group1,group2){
  
  tmp_data <- table(group1, group2)
  chi_sq_result <- chisq.test(tmp_data)
    
  x_value <- chi_sq_result$statistic
  p_value <- chi_sq_result$p.value
  df <- chi_sq_result$parameter
  sig<-ifelse(p_value<.05,'*',ifelse(p_value<.01,'**',ifelse(p_value<.001,"***","")))
  apa_format <- paste("X^2(", df, ")=", round(x_value,2), ", p=", round(p_value,3),sig, sep = "")

  # posthoc tests
  if(sig==""){
    posthoc_output_all<-"--"
  }else{
    posthoc <- pairwise.prop.test(tmp_data,p.adjust.method="none")
    p_values <- posthoc$p.value
    col_names<-colnames(p_values)
    row_names<-rownames(p_values)
  
    for (i in 1:length(row_names)){#post hoc pairwise
     for (j in 1:length(col_names)){
       
       grp1<-row_names[i]
       grp2<-col_names[j]
       
         if(grp1!=grp2){
           if(p_values[i,j]!='NaN'|!is.na(p_values[i,j])){
             if(p_values[i,j]>.05){
              tmp_operator<-""
             }else{
              tmp_operator<-"*"
             }
             posthoc_output<-paste(grp1,"vs",grp2,tmp_operator,sep="")
             if(i==1&j==1){
               posthoc_output_all<-posthoc_output
             }else{
               posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
             }
           }
         }
       }
     } 
  }
  
  
  chisq_output<-list(apa_format,posthoc_output_all)
  return(chisq_output)
}

BFchisq_summary<-function(group1,group2,credMass){
  
  library(BayesFactor)
  library(bayestestR)
  
  tmp_data<-data.frame(group1=group1,group2=group2)
  tmp_data<-na.omit(tmp_data)
  
  tmp_data <- table(tmp_data$group1, tmp_data$group2)
  
  #calculate chi-squared(ish) BF
  result<-contingencyTableBF(tmp_data,sampleType="indepMulti",fixedMargin="rows",progress=FALSE) 
  output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
  
  group_bf<-round(exp(result@bayesFactor[["bf"]]),2) ##need to exp transform bc BF is in log space
  
  #direction of BF evidence
  if(group_bf==1){
    group_bf_direction<-"No Evidence"
  }else if(group_bf>1){
    group_bf_direction<-"Alternative"
  }else{
    group_bf_direction<-"Null"
  }
  
  # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
  if((group_bf >= 100)){
    group_bf_range <- "Extreme"
  }else if(group_bf >= 30 && group_bf < 100){
    group_bf_range <- "Very Strong"
  }else if(group_bf >= 10 && group_bf < 30){
    group_bf_range <- "Strong"
  }else if(group_bf >= 3 && group_bf < 10){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 1 && group_bf < 3){
    group_bf_range <- "Anecdotal"
  }else if(group_bf == 1){
    group_bf_range <- "No Evidence"
  }else if(group_bf >= 0.33 && group_bf < 1){
    group_bf_range <- "Anecdotal"
  }else if(group_bf >= 0.1 && group_bf < 0.33){
    group_bf_range <- "Moderate"
  }else if(group_bf >= 0.03 && group_bf < 0.1){
    group_bf_range <- "Strong"
  }else if(group_bf >= 0.01 && group_bf < 0.03){
    group_bf_range <- "Very Strong"
  }else if(group_bf < 0.01){
    group_bf_range <- "Extreme"
  }else{
    group_bf_range <- ""
  }
  
  apa_format<-paste("BF=",group_bf," (",group_bf_direction,"/",group_bf_range,")",sep="")
  
  ### POST HOC TESTS
  
  #recombine data
  tmp_data<-data.frame(group1=group1,group2=group2)
  tmp_data<-na.omit(tmp_data)
  
  # Get unique levels of the factor
  unique_levels <- unique(tmp_data$group1)
  
  # Create all unique combinations, conver to string, filter out duplicate combinations
  combinations <- t(combn(unique_levels, 2))
  combinations_strings <- apply(combinations, 1, paste, collapse = "")
  combinations_filtered <- data.frame(combo=combinations_strings[!duplicated(combinations_strings)])
  
  combinations_filtered$group1 <- substr(combinations_filtered$combo, 1, 1)
  combinations_filtered$group2 <- substr(combinations_filtered$combo, 2, 2)
  combinations_filtered<-combinations_filtered[,-1]
  
  factor_levels <- levels(tmp_data$group1) # Get levels of group factor
  
  for (j in 1:nrow(combinations_filtered)){
    
    groupidx1<-as.numeric(combinations_filtered$group1[j])
    groupidx2<-as.numeric(combinations_filtered$group2[j])
    
    group1<-factor_levels[groupidx1]
    group2<-factor_levels[groupidx2]
    
    result <- ttestBF(tmp_data$group2[tmp_data$group1 == group1], tmp_data$group2[tmp_data$group1 == group2])
    output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get summary
    
    post_bf<-round(output$BF[which(output$Parameter=="Difference")],2)
    post_ci_lo<-round(output$CI_lo[which(output$Parameter=="Difference")],2)
    post_ci_hi<-round(output$CI_hi[which(output$Parameter=="Difference")],2)
    post_ci<-paste("[",post_ci_lo,", ",post_ci_hi,"]",sep="")
    post_mean<-round(output$Mean[which(output$Parameter=="Difference")],2)
    
    if((post_ci_lo>0&&post_ci_hi>0)|(post_ci_lo<0&&post_ci_hi<0)){ #if difference is credible
      if(post_mean<0){
        tmp_operator<-"<"
      }else{
        tmp_operator<-">"
      }
    }else{
      tmp_operator<-"≈"
    }
    
    posthoc_output<-paste(group1,tmp_operator,group2,sep="")
    
    if(j==1){
      posthoc_output_all<-posthoc_output
    }else{
      posthoc_output_all<-paste(posthoc_output_all,posthoc_output,sep="; ")
    }
  }
  
  chisq_output<-list(apa_format,posthoc_output_all)
  return(chisq_output)
}
  
rows_with_percent <- grepl("%", rownames(demo_table_all))
demo_table_all$GroupDiff<-"--"
demo_table_all$PostHoc<-"--"

for (i in 1:nrow(demo_table_all)){

  tmp_var<-rownames(demo_table_all)[i]
  
  # skip variables with data only in 1 group
  if(grepl("Hallucination|Delusion|Motivation|Expressive|Schizo|Bipolar|User|Asian|Hispanic", tmp_var)==FALSE){
    grp_var<-factor(demo_table$Group,levels=c("HC","BD","SZ"))
    tmp_data<-demo_table[[tmp_var]]
    remove<-which(is.na(tmp_data))
    if(is_empty(remove)==FALSE){
      grp_var<-grp_var[-remove]
      tmp_data<-tmp_data[-remove]
    }
    
    if(rows_with_percent[i]==TRUE){#if categorical
      demo_table_all$GroupDiff[i]<-BFchisq_summary(grp_var,tmp_data,credMass=0.9)[[1]]
      demo_table_all$PostHoc[i]<-BFchisq_summary(grp_var,tmp_data,credMass=0.9)[[2]]
    }else{ #if continuous
      demo_table_all$GroupDiff[i]<-BFanova_summary(grp_var,tmp_data,credMass=0.9)[[1]]
      demo_table_all$PostHoc[i]<-BFanova_summary(grp_var,tmp_data,credMass=0.9)[[2]]

    }
  }
}

demo_table_all %>%
  kbl(caption = "Sample Characteristics",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  pack_rows("Demographic", 1, 4) %>%
  pack_rows("Race", 5, 10,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Clinical", 11, 20) %>%
  pack_rows("Diagnosis", 12, 14,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Symptoms", 15, 20,hline_before = T,italic=T,bold=F) %>%
  pack_rows("Medications", 21, 28,hline_before = T,italic=T,bold=F) %>%
  pack_rows("General/Social Cognition", 29, 30) %>%
  pack_rows("Gaze Task Performance", 31, 34) %>%
  pack_rows("Social Functioning", 35, 35) %>%
  kable_styling(full_width=T)

```

# Correlations

We ran exploratory Correlations Between DDM Parameters, traditional performance metrics (Gaze accuracy, RT, signal detection discriminability and criterion), general cognition measures (BACS), emotion-based social cognition measures (MSCEIT), SZ symptoms (SANS/SAPS), and mood symptoms (BDI/ASRM). This was done using functions from the BayesFactor R package (Morey, 2022). For each correlation, we calculate the mean and 90% HDI of the correlation coefficient, and the Bayes factor (BF). We interpret associations using the 90% HDI: intervals that do not contain zero are considered credible. For each credible association, we also consider -- as an added source of information -- the strength of the evidence using its BF. 

The sections that follow present a correlation matrix plot (to illustrate the nature of relationships between all measures); sensitivity analyses to test the robustness of observed relationships; and a series of post hoc follow-up tests, including correlations with SZ and BD, as well as associations looking at paranoia in SZ specifically.

## Correlations

Below is a full correlation matrix plot with correlations to illustrate the nature of relationships between DDM parameters (in SZ, BD, HC), traditional metrics (in SZ, BD, HC), BACS/MSCEIT (in SZ, HC), SZ symptoms (SAPS/SANS; in SZ), depressive symptoms (BDI; in SZ, BD, HC), and mania symptoms (ASRM; in BD, HC). 

In the plot below, large text displayed in cells below represent the posterior means of the correlation coefficient. This value is equivalent to the Pearson R value. The magnitude and direction of the posterior mean estimates control the color scheme of the correlation matrix (positive-going relationships are in red and negative-going are in blue). 90% HDI's that do not contain zero are considered credible (marked with a '*' below).  

```{r id173333, echo=FALSE, fig.height=10.75, fig.width=12, message=FALSE, warning=FALSE, dpi=500, out.width='1800px',cache=TRUE}

tmp_corr_data<-dplyr::select(all_sub_data,c('group',
                                            'delta_for','delta_dev',
                                            'alpha','beta',
                                            'delta_bias_for','delta_bias_dev',
                                            'acc_all','rt_all','d_stan_all','c_stan_all',
                                            'BACS','MSCEIT',
                                            'asrm','bdi',
                                            'sans_MAP','saps_Delusion','saps_Hallucination'))

colnames(tmp_corr_data)<-c('Group',
              'Drift Rate (Forward)','Drift Rate (Deviated)',
              'Threshold Separation','Start Point',
              'Drift Bias (Forward)','Drift Bias (Deviated)',
              'Accuracy','RT','SDT-Discriminability','SDT-Criterion',
              'BACS','MSCEIT',
              'ASRM Mania','BDI Depression',
              'SANS Amotivation','SAPS Delusion','SAPS Hallucination')

tmp_corr_data$Group<-as.factor(tmp_corr_data$Group)

bayes_corr_matrix<-function(data,credMass){

  library(bayestestR)
  options(scipen = 999) #deters r from using scientific notation
  
  n_vars<-ncol(data)
  vars<-colnames(data)
  
  # create empty correlation matrix to hold outputs
  corr_mat<-as.data.frame(matrix(data="",ncol=n_vars,nrow=n_vars))
  rownames(corr_mat)<-colnames(corr_mat)<-vars
  
  #create correlation matrices to hold mean rho estimates, bayes factors, rho ci's, and bf ranges
  corr_mat_credible<-corr_mat_rho<-corr_mat_bf<-corr_mat_ci<-corr_mat_bf_range<-corr_mat
  
  iteration<-0
  
  for (i in 1:n_vars){ #var1 / rows
    for (j in 1:n_vars){ #var2 / cols
      
      var1<-vars[i] #name of variable1
      var2<-vars[j] #name of variable2
      data1<-as.numeric(data[[var1]]) #get data for variable1
      data2<-as.numeric(data[[var2]]) #get data for variable2
      
      n_obs<-nrow(na.omit(cbind(data1,data2)))#number of complete observations
      
      if(var1==var2){ #if variables match, set rho (corr coefficient) to 1, otherwise run bayes correlations
        corr_mat_rho[i,j]<-1
      }else{
        
        if(n_obs>2){ #if at least 3 observations, set rho to NA and skip
          iteration<-iteration+1
          
          result <- correlationBF(data1, data2) #calculate BF
          output<-describe_posterior(result, ci=credMass, centrality = "mean",ci_method="HDI") #get output 
          rho<-round(output$Mean,2) #mean of rho (correlation coefficient estimate)
          ci_lo<-round(output$CI_low,2) #lower bound of credible interval around rho
          ci_hi<-round(output$CI_high,2) #upper bound of credible interval around rho
          bf<-output$BF #bayes factor
          bf_direction<-ifelse(bf<1,"Null","Alternative")
          
          # ranges from Lee and Wagenmakers (2014)
          if(bf >= 100){
            bf_range <- "Extreme"
          }else if(bf >= 30 && bf < 100){
            bf_range <- "Very Strong"
          }else if(bf >= 10 && bf < 30){
            bf_range <- "Strong"
          }else if(bf >= 3 && bf < 10){
            bf_range <- "Moderate"
          }else if(bf >= 1 && bf < 3){
            bf_range <- "Anecdotal"
          }else if(bf == 1){
            bf_range <- "No Evidence"
          }else if(bf >= 0.33 && bf < 1){
            bf_range <- "Anecdotal"
          }else if(bf >= 0.1 && bf < 0.33){
            bf_range <- "Moderate"
          }else if(bf >= 0.03 && bf < 0.1){
            bf_range <- "Strong"
          }else if(bf >= 0.01 && bf < 0.03){
            bf_range <- "Very Strong"
          }else if(bf < 0.01){
            bf_range <- "Extreme"
          }else{
            bf_range <- ""
          }
          
          if((ci_lo>0&&ci_hi>0)|(ci_lo<0&&ci_hi<0)){
            credible<-"*"
          }else{
            credible<-""
          }
          
          tmp_long_summary<-data.frame(Variable1=var1,
                                       Variable2=var2,
                                       Mean=rho,
                                       HDI=paste("[",ci_lo,",",ci_hi,"]",credible,sep=""),
                                       BF=round(bf,2),
                                       BF_Evidence_Strength=bf_range,
                                       BF_Evidence_Favors=bf_direction,
                                       N=n_obs)
          
          corr_mat_rho[i,j]<-rho
          corr_mat_bf[i,j]<-bf
          corr_mat_bf_range[i,j]<-bf_range
          corr_mat_ci[i,j]<-paste("[",ci_lo,",",ci_hi,"]",sep="")
          corr_mat_credible[i,j]<-credible
          
          if(iteration==1){
            all_long_summary<-tmp_long_summary
          }else{
            all_long_summary<-rbind(all_long_summary,tmp_long_summary)
          }
        }else{
          corr_mat_rho[i,j]<-NA #if fewer than 3 observations, set rho to NA
          corr_mat_credible[i,j]<-""
        }
      }
    }
  }
  
  var_list<-data.frame(Variable=rownames(corr_mat_rho))
  
  # Convert dataframe to long format for Rho values
  corr_mat_rho<-cbind(var_list,corr_mat_rho)
  df_long <- melt(corr_mat_rho,id="Variable",varnames = c("Variable","value"))
  colnames(df_long)<-c("Variable1","Variable2","Mean")
  df_long$Mean<-as.numeric(df_long$Mean)
  
  #do the same for CI's
  corr_mat_ci<-cbind(var_list,corr_mat_ci)
  df_long_ci <- melt(corr_mat_ci,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_ci)<-c("Variable1","Variable2","CI")
  
  #do the same for BF
  corr_mat_bf<-cbind(var_list,corr_mat_bf)
  df_long_bf <- melt(corr_mat_bf,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_bf)<-c("Variable1","Variable2","BF")
  
  #do the same for BF range
  corr_mat_bf_range<-cbind(var_list,corr_mat_bf_range)
  df_long_bf_range <- melt(corr_mat_bf_range,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_bf_range)<-c("Variable1","Variable2","BF_Range")
  
  #do the same for credible
  corr_mat_credible<-cbind(var_list,corr_mat_credible)
  df_long_credible <- melt(corr_mat_credible,id="Variable",varnames = c("Variable","value"))
  colnames(df_long_credible)<-c("Variable1","Variable2","Credible")
  
  # Create the plot with separate labels for Rho and corr_mat_credible
  p <- ggplot(df_long, aes(x = factor(Variable1, levels = vars), y = factor(Variable2, levels = vars))) +
    geom_tile(aes(fill = Mean), color = "black") +
    geom_text(aes(label = round(Mean, 2)), color = "black", vjust = 0.3) +  # Add Rho values
    geom_text(data = df_long_credible, aes(label = Credible), color = "black", vjust = 1.5,size=6) +  # Add credible values
    scale_fill_gradient2(low = "steelblue", high = "firebrick", midpoint = 0, na.value = "transparent", guide = "legend") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    labs(title = "Correlation Matrix",
         x = "", y = "",
         fill = "Correlation\nStrength")
  
  rownames(all_long_summary)<-NULL
  
  combined<-list(long_summary=all_long_summary,
                 corr_mat_rho=corr_mat_rho,
                 corr_mat_ci=corr_mat_ci,
                 corr_mat_bf_range=corr_mat_bf_range,
                 corr_mat_bf=corr_mat_bf,
                 corr_mat_credible=corr_mat_credible,
                 plot=p)

  return(combined)
}

# calculate bayesian corr matrix. feed in data without group column. returns the following in a list:
### long_summary (rho mean/ci, bf/bf range in long dataframe)
### corr_mat_rho (corr matrix of mean rho values - corr coefficients)
### corr_mat_ci (corr matrix of HDI credible intervals for rho)
### corr_mat_bf (corr matrix of bayes factors)
### corr_mat_bf_range (corr matrix of bayes factors ranges based on Lee and Wagenmakers 2014)
### corr_mat_credible (corr matrix of credible "*" or not "")

bf_corr_result<-bayes_corr_matrix(tmp_corr_data[,-1],credMass=0.9)

print(bf_corr_result[["plot"]])

summary<-bf_corr_result[["long_summary"]] #extract correlationBF from output

# get rid of redundant rows in output
summary <- summary[!(summary$Variable1 %in% c('BACS','MSCEIT','SANS Amotivation','SAPS Delusion','SAPS Hallucination','ASRM Mania','BDI Depression')), ]
summary <- summary[!(summary$Variable2 %in% c('Drift Rate (Forward)','Drift Rate (Deviated)','Threshold Separation','Start Point','Drift Bias (Forward)','Drift Bias (Deviated)','Accuracy','RT','SDT-Discriminability','SDT-Criterion')), ]

rownames(summary)<-NULL

summary %>%
  kbl(caption = "Correlations Between DDM Parameters, Traditional Metrics, and Clinical Metrics (Full Sample)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote(general = "* = Credible association (i.e., 90% HDI does not contain zero); Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 90% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); BF_Evidence_Favors = whether evidence favors the null hypothesis (i.e., the association between variable 1 and 2 is 0) or the alternative hypothesis (i.e., an association between variable 1 and 2 is not 0); N = Number of subjects with complete data included in correlation.")

```
 
## Sensitivity Analyses

Because some of the clinical metrics (SAPS, SANS) were zero-inflated and clinical metrics and/or performance metrics contained potentially influential observations, we ran sensitivity analyses to see if credible relationships identified above held when 1) potentially influential cases were removed, and 2) subjects with '0s' for those Hallucination/Delusion symptom dimensions were removed.

Results of sensitivity analyses show that the correlation between hallucinations and drift bias for forward heads remains credible when outliers and subjects with hallucinations scores = 0 are removed. In fact, the correlation is strengthened somewhat. This suggests the correlation is not merely the result of influential cases. However, the correlations between hallucinations and drift bias for deviated heads--as well as SDT-Criterion--were no longer credible after removing potentially influential observations. As such, we will only interpret the correlation between hallucinations and drift bias (forward). Results also show that the correlation between delusions and start point is stronger when potentially influential cases and subject with delusions = '0' are removed, suggesting the relationship is not the by-product of influential observations.

```{r id17133333, echo=FALSE, fig.height=12, fig.width=12, message=FALSE, warning=FALSE, dpi=500, out.width='1800px',cache=TRUE}

#hallucinations * drift bias forward = remove influential cases
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'Drift Bias (Forward)'>1.1),
                              c('SAPS Hallucination','Drift Bias (Forward)')]
analysis<-data.frame(Sensitivity_Analysis="Halluc*Drift Bias (Forward) - Influential Cases Removed")
output1<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output1<-cbind(analysis,output1)

#hallucinations * drift bias deviated = remove influential cases
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'Drift Bias (Deviated)' < -3),
                              c('SAPS Hallucination','Drift Bias (Deviated)')]
analysis<-data.frame(Sensitivity_Analysis="Halluc*Drift Bias (Deviated) - Influential Cases Removed")
output2<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output2<-cbind(analysis,output2)

#hallucinations * sdt-criterion = remove influential cases
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'SDT-Criterion' > 1),
                              c('SAPS Hallucination','SDT-Criterion')]
analysis<-data.frame(Sensitivity_Analysis="Halluc*SDT-Criterion - Influential Cases Removed")
output6<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output6<-cbind(analysis,output6)

#hallucinations * drift bias forward = remove zero inflation
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'SAPS Halluc'==0),
                              c('SAPS Hallucination','Drift Bias (Forward)')]
analysis<-data.frame(Sensitivity_Analysis="Halluc*Drift Bias (Forward) - SAPS Halluc=0 Cases Removed")
output3<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output3<-cbind(analysis,output3)

#delusions * start point = remove influential cases
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'SAPS Delusion'>26),
                              c('SAPS Delusion','Start Point')]
analysis<-data.frame(Sensitivity_Analysis="Delusion*Start Point - Influential Cases Removed")
output4<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output4<-cbind(analysis,output4)

#delusions * start point = remove zero inflation
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'SAPS Delusion'==0),
                              c('SAPS Delusion','Start Point')]
analysis<-data.frame(Sensitivity_Analysis="Delusion*Start Point - SAPS Delusion=0 Cases Removed")
output5<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output5<-cbind(analysis,output5)

#SANS Amotiv * drift bias forward = remove influential cases
tmp_corr_data1<-tmp_corr_data[-which(tmp_corr_data$'Drift Bias (Forward)'>1.1),
                              c('SANS Amotivation','Drift Bias (Forward)')]
analysis<-data.frame(Sensitivity_Analysis="Amotiv*Drift Bias (Forward) - Influential Cases Removed")
output7<-bayes_corr_matrix(tmp_corr_data1,credMass=0.9)[["long_summary"]] #extract correlationBF 
output7<-cbind(analysis,output7)

output<-rbind(output1,output2,output6,output3,output4,output5,output7) #combine outputs
output<-output[!duplicated(output$Sensitivity_Analysis),] #remove duplicate rows
rownames(output)<-NULL

output %>%
  kbl(caption = "Correlations Between DDM Parameters, Traditional Metrics, and Clinical Metrics After Removing Potentially Influential and Zero-Inflated Cases",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote(general = "* = Credible association (i.e., 90% HDI does not contain zero); Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 90% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); N = Number of subjects with complete data included in correlation.")

```
 
## Post-Hoc Correlations

Following the primary correlations above, we performed a series of post-hoc correlation analyses.

### Within SZ Only

We repeated the same key correlations of interest (from above) within the SZ group only. We exclude SAPS hallucinations and delusions below because the correlations above were already within SZ only.
Credible associations between drift rates and BACS and drift rate and SDT-discriminability remain credible in SZ. The same is true for the association between drift rate for forward heads and MSCEIT. Several other credible associations also emerge but they are supported by only anecdotal evidence and we chose not to interpret those.

```{r id19199, echo=FALSE, fig.height=12, fig.width=12, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1800px'}

tmp_corr_data<-dplyr::select(all_sub_data,c('group',
                                            'delta_for','delta_dev',
                                            'alpha','beta',
                                            'delta_bias_for','delta_bias_dev',
                                            'acc_all','rt_all','d_stan_all','c_stan_all',
                                            'BACS','MSCEIT',
                                            'asrm','bdi',))

colnames(tmp_corr_data)<-c('Group',
              'Drift Rate (Forward)','Drift Rate (Deviated)',
              'Threshold Separation','Start Point',
              'Drift Bias (Forward)','Drift Bias (Deviated)',
              'Accuracy','RT','SDT-Discriminability','SDT-Criterion',
              'BACS','MSCEIT',
              'ASRM Mania','BDI Depression')

tmp_corr_data$Group<-as.factor(tmp_corr_data$Group)
tmp_corr_data<-subset(tmp_corr_data,tmp_corr_data$Group=="SZ")

bf_corr_result<-bayes_corr_matrix(tmp_corr_data[,-1],credMass=0.9)

summary<-bf_corr_result[["long_summary"]] #extract correlationBF from output

# get rid of redundant rows in output
summary <- summary[!(summary$Variable1 %in% c('BACS','MSCEIT','ASRM Mania','BDI Depression')), ]
summary <- summary[!(summary$Variable2 %in% c('Drift Rate (Forward)','Drift Rate (Deviated)','Threshold Separation','Start Point','Drift Bias (Forward)','Drift Bias (Deviated)','Accuracy','RT','SDT-Discriminability','SDT-Criterion')), ]

rownames(summary)<-NULL

summary %>%
  kbl(caption = "Correlations (Within SZ Only) Between DDM Parameters, Traditional Metrics, Clinical Metrics",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote(general = "Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 90% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); N = Number of subjects with complete data included in correlation.")

```

### Within BD Only

We repeated the same key correlations of interest (from above) within the BD group only. We only include ASRM and BDI because the other measures were not collected in BD. Results showed that neither ASRM or BDI showed credible associations with DDM parameters or traditional metrics in the BD sample.

```{r id191992, echo=FALSE, fig.height=12, fig.width=12, message=FALSE, warning=FALSE, cache=TRUE, dpi=500, out.width='1800px'}

tmp_corr_data<-dplyr::select(all_sub_data,c('group',
                                            'delta_for','delta_dev',
                                            'alpha','beta',
                                            'delta_bias_for','delta_bias_dev',
                                            'acc_all','rt_all','d_stan_all','c_stan_all',
                                            'asrm','bdi',))

colnames(tmp_corr_data)<-c('Group',
              'Drift Rate (Forward)','Drift Rate (Deviated)',
              'Threshold Separation','Start Point',
              'Drift Bias (Forward)','Drift Bias (Deviated)',
              'Accuracy','RT','SDT-Discriminability','SDT-Criterion',
              'ASRM Mania','BDI Depression')

tmp_corr_data$Group<-as.factor(tmp_corr_data$Group)
tmp_corr_data<-subset(tmp_corr_data,tmp_corr_data$Group=="BD")

bf_corr_result<-bayes_corr_matrix(tmp_corr_data[,-1],credMass=0.9)

summary<-bf_corr_result[["long_summary"]] #extract correlationBF from output

# get rid of redundant rows in output
summary <- summary[!(summary$Variable1 %in% c('ASRM Mania','BDI Depression')), ]
summary <- summary[!(summary$Variable2 %in% c('Drift Rate (Forward)','Drift Rate (Deviated)','Threshold Separation','Start Point','Drift Bias (Forward)','Drift Bias (Deviated)','Accuracy','RT','SDT-Discriminability','SDT-Criterion')), ]

rownames(summary)<-NULL

summary %>%
  kbl(caption = "Correlations (Within BD Only) Between DDM Parameters, Traditional Metrics, Clinical Metrics",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote(general = "Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 90% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); N = Number of subjects with complete data included in correlation.")

```
 
### Start Point and Paranoia

Given the relationship we observed between start point and SAPS Delusions, we ran a post hoc follow-up test to examine whether this relationship was evident within paranoia symptoms specifically. To achieve this, we calculated a paranoia factor using results of a factor analysis on SAPS items from Peralta (1999). That study identified that two items from the SAPS -- delusions of persecution and delusions of reference -- loaded onto a 'paranoia' factor. In our data, we generated a paranoia factor by scaling participants' scores on these two items by the factor loadings of Peralta (1999) and summing them. Then we ran a correlation (using a Bayesian approach) on this paranoia factor and the DDM start point parameter.

Results revealed a credible positive correlation, such that SZ patients with more paranoia also had higher start points (i.e., greater initial self-referential biases) on the gaze task.

```{r id1711, echo=FALSE, fig.height=12, fig.width=12, message=FALSE, warning=FALSE, dpi=500, out.width='1800px',cache=TRUE}

tmp_corr_data2<-dplyr::select(all_sub_data,c('group','beta','saps_PersecDelusion','saps_RefDelusion'))
colnames(tmp_corr_data2)<-c('Group','Start Point','SAPS Persecutory (D8)','SAPS Reference (D14)')

tmp_corr_data2$Group<-as.factor(tmp_corr_data2$Group)

#scale and sum the D8 and D14 items using factor weights obtained from Peralta & Cuesta 1999
## their paranoia factor consisted of items D8 (persecution) and D14 (delusions of reference) with factor weights of .79 and .74 respectively.
## Peralta, V., & Cuesta, M. J. (1999). Dimensional structure of psychotic symptoms: an item-level analysis of SAPS and SANS symptoms in psychotic disorders. Schizophrenia research, 38(1), 13-26.

tmp_corr_data2$'SAPS Paranoia Factor'<-tmp_corr_data2$`SAPS Persecutory (D8)`*0.79+tmp_corr_data2$`SAPS Reference (D14)`*0.74

p<-bayes_corr_matrix(tmp_corr_data2[,-1],credMass=0.9)[['long_summary']]

#get rid of comparisons we aren't interested in
p<-p[which(p$Variable1=='Start Point'&p$Variable2=='SAPS Paranoia Factor'),]
rownames(p)<-NULL

p %>%
  kbl(caption = "Correlation Between Start Point and SAPS Paranoia Factor",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote(general = "Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 90% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); N = Number of subjects with complete data included in correlation.")

```
 
### Antipsychotics and Performance

To assess whether antipsychotic doses (CPZeq) were related to measures that tap processing speed, we ran correlations between CPZeq and measures that were sensitive to processing speed. This was done exclusively within participants in the SZ and BD groups taking antipsychotic medicaitons. This included BACS, DDM parameters, and traditional performance metrics on the gaze task. MSCEIT was not included as it does not impose a time limit on participants. 

Results did not show credible associations between CPZ dose and any of the DDM parameters, BACS scores, or other gaze task performance metrics. This suggested that observed results were not merely the result of the influence of antipsychotic dosing.

```{r id193333, echo=FALSE, fig.height=9, fig.width=10, message=FALSE, warning=FALSE, dpi=500,cache=TRUE, out.width='1600px'}

#check correlations with CPZ
tmp_corr_data<-dplyr::select(all_sub_data,c('group','meds_antipsychotic_user',
                                            'BACS','meds_CPZeq_noHC',
                                            'alpha','beta',
                                            'delta_for','delta_dev',
                                            'delta_bias_for','delta_bias_dev',
                                            'acc_all','rt_all','c_stan_all','d_stan_all'))

colnames(tmp_corr_data)<-c('Group', 'Antipsychotic_User','BACS','CPZeq','Threshold_Separation','Start_Point','Drift_Rate_Forward','Drift_Rate_Deviated','Drift_Bias_Forward','Drift_Bias_Deviated','Accuracy','RT','Criterion','Discriminability')

tmp_corr_data$Group<-as.factor(tmp_corr_data$Group)
tmp_corr_data1<-filter(tmp_corr_data, Antipsychotic_User == "1")

p<-bayes_corr_matrix(tmp_corr_data[,-c(1,2)],credMass=0.9)
p<-p[['long_summary']]

#get rid of comparisons we aren't interested in
p<-p[which(p$Variable1=='CPZeq'),]
rownames(p)<-NULL

p %>%
  kbl(caption = "Correlations Between Antipsychotic Dose and Performance",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) %>%
  footnote(general = "Mean = mean posterior estimate of correlation coefficient (equivalent to Pearson R); HDI = 90% HDI of the correlation coefficient; BF = Bayes Factor; BF_Evidence_Strength =  Bayes Factor interpretation scheme based on Lee and Wagenmakers (2014); N = Number of subjects with complete data included in correlation.")

```
 
# Regressions

We ran separate hierarchical linear regressions to assess whether any of the DDM parameters and/or traditional gaze task performance metrics (accuracy, RT, SDT Criterion, SDT discriminability) could predict social functioning (Social Adjustment Scale-SR, Social/Leisure Sub scale) across SZ and HC, above and beyond diagnosis and common measures of general (BACS) and emotion-based social cognition (MSCEIT). We assessed  these predictors on whether:
1) They were credible predictors of social functioning. Credible predictors were those in which the 90% HDI of the predictor coefficient did not contain zero.
2) They improved the out-of-sample predictive accuracy. This was done using LOO model comparisons.

These regression models were run in Stan via brms (Burkner, 2017) using standardized predictors and weakly informative priors (i.e., Normal(0,1) for each predictor). Models were sampled using 1000 warmup samples and 4000 postwarmup draws for each of 4 chains, resulting in 16000 total post warmup samples. The same procedure for assessing convergence (described above) indicated that parameters of all models had converged to their target distributions. 

```{r id21333, echo=FALSE, fig.height=9,cache=TRUE, fig.show="hold", fig.width=12, message=FALSE, warning=FALSE, dpi=500, out.width='1800px'}

#############################################################################
# Prep Data
#############################################################################

#dummy code group variables
tmp_all_sub_data<-all_sub_data
tmp<-data.frame(dummy.code(tmp_all_sub_data$group))
colnames(tmp)<-c('dummy_bd','dummy_hc','dummy_sz')
tmp_all_sub_data<-cbind(tmp_all_sub_data,tmp)

tmp_corr_data<-dplyr::select(tmp_all_sub_data,c('group','dummy_sz',
                                            'SASSR_Social_Leisure','BACS','MSCEIT',
                                            'alpha','beta',
                                            'delta_for','delta_dev',
                                            'delta_bias_for','delta_bias_dev',
                                            'acc_all','rt_all','c_stan_all','d_stan_all','meds_CPZeq_wHC'))

colnames(tmp_corr_data)<-c('group','SZ_Dummy',
                                            'SASSR_Social','BACS','MSCEIT',
                                            'Threshold_Separation','Start_Point',
                                            'Drift_Rate_Forward','Drift_Rate_Deviated',
                                            'Drift_Bias_Forward','Drift_Bias_Deviated',
                                            'Accuracy','RT','Criterion','Discriminability','CPZeq')

tmp_corr_data<-subset(tmp_corr_data,tmp_corr_data$group != "BD")
tmp_corr_data<-tmp_corr_data[-which(is.na(tmp_corr_data$SASSR_Social)),-1]

#calculates zscores over all columns 
calculate_zscore <- function(data) {
  
  # Loop through each column in the data frame
  for (col in names(data)) {
    # Calculate mean and standard deviation of the column
    col_mean <- mean(data[[col]])
    col_sd <- sd(data[[col]])
    
    # Calculate z-score for each value in the column
    z_scores <- (data[[col]] - col_mean) / col_sd
    
    # Replace the column with z-scores
    data[[col]] <- z_scores
  }
  return(data)
} 

# runs regression models in brms and does model comparisons
brms_glm_compare<-function(data,test_models,outcome,outfile){
  
  library(brms)
  library(loo)
  library(stats)
  
  fits<-list()
  
  if(file.exists(outfile)){
    fits<-readRDS(outfile)
  }else{
    for(i in 1:length(test_models)){
      
      formula<-paste(outcome, paste(test_models[[i]],collapse=" + "),sep=" ~ ")
      brms_formula<-bf(formula)
      predictors<-test_models[[i]] #get b predictors for this model (not including intercept and sigma)
      
      #function dynamically sets priors on centered predictors (input to function) for brms models
      #it also sets priors on intercept and sigma terms
      dynamic_prior<-function(predictors){ 
        #set prior for intercept and sigma
        ## note: intercept = mean centered b_Intercept = original scale. set prior on Intercept
        prior_combine<-brms::prior_string(paste0("normal(0, 1)"), class = "Intercept")+
          brms::prior_string(paste0("student_t(3, 0, 2.5)"), class = "sigma") #default prior in brms
        
        #set priors on b coefs
        for (k in 1:length(predictors)){
          tmp_prior<-brms::prior_string(paste0("normal(0, 1)"), class = "b",coef=predictors[k])
          prior_combine<-prior_combine+tmp_prior
        }
        return(prior_combine)
      }
      
      fit <- brm(
        formula=brms_formula,
        data = clean_data,
        sample_prior = TRUE,
        family = gaussian(),  
        save_pars = save_pars(all = TRUE),
        chains = 4, cores = 4,seed=42,iter = 5000, warmup = 1000, thin = 1,
        prior = dynamic_prior(predictors),
        control = list(adapt_delta = 0.95))
      
      fits[[i]] <- add_criterion(fit, "loo")
    }
    saveRDS(fits, outfile)
  }
  
  for (i in 1:length(test_models)){

    #get posterior samples
    samples <- as.data.frame(fits[[i]]) 
    
    #extract prior samples
    prior_samples<-samples[,grep("prior", names(samples), ignore.case = TRUE)]
    
    # remove extra columns (interpret b_Intercept, not Intercept,priors)
    samples <- samples[, !(names(samples) %in% c("Intercept","sigma", "prior", "lp__"))]
    samples <- samples[,-grep("prior", names(samples), ignore.case = TRUE)]
    n_pars<-ncol(samples) #define number of parameters in current model
    
    tmp_loo<-data.frame(loo(fits[[i]])[['estimates']])
    elpd_loo<-round(tmp_loo$Estimate[1],2)
    elpd_loo_se<-round(tmp_loo$SE[1],2)
    
    if(i==1){
      delta_elpd_se<-"--"
      delta_elpd<-"--"
      model_bf<-"--"
      model_bf_direction<-"--"
      model_bf_range<-"--"
    }else{
      tmp_loo_compare<-data.frame(loo_compare(fits[[i]],fits[[1]]))#compare current model to model1 
      tmp_bf_compare<-capture.output(brms::bayes_factor(fits[[i]],fits[[1]],log=FALSE)) #model-level bayes factor
      tmp_bf_compare<-tmp_bf_compare[length(tmp_bf_compare)]
      model_bf <- round(as.numeric(sub(".+\\s(\\d+\\.?\\d*)$", "\\1", tmp_bf_compare)),2) #bf of current model over model1
      
      #direction of BF evidence
      if(model_bf==1){
        model_bf_direction<-"No Evidence"
      }else if(model_bf>1){
        model_bf_direction<-"Alternative"
      }else{
        model_bf_direction<-"Null"
      }
      
      # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
      if((model_bf >= 100)){
        model_bf_range <- "Extreme"
      }else if(model_bf >= 30 && model_bf < 100){
        model_bf_range <- "Very Strong"
      }else if(model_bf >= 10 && model_bf < 30){
        model_bf_range <- "Strong"
      }else if(model_bf >= 3 && model_bf < 10){
        model_bf_range <- "Moderate"
      }else if(model_bf >= 1 && model_bf < 3){
        model_bf_range <- "Anecdotal"
      }else if(model_bf == 1){
        model_bf_range <- "No Evidence"
      }else if(model_bf >= 0.33 && model_bf < 1){
        model_bf_range <- "Anecdotal"
      }else if(model_bf >= 0.1 && model_bf < 0.33){
        model_bf_range <- "Moderate"
      }else if(model_bf >= 0.03 && model_bf < 0.1){
        model_bf_range <- "Strong"
      }else if(model_bf >= 0.01 && model_bf < 0.03){
        model_bf_range <- "Very Strong"
      }else if(model_bf < 0.01){
        model_bf_range <- "Extreme"
      }else{
        model_bf_range <- ""
      }
      
      #if current model = better fit than model 1, then it will appear in row1.
      ## then we need to get the elpd diff from row 2 and flip it.
      if(rownames(tmp_loo_compare)[1]=="fits[[i]]"){ 
        
        # get difference from row2 and flip it
        delta_elpd<-round(tmp_loo_compare$elpd_diff[2],2)*-1
        delta_elpd_se<-round(tmp_loo_compare$se_diff[2],2)
        
      }else{  #if current model = worse fit than model 1, then it will appear in row2
        delta_elpd<-round(tmp_loo_compare$elpd_diff[2],2)
        delta_elpd_se<-round(tmp_loo_compare$se_diff[2],2)
      }
    }
    
    #generate model summary
    tmp_mod_summary<-data.frame(Model=i,
                                Reference_Model=1,
                                elpd_loo=elpd_loo,
                                elpd_loo_se=elpd_loo_se,
                                delta_elpd=delta_elpd,
                                delta_elpd_se=delta_elpd_se,
                                model_bf=model_bf,
                                model_bf_direction_strength=paste(model_bf_direction,
                                                                  "/",model_bf_range,sep=""),
                                Predictors="", #predictor name
                                pred_bf="",#predictor bf
                                pred_bf_direction_strength="", #predictor bf strength/direction
                                Mean_and_HDI="") #mean and 90% HDI of posterior
    
    for (j in 1:n_pars){
      tmp_par<-colnames(samples)[j]
      tmp_par <- sub("^b_", "", tmp_par)

      post_par<-paste("b_",tmp_par,sep="") #par name formatted as posterior pars
      
      if(post_par=="b_Intercept"){
        prior_par<-paste("prior_",tmp_par,sep="") #par name formatted as prior pars
      }else{
        prior_par<-paste("prior_b_",tmp_par,sep="") #par name formatted as prior pars
      }

      # CALCULATE BAYES FACTORS FOR PREDICTORS W/ SAVAGE DICKEY DENSITY RATIO (WAGENMAKERS 2010)
      ## 3 methods we could use here: 1) brms "hypothesis" function
      ## 2) kernel density estimation in 'stats' package, or 3) logspline
      ## kernel density estimation (as in Wagenmakers 2010). 
      ## code for all 3 is provided below, but we opted for option #3.
      
      ## for function, enter posterior samples, prior samples, and estimation method (we use logspline)
      
      ## we calculate the ratio to be consistent with the rest of the paper,
      ## such that values >1 favor the alternative and values <1 favor the null
      savage_dickey_ratio<-function(posterior,prior,method){ 
        
        # ### 1) brms 'hypothesis' function (similar result to kernel density est. in 'stats')
        # if(method=="brms_hypothesis"){
        #   library(brms)
        #   h <- hypothesis(fits[[i]], "SZ_Dummy = 0") #no effect of given parameter
        #   bf<-h[["hypothesis"]][["Evid.Ratio"]]
        #   bf<-1/bf
        #   return(bf)
        # }

        # ### 2) kernel density estimation in stats package 
        # if(method=="kernel_density"){
        #   library(stats)
        #   density_estimate<-density(samples$b_SZ_Dummy)
        #   posterior_density <- density_estimate[["y"]][which.min(abs(density_estimate[["x"]] - 0))]
        #   density_estimate<-density(prior_samples$prior_b_SZ_Dummy)
        #   prior_density <- density_estimate[["y"]][which.min(abs(density_estimate[["x"]] - 0))]
        #   bf<-prior_density/posterior_density
        # }
        
        ### 3) logspline kernel density estimation (as in Wagenmakers 2010)
        if(method=="logspline"){
          library(logspline)
          
          log_spline_density<-invisible(logspline(posterior,silent=TRUE)) #density estimate for posterior
          posterior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
          
          log_spline_density<-invisible(logspline(prior,silent=TRUE)) #density estimate for prior
          prior_density<-dlogspline(0, log_spline_density, log = F)  #density @ 0
          
          # a group effect of 0 is ~11 times LESS likely than it was before seeing the data (i.e., 1/.09)
          bf<-round(prior_density/posterior_density,2)
        }
        
        #direction of BF evidence
        if(bf==1){
          bf_direction<-"No Evidence"
        }else if(bf>1){
          bf_direction<-"Alternative"
        }else{
          bf_direction<-"Null"
        }
        
        # strength of bayes factor evidence. ranges from Lee and Wagenmakers (2014)
        if((bf >= 100)){
          bf_range <- "Extreme"
        }else if(bf >= 30 && bf < 100){
          bf_range <- "Very Strong"
        }else if(bf >= 10 && bf < 30){
          bf_range <- "Strong"
        }else if(bf >= 3 && bf < 10){
          bf_range <- "Moderate"
        }else if(bf >= 1 && bf < 3){
          bf_range <- "Anecdotal"
        }else if(bf == 1){
          bf_range <- "No Evidence"
        }else if(bf >= 0.33 && bf < 1){
          bf_range <- "Anecdotal"
        }else if(bf >= 0.1 && bf < 0.33){
          bf_range <- "Moderate"
        }else if(bf >= 0.03 && bf < 0.1){
          bf_range <- "Strong"
        }else if(bf >= 0.01 && bf < 0.03){
          bf_range <- "Very Strong"
        }else if(bf < 0.01){
          bf_range <- "Extreme"
        }else{
          bf_range <- ""
          
        }
        combine<-list(bf=bf,
                      bf_range=bf_range,
                      bf_direction=bf_direction)
        
          return(combine)
      }
      
      # reflects how likely are data under alternative hypothesis (vs null hypothesis that effect = 0)
      ## values >1 favor the alternative; values < 1 favor the null
      pred_bf<-savage_dickey_ratio(samples[[post_par]],prior_samples[[prior_par]],"logspline")
      
      #calculate mean and HDI of given set of samples, for a given credible mass
      mean_and_hdi<-function(samples, credMass){
        library(bayestestR)
        lower<-bayestestR::hdi(samples,ci=credMass)[[2]]
        upper<-bayestestR::hdi(samples,ci=credMass)[[3]]
        mean<-mean(samples)
        
        if(lower>0&&upper>0|lower<0&&upper<0){
          credible<-"*"
        }else{
          credible<-""
        }
        
        summary<-paste(round(mean,2)," [",round(lower,2),", ",round(upper,2),"]",credible,sep="")
        return(summary)
      } 

      #generate predictor summary
      tmp_pred_summary<-data.frame(Model="",
                                   Reference_Model="",
                                   elpd_loo="",
                                   elpd_loo_se="",
                                   delta_elpd="",
                                   delta_elpd_se="",
                                   model_bf="",
                                   model_bf_direction_strength="",
                                   Predictors=tmp_par, #predictor name
                                   pred_bf=pred_bf[["bf"]],#predictor bf
                                   pred_bf_direction_strength=paste(pred_bf[["bf_direction"]],
                                                                    "/",pred_bf[["bf_range"]],sep=""), #predictor bf strength/direction
                                   Mean_and_HDI=mean_and_hdi(samples=samples[[post_par]],credMass = 0.9)) #mean and 90% HDI of posterior
      
      if(j==1){ #append to predictor summary dataframe
        running_summary<-tmp_pred_summary
      }else{
        running_summary<-rbind(running_summary,tmp_pred_summary)
      }
    }
    
    #combine model summary and predictor summary
    combined_summary<-rbind(tmp_mod_summary,running_summary)
    
    if(i==1){
      all_summary<-combined_summary
    }else{
      all_summary<-rbind(all_summary,combined_summary)
    }
  }
  
  colnames(all_summary)<-c("Model","Ref. Model","LOO ELPD","LOO ELPD SE",
                        paste("\u0394","ELPD",sep=""),paste("\u0394","ELPD SE",sep=""),
                        "Model BF","Model BF Direction/ Strength","Predictor",
                        "Pred BF","Pred BF Strength/ Direction",
                        "Pred Mean [90% HDI]")
  return(all_summary)
}

```

## DDM Parameters Predicting Social Functioning

Start point predicts social functioning above and beyond diagnosis, general cognition, and social cognition. The model including start point showed increases in predictive accuracy relative to the null model, but the strength of evidence for both the full model (relative to the null) and strength of evidence for the start point predictor are both only anecdotal.

```{r id223333, echo=FALSE,cache=TRUE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, out.width='2500px'}

tmp_corr_data1<-na.omit(tmp_corr_data) #keep complete cases only

# standardizing predictors to make it easier to set priors on predictors
clean_data<-calculate_zscore(tmp_corr_data1)

#define models to test (list predictors in order they should be entered)
test_models<-list(c("SZ_Dummy","BACS", "MSCEIT"),
           c("SZ_Dummy","BACS","MSCEIT","Drift_Rate_Forward"),
           c("SZ_Dummy","BACS","MSCEIT","Drift_Rate_Deviated"),
           c("SZ_Dummy","BACS","MSCEIT","Drift_Bias_Forward"),
           c("SZ_Dummy","BACS","MSCEIT","Drift_Bias_Deviated"),
           c("SZ_Dummy","BACS","MSCEIT","Start_Point"),
           c("SZ_Dummy","BACS","MSCEIT","Threshold_Separation"))

outcome<-"SASSR_Social" #outcome variable
outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_1.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Functioning from DDM Parameters",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T) 

```

## Sensitivity Analysis (Control for CPZeq)

Results are the same when we control for antipsychotic dose.

```{r id233333, echo=FALSE,cache=TRUE, fig.height=5, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, out.width='2500px'}

#define models to test (list predictors in order they should be entered)
test_models<-list(c("SZ_Dummy","BACS","MSCEIT","CPZeq"),
           c("SZ_Dummy","BACS","MSCEIT","CPZeq","Drift_Rate_Forward"),
           c("SZ_Dummy","BACS","MSCEIT","CPZeq","Drift_Rate_Deviated"),
           c("SZ_Dummy","BACS","MSCEIT","CPZeq","Drift_Bias_Forward"),
           c("SZ_Dummy","BACS","MSCEIT","CPZeq","Drift_Bias_Deviated"),
           c("SZ_Dummy","BACS","MSCEIT","CPZeq","Start_Point"),
           c("SZ_Dummy","BACS","MSCEIT","CPZeq","Threshold_Separation"))

outcome<-"SASSR_Social" #outcome variable

outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_2.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression (Sensitivity Analysis): Predicting Social Functioning from DDM Parameters after Controlling for Antipsychotic Dose",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```
 
## Post-Hoc Analyses

Results look similar in SZ. The direction of the relationship between the start point predictor and outcome is similar, but no longer credible. This isn't surprising given the loss of small sample size from doing this within groups. 

### DDM Parameters as Predictors (SZ Only)

```{r id2523333, echo=FALSE, fig.height=5,cache=TRUE, fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, out.width='2500px'}

tmp_corr_data1<-na.omit(tmp_corr_data)
tmp_corr_data1<-subset(tmp_corr_data1,tmp_corr_data1$SZ_Dummy == 1)

# standardizing predictors to make it easier to set priors on predictors
clean_data<-calculate_zscore(tmp_corr_data1)

#define models to test (list predictors in order they should be entered)
test_models<-list(c("BACS","MSCEIT"),
           c("BACS","MSCEIT", "Drift_Rate_Forward"),
           c("BACS","MSCEIT","Drift_Rate_Deviated"),
           c("BACS","MSCEIT","Drift_Bias_Forward"),
           c("BACS","MSCEIT","Drift_Bias_Deviated"),
           c("BACS","MSCEIT","Start_Point"),
           c("BACS","MSCEIT","Threshold_Separation"))

outcome<-"SASSR_Social" #outcome variable
outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_3.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Functioning from DDM Parameters (Within SZ Only)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

### DDM Parameters as Predictors (HC Only)

```{r id262333, echo=FALSE, fig.height=5, fig.show="hold",cache=TRUE, fig.width=5, message=FALSE, warning=FALSE, out.width='2500px'}

tmp_corr_data1<-na.omit(tmp_corr_data)
tmp_corr_data1<-subset(tmp_corr_data1,tmp_corr_data1$SZ_Dummy == 0)

# standardizing predictors to make it easier to set priors on predictors
clean_data<-calculate_zscore(tmp_corr_data1)

#define models to test (list predictors in order they should be entered)
test_models<-list(c("BACS","MSCEIT"),
           c("BACS","MSCEIT", "Drift_Rate_Forward"),
           c("BACS","MSCEIT","Drift_Rate_Deviated"),
           c("BACS","MSCEIT","Drift_Bias_Forward"),
           c("BACS","MSCEIT","Drift_Bias_Deviated"),
           c("BACS","MSCEIT","Start_Point"),
           c("BACS","MSCEIT","Threshold_Separation"))

outcome<-"SASSR_Social" #outcome variable
outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_4.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Functioning from DDM Parameters (Within HC Only)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

### Traditional Metrics as Predictors (Full Sample)

None of the traditional metrics can predict social functioning above and beyond diagnosis, BACS, and MSCEIT.

```{r id22133, echo=FALSE, fig.height=5,cache=TRUE,fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, out.width='2500px'}

tmp_corr_data1<-na.omit(tmp_corr_data)

# standardizing predictors to make it easier to set priors on predictors
clean_data<-calculate_zscore(tmp_corr_data1)

#define models to test (list predictors in order they should be entered)
test_models<-list(c("SZ_Dummy","BACS", "MSCEIT"),
           c("SZ_Dummy","BACS","MSCEIT","Accuracy"),
           c("SZ_Dummy","BACS","MSCEIT","RT"),
           c("SZ_Dummy","BACS","MSCEIT","Criterion"),
           c("SZ_Dummy","BACS","MSCEIT","Discriminability"))

outcome<-"SASSR_Social" #outcome variable
outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_5.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Functioning from Traditional Metrics (Full Sample)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

### Traditional Metrics as Predictors (SZ Only)

```{r id2211, echo=FALSE, fig.height=5,cache=TRUE,fig.show="hold", fig.width=5, message=FALSE, warning=FALSE, out.width='2500px'}

tmp_corr_data1<-na.omit(tmp_corr_data)
tmp_corr_data1<-subset(tmp_corr_data1,tmp_corr_data1$SZ_Dummy == "1")

# standardizing predictors to make it easier to set priors on predictors
clean_data<-calculate_zscore(tmp_corr_data1)

#define models to test (list predictors in order they should be entered)
test_models<-list(c("BACS","MSCEIT"),
           c("BACS","MSCEIT", "Accuracy"),
           c("BACS","MSCEIT","RT"),
           c("BACS","MSCEIT","Criterion"),
           c("BACS","MSCEIT","Discriminability"))

outcome<-"SASSR_Social" #outcome variable
outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_6.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Functioning from Traditional Metrics (Within SZ Only)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

### Traditional Metrics as Predictors (HC Only)

```{r id2212, echo=FALSE, fig.height=5, fig.show="hold",cache=TRUE, fig.width=5, message=FALSE, out.width='2500px',warning=FALSE}

tmp_corr_data1<-na.omit(tmp_corr_data)
tmp_corr_data1<-subset(tmp_corr_data1,tmp_corr_data1$SZ_Dummy == "0")

# standardizing predictors to make it easier to set priors on predictors
clean_data<-calculate_zscore(tmp_corr_data1)

#define models to test (list predictors in order they should be entered)
test_models<-list(c("BACS","MSCEIT"),
           c("BACS","MSCEIT", "Accuracy"),
           c("BACS","MSCEIT","RT"),
           c("BACS","MSCEIT","Criterion"),
           c("BACS","MSCEIT","Discriminability"))

outcome<-"SASSR_Social" #outcome variable
outfile<-"/Users/carlylasagna/Dropbox (University of Michigan)/ddm_gaze_bdeeg/beh_manuscript/analysis/published version/schizbull_revision1/saved_brms_models_7.rds"

result_glm_ddm<-brms_glm_compare(data=clean_data,
                         test_models = test_models,
                         outcome=outcome,
                         outfile=outfile)

result_glm_ddm %>%
  kbl(caption = "Bayesian Regression: Predicting Social Functioning from Traditional Metrics (Within HC Only)",valign = "t") %>%
  kable_classic(full_width=F, html_font = "Cambria") %>%
  kable_styling(full_width=T)

```

# [References]{.underline} {.unnumbered}



Bürkner PC. brms: An R Package for Bayesian Multilevel Models Using Stan. J Stat Softw. 2017;80:1-28. doi:10.18637/jss.v080.i01

Keefe R, Harvey P, Goldberg T, et al. Norms and standardization of the Brief Assessment of Cognition in Schizophrenia (BACS). Schizophr Res. 2008;102(1-3):108-115. doi:10.1016/j.schres.2008.03.024

Mayer J, Salovey P, Caruso D. Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) User’s Manual. MHS Publishers; 2002.

Lee MD, Wagenmakers E-J. Bayesian cognitive modeling: a practical course. Cambridge: Cambridge University Press; 2014.

Morey R, Rouder J (2022). BayesFactor: Computation of Bayes Factors for Common Designs. R package version
 0.9.12-4.4, https://CRAN.R-project.org/package=BayesFactor.

Peralta, V., & Cuesta, M. J. (1999). Dimensional structure of psychotic symptoms: an item-level analysis of SAPS and SANS symptoms in psychotic disorders. Schizophrenia research, 38(1), 13-26.

Sayers SL, Curran PJ, Mueser KT. Factor structure and construct validity of the Scale for the Assessment of Negative Symptoms. Psychol Assess. 1996;8(3):269-280. doi:10.1037/1040-3590.8.3.269

Vehtari, A., Gelman, A., and Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. *Statistics and Computing*. 27(5), 1413\--1432. <doi:10.1007/s11222-016-9696-4>.

Vehtari, A., Simpson, D., Gelman, A., Yao, Y., and Gabry, J. (2022). Pareto smoothed importance sampling. [preprint arXiv:1507.02646](https://arxiv.org/abs/1507.02646)

Wabersich, D., & Vandekerckhove, J. (2014). The RWiener Package: an R Package Providing Distribution Functions for the Wiener Diffusion Model. R Journal, 6(1).

Wagenmakers, Eric-Jan, Tom Lodewyckx, Himanshu Kuriyal, and Raoul Grasman. 2010. “Bayesian Hypothesis Testing for Psychologists: A Tutorial on the Savage–Dickey Method.” Cognitive Psychology 60 (3): 158–89. https://doi.org/10.1016/j.cogpsych.2009.12.001.
